[2024-05-30 10:46:48,166][HYDRA] Launching 490 jobs locally
[2024-05-30 10:46:48,166][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 10:46:48,462][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:46:49,944][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
2024-05-30 10:46:51 [ERROR]: ❌ No module named 'torch_geometric'
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
2024-05-30 10:46:51 [ERROR]: ❌ name 'MessagePassing' is not defined
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
[2024-05-30 10:46:51,587][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:46:51,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:46:51,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:46:51,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:46:51,620][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:46:51,621][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:46:51,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:46:51,625][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:46:51,625][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:46:51,627][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:46:52,191][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.97it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.782  
                                                              train/f1: 0.738   
                                                              train/precision:  
                                                              0.927             
                                                              train/recall:     
                                                              0.613 train/mre:  
                                                              nan               
[2024-05-30 10:47:18,982][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/0>
[2024-05-30 10:47:18,983][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:47:18,986][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 10:47:19,287][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:47:19,290][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:47:19,293][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:47:19,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:47:19,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:47:19,298][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:47:19,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:47:19,300][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:47:19,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:47:19,302][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:47:19,303][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:47:19,305][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:47:19,348][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.90it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.831  
                                                              train/f1: 0.821   
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              nan               
[2024-05-30 10:47:44,933][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/1>
[2024-05-30 10:47:44,934][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:47:44,937][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 10:47:45,229][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:47:45,232][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:47:45,234][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:47:45,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:47:45,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:47:45,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:47:45,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:47:45,240][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:47:45,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:47:45,243][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:47:45,243][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:47:45,245][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:47:45,288][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.23it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.669  
                                                              train/f1: 0.655   
                                                              train/precision:  
                                                              0.684             
                                                              train/recall:     
                                                              0.629 train/mre:  
                                                              nan               
[2024-05-30 10:48:10,631][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/2>
[2024-05-30 10:48:10,632][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:48:10,635][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 10:48:10,915][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:48:10,917][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:48:10,919][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:48:10,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:48:10,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:48:10,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:48:10,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:48:10,925][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:48:10,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:48:10,930][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:48:10,930][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:48:10,932][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:48:11,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.65it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.798  
                                                              train/f1: 0.806   
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              nan               
[2024-05-30 10:48:36,629][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/3>
[2024-05-30 10:48:36,630][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:48:36,633][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 10:48:36,921][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:48:36,924][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:48:36,926][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:48:36,926][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:48:36,930][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:48:36,930][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:48:36,931][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:48:36,932][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:48:36,934][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:48:36,935][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:48:36,935][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:48:36,937][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:48:36,979][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.15it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.597  
                                                              train/f1: 0.432   
                                                              train/precision:  
                                                              0.731             
                                                              train/recall:     
                                                              0.306 train/mre:  
                                                              nan               
[2024-05-30 10:49:02,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/4>
[2024-05-30 10:49:02,036][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:49:02,039][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 10:49:02,318][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:49:02,321][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:49:02,323][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:49:02,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:49:02,327][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:49:02,328][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:49:02,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:49:02,329][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:49:02,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:49:02,333][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:49:02,333][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:49:02,335][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:49:02,378][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.89it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.669  
                                                              train/f1: 0.732   
                                                              train/precision:  
                                                              0.615             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              nan               
[2024-05-30 10:49:27,801][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/5>
[2024-05-30 10:49:27,802][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:49:27,808][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 10:49:28,089][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:49:28,091][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:49:28,093][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:49:28,094][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:49:28,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:49:28,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:49:28,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:49:28,099][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:49:28,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:49:28,104][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:49:28,104][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:49:28,106][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:49:28,190][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.89it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.742  
                                                              train/f1: 0.758   
                                                              train/precision:  
                                                              0.714             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              nan               
[2024-05-30 10:49:53,894][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/6>
[2024-05-30 10:49:53,895][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:49:53,898][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 10:49:54,191][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:49:54,193][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:49:54,195][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:49:54,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:49:54,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:49:54,200][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:49:54,201][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:49:54,202][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:49:54,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:49:54,204][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:49:54,205][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:49:54,207][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:49:54,260][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.84it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.831  
                                                              train/f1: 0.832   
                                                              train/precision:  
                                                              0.825             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              nan               
[2024-05-30 10:50:19,365][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/7>
[2024-05-30 10:50:19,366][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:50:19,369][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 10:50:19,652][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:50:19,654][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:50:19,656][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:50:19,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:50:19,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:50:19,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:50:19,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:50:19,662][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:50:19,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:50:19,666][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:50:19,666][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:50:19,668][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:50:19,711][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.56it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.702  
                                                              train/f1: 0.641   
                                                              train/precision:  
                                                              0.805             
                                                              train/recall:     
                                                              0.532 train/mre:  
                                                              nan               
[2024-05-30 10:50:45,384][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/8>
[2024-05-30 10:50:45,384][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:50:45,387][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 10:50:45,668][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:50:45,671][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:50:45,673][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:50:45,673][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:50:45,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:50:45,677][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:50:45,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:50:45,679][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:50:45,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:50:45,707][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:50:45,707][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:50:45,711][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:50:45,808][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.02it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.895  
                                                              train/f1: 0.899   
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              nan               
[2024-05-30 10:51:11,985][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/9>
[2024-05-30 10:51:11,986][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:51:11,988][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 10:51:12,273][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:51:12,276][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:51:12,278][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:51:12,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:51:12,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:51:12,283][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:51:12,283][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:51:12,284][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:51:12,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:51:12,287][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:51:12,287][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:51:12,289][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:51:12,332][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.10it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.090 train/auc:  
                                                              0.815 train/f1:   
                                                              0.810             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.113             
[2024-05-30 10:51:38,106][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/0>
[2024-05-30 10:51:38,107][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:51:38,110][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 10:51:38,395][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:51:38,398][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:51:38,400][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:51:38,401][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:51:38,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:51:38,405][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:51:38,406][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:51:38,407][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:51:38,408][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:51:38,410][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:51:38,410][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:51:38,412][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:51:38,459][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.15it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.106 train/auc:  
                                                              0.823 train/f1:   
                                                              0.825             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.117             
[2024-05-30 10:52:03,749][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/1>
[2024-05-30 10:52:03,749][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:52:03,752][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 10:52:04,034][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:52:04,037][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:52:04,039][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:52:04,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:52:04,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:52:04,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:52:04,044][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:52:04,045][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:52:04,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:52:04,066][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:52:04,067][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:52:04,072][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:52:04,164][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.50it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.091 train/auc:  
                                                              0.806 train/f1:   
                                                              0.810             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.114             
[2024-05-30 10:52:29,541][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/2>
[2024-05-30 10:52:29,542][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:52:29,545][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 10:52:29,843][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:52:29,845][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:52:29,847][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:52:29,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:52:29,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:52:29,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:52:29,853][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:52:29,854][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:52:29,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:52:29,856][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:52:29,857][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:52:29,859][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:52:29,911][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.86it/s v_num: 0.000      
                                                              val/auc: 0.728    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.556 val/mre:    
                                                              0.088 train/auc:  
                                                              0.669 train/f1:   
                                                              0.709             
                                                              train/precision:  
                                                              0.633             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.104             
[2024-05-30 10:52:55,692][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/3>
[2024-05-30 10:52:55,693][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:52:55,695][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 10:52:55,982][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:52:55,985][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:52:55,987][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:52:55,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:52:55,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:52:55,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:52:55,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:52:55,993][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:52:55,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:52:55,996][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:52:55,997][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:52:55,999][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:52:56,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.89it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.091 train/auc:  
                                                              0.645 train/f1:   
                                                              0.656             
                                                              train/precision:  
                                                              0.636             
                                                              train/recall:     
                                                              0.677 train/mre:  
                                                              0.108             
[2024-05-30 10:53:21,593][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/4>
[2024-05-30 10:53:21,594][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:53:21,597][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 10:53:21,879][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:53:21,881][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:53:21,884][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:53:21,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:53:21,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:53:21,888][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:53:21,889][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:53:21,890][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:53:21,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:53:21,895][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:53:21,895][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:53:21,897][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:53:21,980][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.65it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.092 train/auc:  
                                                              0.758 train/f1:   
                                                              0.773             
                                                              train/precision:  
                                                              0.729             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.113             
[2024-05-30 10:53:47,052][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/5>
[2024-05-30 10:53:47,053][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:53:47,055][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 10:53:47,346][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:53:47,348][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:53:47,350][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:53:47,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:53:47,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:53:47,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:53:47,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:53:47,357][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:53:47,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:53:47,359][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:53:47,359][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:53:47,362][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:53:47,404][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.72it/s v_num: 0.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre:    
                                                              0.085 train/auc:  
                                                              0.556 train/f1:   
                                                              0.513             
                                                              train/precision:  
                                                              0.569             
                                                              train/recall:     
                                                              0.468 train/mre:  
                                                              0.114             
[2024-05-30 10:54:12,681][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/6>
[2024-05-30 10:54:12,682][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:54:12,685][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 10:54:12,970][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:54:12,972][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:54:12,975][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:54:12,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:54:12,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:54:12,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:54:12,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:54:12,981][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:54:12,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:54:12,983][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:54:12,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:54:12,986][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:54:13,036][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.69it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.100 train/auc:  
                                                              0.903 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.116             
[2024-05-30 10:54:38,439][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/7>
[2024-05-30 10:54:38,440][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:54:38,443][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 10:54:38,970][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:54:38,973][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:54:38,975][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:54:38,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:54:38,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:54:38,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:54:38,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:54:38,980][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:54:38,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:54:38,985][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:54:38,985][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:54:38,987][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:54:39,082][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.78it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.090 train/auc:  
                                                              0.661 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.652             
                                                              train/recall:     
                                                              0.694 train/mre:  
                                                              0.101             
[2024-05-30 10:55:04,855][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/8>
[2024-05-30 10:55:04,855][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:55:04,859][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 10:55:05,146][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:55:05,148][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:55:05,150][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:55:05,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:55:05,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:55:05,155][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:55:05,156][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:55:05,156][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:55:05,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:55:05,160][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:55:05,160][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:55:05,162][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:55:05,206][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.55it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.100 train/auc:  
                                                              0.863 train/f1:   
                                                              0.860             
                                                              train/precision:  
                                                              0.881             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.119             
[2024-05-30 10:55:30,700][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/9>
[2024-05-30 10:55:30,701][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:55:30,705][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 10:55:30,991][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:55:30,993][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:55:30,996][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:55:30,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:55:30,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:55:31,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:55:31,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:55:31,001][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:55:31,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:55:31,004][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:55:31,004][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:55:31,007][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:55:31,047][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.65it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.102 train/auc:  
                                                              0.839 train/f1:   
                                                              0.818             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.726 train/mre:  
                                                              0.122             
[2024-05-30 10:55:56,800][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/0>
[2024-05-30 10:55:56,801][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:55:56,805][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 10:55:57,099][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:55:57,102][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:55:57,104][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:55:57,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:55:57,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:55:57,109][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:55:57,109][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:55:57,110][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:55:57,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:55:57,118][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:55:57,119][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:55:57,121][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:55:57,222][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.30it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.090 train/auc:  
                                                              0.839 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.809             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.111             
[2024-05-30 10:56:22,480][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/1>
[2024-05-30 10:56:22,481][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:56:22,484][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 10:56:22,773][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:56:22,776][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:56:22,778][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:56:22,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:56:22,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:56:22,782][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:56:22,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:56:22,784][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:56:22,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:56:22,787][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:56:22,788][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:56:22,790][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:56:22,837][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.33it/s v_num: 0.000      
                                                              val/auc: 0.728    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.556 val/mre:    
                                                              0.103 train/auc:  
                                                              0.710 train/f1:   
                                                              0.753             
                                                              train/precision:  
                                                              0.655             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.131             
[2024-05-30 10:56:48,209][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/2>
[2024-05-30 10:56:48,209][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:56:48,212][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 10:56:48,491][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:56:48,494][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:56:48,496][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:56:48,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:56:48,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:56:48,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:56:48,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:56:48,502][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:56:48,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:56:48,504][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:56:48,505][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:56:48,507][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:56:48,548][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.86it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.095 train/auc:  
                                                              0.718 train/f1:   
                                                              0.762             
                                                              train/precision:  
                                                              0.659             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.101             
[2024-05-30 10:57:14,094][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/3>
[2024-05-30 10:57:14,095][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:57:14,098][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 10:57:14,381][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:57:14,384][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:57:14,386][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:57:14,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:57:14,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:57:14,391][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:57:14,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:57:14,392][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:57:14,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:57:14,395][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:57:14,396][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:57:14,398][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:57:14,440][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.78it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.098 train/auc:  
                                                              0.702 train/f1:   
                                                              0.730             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.110             
[2024-05-30 10:57:40,461][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/4>
[2024-05-30 10:57:40,462][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:57:40,465][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 10:57:40,748][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:57:40,751][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:57:40,753][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:57:40,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:57:40,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:57:40,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:57:40,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:57:40,759][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:57:40,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:57:40,762][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:57:40,763][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:57:40,765][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:57:40,810][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.00it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.084 train/auc:  
                                                              0.766 train/f1:   
                                                              0.785             
                                                              train/precision:  
                                                              0.726             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.105             
[2024-05-30 10:58:06,796][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/5>
[2024-05-30 10:58:06,797][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:58:06,801][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 10:58:07,091][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:58:07,093][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:58:07,095][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:58:07,096][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:58:07,099][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:58:07,100][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:58:07,101][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:58:07,102][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:58:07,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:58:07,107][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:58:07,107][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:58:07,109][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:58:07,197][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.23it/s v_num: 0.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre:    
                                                              0.086 train/auc:  
                                                              0.637 train/f1:   
                                                              0.646             
                                                              train/precision:  
                                                              0.631             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.108             
[2024-05-30 10:58:32,861][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/6>
[2024-05-30 10:58:32,862][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:58:32,865][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 10:58:33,433][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:58:33,435][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:58:33,437][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:58:33,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:58:33,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:58:33,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:58:33,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:58:33,443][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:58:33,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:58:33,446][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:58:33,446][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:58:33,448][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-05-30 10:58:33,489][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.57it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.096 train/auc:  
                                                              0.806 train/f1:   
                                                              0.812             
                                                              train/precision:  
                                                              0.788             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.109             
[2024-05-30 10:58:59,084][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/7>
[2024-05-30 10:58:59,085][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:58:59,087][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 10:58:59,375][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:58:59,377][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:58:59,379][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:58:59,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:58:59,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:58:59,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:58:59,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:58:59,385][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:58:59,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:58:59,388][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:58:59,388][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:58:59,390][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:58:59,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.07it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.086 train/auc:  
                                                              0.669 train/f1:   
                                                              0.602             
                                                              train/precision:  
                                                              0.756             
                                                              train/recall:     
                                                              0.500 train/mre:  
                                                              0.110             
[2024-05-30 10:59:25,137][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/8>
[2024-05-30 10:59:25,138][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:59:25,141][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 10:59:25,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:59:25,438][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:59:25,441][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:59:25,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:59:25,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:59:25,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:59:25,446][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:59:25,447][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:59:25,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:59:25,451][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:59:25,451][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:59:25,454][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:59:25,500][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.66it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.090 train/auc:  
                                                              0.798 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.740             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.113             
[2024-05-30 10:59:51,328][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/9>
[2024-05-30 10:59:51,328][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 10:59:51,332][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 10:59:51,632][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:59:51,635][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 10:59:51,637][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:59:51,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:59:51,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:59:51,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:59:51,643][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:59:51,644][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:59:51,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:59:51,647][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:59:51,647][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:59:51,650][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:59:51,698][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.55it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.088 train/auc:  
                                                              0.927 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.114             
[2024-05-30 11:00:17,740][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/0>
[2024-05-30 11:00:17,741][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:00:17,744][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:00:18,060][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:00:18,063][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:00:18,065][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:00:18,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:00:18,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:00:18,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:00:18,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:00:18,071][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:00:18,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:00:18,074][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:00:18,075][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:00:18,077][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:00:18,161][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.34it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.087 train/auc:  
                                                              0.903 train/f1:   
                                                              0.900             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.109             
[2024-05-30 11:00:44,072][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/1>
[2024-05-30 11:00:44,073][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:00:44,076][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:00:44,420][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:00:44,424][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:00:44,426][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:00:44,426][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:00:44,431][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:00:44,431][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:00:44,432][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:00:44,433][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:00:44,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:00:44,436][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:00:44,436][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:00:44,438][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:00:44,503][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.49it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.086 train/auc:  
                                                              0.790 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.781             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.111             
[2024-05-30 11:01:11,385][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/2>
[2024-05-30 11:01:11,386][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:01:11,389][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:01:11,675][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:01:11,677][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:01:11,680][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:01:11,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:01:11,683][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:01:11,684][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:01:11,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:01:11,686][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:01:11,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:01:11,690][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:01:11,690][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:01:11,692][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:01:11,736][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.91it/s v_num: 0.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre:    
                                                              0.090 train/auc:  
                                                              0.702 train/f1:   
                                                              0.748             
                                                              train/precision:  
                                                              0.647             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.103             
[2024-05-30 11:01:37,590][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/3>
[2024-05-30 11:01:37,591][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:01:37,594][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:01:37,890][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:01:37,893][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:01:37,895][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:01:37,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:01:37,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:01:37,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:01:37,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:01:37,902][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:01:37,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:01:37,907][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:01:37,907][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:01:37,910][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:01:37,996][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.66it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.089 train/auc:  
                                                              0.669 train/f1:   
                                                              0.709             
                                                              train/precision:  
                                                              0.633             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.109             
[2024-05-30 11:02:03,738][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/4>
[2024-05-30 11:02:03,738][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:02:03,741][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:02:04,028][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:02:04,031][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:02:04,033][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:02:04,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:02:04,037][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:02:04,038][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:02:04,039][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:02:04,040][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:02:04,041][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:02:04,042][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:02:04,043][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:02:04,045][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:02:04,086][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.63it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.099 train/auc:  
                                                              0.581 train/f1:   
                                                              0.409             
                                                              train/precision:  
                                                              0.692             
                                                              train/recall:     
                                                              0.290 train/mre:  
                                                              0.120             
[2024-05-30 11:02:30,041][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/5>
[2024-05-30 11:02:30,042][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:02:30,045][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:02:30,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:02:30,336][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:02:30,338][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:02:30,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:02:30,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:02:30,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:02:30,344][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:02:30,345][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:02:30,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:02:30,348][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:02:30,348][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:02:30,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:02:30,665][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.92it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.889 val/mre:    
                                                              0.089 train/auc:  
                                                              0.629 train/f1:   
                                                              0.589             
                                                              train/precision:  
                                                              0.660             
                                                              train/recall:     
                                                              0.532 train/mre:  
                                                              0.101             
[2024-05-30 11:02:56,903][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/6>
[2024-05-30 11:02:56,904][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:02:56,907][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:02:57,229][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:02:57,231][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:02:57,233][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:02:57,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:02:57,237][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:02:57,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:02:57,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:02:57,240][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:02:57,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:02:57,244][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:02:57,244][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:02:57,246][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:02:57,338][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.56it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.106 train/auc:  
                                                              0.847 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.120             
[2024-05-30 11:03:23,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/7>
[2024-05-30 11:03:23,425][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:03:23,427][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:03:23,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:03:23,720][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:03:23,722][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:03:23,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:03:23,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:03:23,727][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:03:23,727][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:03:23,728][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:03:23,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:03:23,732][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:03:23,732][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:03:23,735][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:03:23,776][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.95it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.778 val/mre:    
                                                              0.087 train/auc:  
                                                              0.742 train/f1:   
                                                              0.765             
                                                              train/precision:  
                                                              0.703             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.111             
[2024-05-30 11:03:49,500][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/8>
[2024-05-30 11:03:49,501][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:03:49,504][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:03:49,788][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:03:49,790][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:03:49,792][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:03:49,793][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:03:49,796][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:03:49,797][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:03:49,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:03:49,799][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:03:49,800][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:03:49,802][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:03:49,802][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:03:49,804][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:03:49,846][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.55it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.094 train/auc:  
                                                              0.863 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.117             
[2024-05-30 11:04:15,569][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/9>
[2024-05-30 11:04:15,570][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:04:15,573][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:04:15,865][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:04:15,867][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:04:15,870][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:04:15,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:04:15,874][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:04:15,874][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:04:15,875][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:04:15,876][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:04:15,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:04:15,879][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:04:15,879][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:04:15,882][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:04:15,924][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.69it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.089 train/auc:  
                                                              0.911 train/f1:   
                                                              0.904             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.113             
[2024-05-30 11:04:41,751][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/0>
[2024-05-30 11:04:41,752][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:04:41,754][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:04:42,035][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:04:42,038][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:04:42,040][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:04:42,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:04:42,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:04:42,045][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:04:42,046][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:04:42,046][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:04:42,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:04:42,049][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:04:42,050][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:04:42,052][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:04:42,095][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.28it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.099 train/auc:  
                                                              0.823 train/f1:   
                                                              0.828             
                                                              train/precision:  
                                                              0.803             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.115             
[2024-05-30 11:05:07,584][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/1>
[2024-05-30 11:05:07,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:05:07,588][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:05:07,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:05:07,870][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:05:07,872][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:05:07,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:05:07,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:05:07,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:05:07,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:05:07,878][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:05:07,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:05:07,881][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:05:07,881][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:05:07,883][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:05:07,925][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.09it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.088 train/auc:  
                                                              0.863 train/f1:   
                                                              0.872             
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.117             
[2024-05-30 11:05:33,536][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/2>
[2024-05-30 11:05:33,537][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:05:33,540][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:05:33,821][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:05:33,823][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:05:33,825][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:05:33,826][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:05:33,829][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:05:33,830][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:05:33,831][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:05:33,832][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:05:33,833][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:05:33,834][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:05:33,835][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:05:33,837][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:05:33,878][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.27it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.095 train/auc:  
                                                              0.750 train/f1:   
                                                              0.767             
                                                              train/precision:  
                                                              0.718             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.104             
[2024-05-30 11:05:59,268][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/3>
[2024-05-30 11:05:59,269][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:05:59,272][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:05:59,564][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:05:59,567][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:05:59,569][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:05:59,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:05:59,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:05:59,574][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:05:59,575][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:05:59,575][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:05:59,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:05:59,578][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:05:59,578][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:05:59,581][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:05:59,623][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.83it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.097 train/auc:  
                                                              0.685 train/f1:   
                                                              0.715             
                                                              train/precision:  
                                                              0.653             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.108             
[2024-05-30 11:06:25,932][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/4>
[2024-05-30 11:06:25,932][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:06:25,935][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:06:26,242][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:06:26,244][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:06:26,247][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:06:26,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:06:26,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:06:26,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:06:26,252][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:06:26,253][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:06:26,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:06:26,258][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:06:26,258][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:06:26,260][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:06:26,306][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.65it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.086 train/auc:  
                                                              0.766 train/f1:   
                                                              0.788             
                                                              train/precision:  
                                                              0.720             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.106             
[2024-05-30 11:06:52,896][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/5>
[2024-05-30 11:06:52,896][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:06:52,899][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:06:53,191][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:06:53,193][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:06:53,196][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:06:53,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:06:53,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:06:53,200][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:06:53,201][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:06:53,202][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:06:53,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:06:53,205][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:06:53,205][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:06:53,208][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:06:53,251][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.50it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.778 val/mre:    
                                                              0.090 train/auc:  
                                                              0.718 train/f1:   
                                                              0.724             
                                                              train/precision:  
                                                              0.708             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.107             
[2024-05-30 11:07:19,386][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/6>
[2024-05-30 11:07:19,386][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:07:19,390][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:07:19,685][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:07:19,688][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:07:19,690][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:07:19,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:07:19,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:07:19,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:07:19,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:07:19,697][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:07:19,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:07:19,700][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:07:19,701][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:07:19,703][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:07:19,790][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.76it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.096 train/auc:  
                                                              0.831 train/f1:   
                                                              0.840             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.117             
[2024-05-30 11:07:45,906][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/7>
[2024-05-30 11:07:45,907][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:07:45,910][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:07:46,202][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:07:46,204][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:07:46,207][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:07:46,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:07:46,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:07:46,211][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:07:46,212][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:07:46,213][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:07:46,215][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:07:46,216][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:07:46,216][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:07:46,218][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:07:46,260][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.20it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.778 val/mre:    
                                                              0.091 train/auc:  
                                                              0.710 train/f1:   
                                                              0.673             
                                                              train/precision:  
                                                              0.771             
                                                              train/recall:     
                                                              0.597 train/mre:  
                                                              0.105             
[2024-05-30 11:08:12,566][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/8>
[2024-05-30 11:08:12,567][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:08:12,570][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:08:12,860][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:08:12,862][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:08:12,865][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:08:12,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:08:12,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:08:12,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:08:12,870][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:08:12,871][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:08:12,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:08:12,873][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:08:12,874][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:08:12,876][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:08:12,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.97it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.092 train/auc:  
                                                              0.871 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.811             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.114             
[2024-05-30 11:08:38,978][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/9>
[2024-05-30 11:08:38,978][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:08:38,981][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:08:39,276][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:08:39,279][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:08:39,281][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:08:39,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:08:39,285][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:08:39,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:08:39,287][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:08:39,287][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:08:39,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:08:39,295][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:08:39,296][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:08:39,298][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:08:39,387][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.64it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.091 train/auc:  
                                                              0.887 train/f1:   
                                                              0.877             
                                                              train/precision:  
                                                              0.962             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.114             
[2024-05-30 11:09:05,553][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/0>
[2024-05-30 11:09:05,553][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:09:05,556][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:09:05,859][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:09:05,862][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:09:05,864][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:09:05,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:09:05,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:09:05,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:09:05,870][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:09:05,871][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:09:05,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:09:05,875][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:09:05,876][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:09:05,878][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:09:05,925][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.19it/s v_num: 0.000      
                                                              val/auc: 0.689    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.778 val/mre:    
                                                              0.092 train/auc:  
                                                              0.790 train/f1:   
                                                              0.814             
                                                              train/precision:  
                                                              0.731             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.116             
[2024-05-30 11:09:32,266][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/1>
[2024-05-30 11:09:32,267][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:09:32,270][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:09:32,560][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:09:32,562][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:09:32,565][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:09:32,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:09:32,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:09:32,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:09:32,570][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:09:32,571][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:09:32,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:09:32,576][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:09:32,576][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:09:32,578][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:09:32,621][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.38it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.090 train/auc:  
                                                              0.895 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.116             
[2024-05-30 11:09:59,523][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/2>
[2024-05-30 11:09:59,524][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:09:59,527][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:09:59,835][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:09:59,838][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:09:59,841][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:09:59,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:09:59,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:09:59,847][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:09:59,847][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:09:59,848][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:09:59,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:09:59,851][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:09:59,851][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:09:59,853][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:09:59,896][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.65it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.087 train/auc:  
                                                              0.790 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.105             
[2024-05-30 11:10:25,877][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/3>
[2024-05-30 11:10:25,878][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:10:25,881][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:10:26,182][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:10:26,184][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:10:26,187][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:10:26,187][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:10:26,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:10:26,191][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:10:26,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:10:26,193][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:10:26,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:10:26,198][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:10:26,198][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:10:26,200][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:10:26,249][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.01it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.091 train/auc:  
                                                              0.621 train/f1:   
                                                              0.591             
                                                              train/precision:  
                                                              0.642             
                                                              train/recall:     
                                                              0.548 train/mre:  
                                                              0.102             
[2024-05-30 11:10:52,316][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/4>
[2024-05-30 11:10:52,316][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:10:52,319][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:10:52,600][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:10:52,602][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:10:52,604][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:10:52,605][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:10:52,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:10:52,609][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:10:52,609][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:10:52,610][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:10:52,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:10:52,614][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:10:52,614][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:10:52,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:10:52,712][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.58it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.096 train/auc:  
                                                              0.871 train/f1:   
                                                              0.871             
                                                              train/precision:  
                                                              0.871             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.112             
[2024-05-30 11:11:19,515][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/5>
[2024-05-30 11:11:19,516][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:11:19,519][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:11:19,809][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:11:19,812][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:11:19,814][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:11:19,815][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:11:19,818][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:11:19,819][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:11:19,820][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:11:19,820][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:11:19,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:11:19,823][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:11:19,823][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:11:19,825][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:11:19,876][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.34it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.103 train/auc:  
                                                              0.605 train/f1:   
                                                              0.614             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.629 train/mre:  
                                                              0.118             
[2024-05-30 11:11:45,678][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/6>
[2024-05-30 11:11:45,679][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:11:45,681][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:11:45,991][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:11:45,993][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:11:45,996][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:11:45,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:11:45,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:11:46,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:11:46,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:11:46,002][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:11:46,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:11:46,006][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:11:46,007][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:11:46,009][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:11:46,058][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.61it/s v_num: 0.000      
                                                              val/auc: 0.794    
                                                              val/f1: 0.800     
                                                              val/precision:    
                                                              0.727 val/recall: 
                                                              0.889 val/mre:    
                                                              0.092 train/auc:  
                                                              0.766 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.709             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.108             
[2024-05-30 11:12:12,415][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/7>
[2024-05-30 11:12:12,416][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:12:12,419][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:12:12,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:12:12,718][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:12:12,721][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:12:12,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:12:12,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:12:12,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:12:12,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:12:12,727][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:12:12,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:12:12,732][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:12:12,733][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:12:12,735][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:12:12,817][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.78it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.088 train/auc:  
                                                              0.669 train/f1:   
                                                              0.602             
                                                              train/precision:  
                                                              0.756             
                                                              train/recall:     
                                                              0.500 train/mre:  
                                                              0.105             
[2024-05-30 11:12:39,154][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/8>
[2024-05-30 11:12:39,155][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:12:39,158][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:12:39,457][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:12:39,459][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:12:39,461][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:12:39,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:12:39,465][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:12:39,466][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:12:39,467][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:12:39,468][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:12:39,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:12:39,471][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:12:39,471][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:12:39,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:12:39,516][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.07it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.750     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              1.000 val/mre:    
                                                              0.098 train/auc:  
                                                              0.903 train/f1:   
                                                              0.900             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.115             
[2024-05-30 11:13:05,953][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/9>
[2024-05-30 11:13:05,954][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:13:05,957][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:13:06,250][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:13:06,253][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:13:06,255][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:13:06,256][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:13:06,259][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:13:06,260][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:13:06,261][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:13:06,262][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:13:06,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:13:06,265][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:13:06,265][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:13:06,267][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:13:06,311][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.78it/s v_num: 0.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre:    
                                                              0.100 train/auc:  
                                                              0.879 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.961             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.113             
[2024-05-30 11:13:32,476][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/0>
[2024-05-30 11:13:32,477][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:13:32,481][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:13:32,778][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:13:32,780][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:13:32,783][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:13:32,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:13:32,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:13:32,788][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:13:32,788][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:13:32,789][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:13:32,791][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:13:32,793][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:13:32,793][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:13:32,796][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:13:32,880][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.08it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.094 train/auc:  
                                                              0.879 train/f1:   
                                                              0.876             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.113             
[2024-05-30 11:13:59,078][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/1>
[2024-05-30 11:13:59,078][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:13:59,081][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:13:59,375][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:13:59,377][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:13:59,380][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:13:59,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:13:59,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:13:59,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:13:59,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:13:59,386][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:13:59,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:13:59,391][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:13:59,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:13:59,393][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:13:59,444][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.89it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.093 train/auc:  
                                                              0.863 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.118             
[2024-05-30 11:14:25,632][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/2>
[2024-05-30 11:14:25,632][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:14:25,636][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:14:25,924][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:14:25,927][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:14:25,929][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:14:25,929][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:14:25,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:14:25,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:14:25,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:14:25,935][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:14:25,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:14:25,938][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:14:25,938][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:14:25,941][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:14:25,984][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.16it/s v_num: 0.000      
                                                              val/auc: 0.689    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.778 val/mre:    
                                                              0.086 train/auc:  
                                                              0.718 train/f1:   
                                                              0.755             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.107             
[2024-05-30 11:14:51,957][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/3>
[2024-05-30 11:14:51,958][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:14:51,961][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:14:52,250][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:14:52,252][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:14:52,255][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:14:52,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:14:52,259][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:14:52,259][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:14:52,260][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:14:52,261][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:14:52,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:14:52,265][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:14:52,265][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:14:52,267][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:14:52,350][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.66it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.097 train/auc:  
                                                              0.726 train/f1:   
                                                              0.738             
                                                              train/precision:  
                                                              0.706             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.121             
[2024-05-30 11:15:18,680][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/4>
[2024-05-30 11:15:18,681][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:15:18,684][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:15:18,996][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:15:18,998][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:15:19,000][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:15:19,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:15:19,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:15:19,005][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:15:19,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:15:19,007][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:15:19,008][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:15:19,023][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:15:19,024][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:15:19,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:15:19,103][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.43it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.889 val/mre:    
                                                              0.093 train/auc:  
                                                              0.871 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.119             
[2024-05-30 11:15:45,585][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/5>
[2024-05-30 11:15:45,586][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:15:45,588][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:15:45,877][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:15:45,879][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:15:45,881][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:15:45,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:15:45,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:15:45,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:15:45,887][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:15:45,888][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:15:45,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:15:45,890][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:15:45,891][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:15:45,893][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:15:45,936][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.96it/s v_num: 0.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre:    
                                                              0.089 train/auc:  
                                                              0.806 train/f1:   
                                                              0.786             
                                                              train/precision:  
                                                              0.880             
                                                              train/recall:     
                                                              0.710 train/mre:  
                                                              0.110             
[2024-05-30 11:16:11,425][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/6>
[2024-05-30 11:16:11,426][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:16:11,431][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:16:11,712][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:16:11,715][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:16:11,717][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:16:11,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:16:11,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:16:11,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:16:11,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:16:11,723][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:16:11,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:16:11,725][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:16:11,726][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:16:11,728][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:16:11,768][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.39it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.095 train/auc:  
                                                              0.855 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.844             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.109             
[2024-05-30 11:16:37,874][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/7>
[2024-05-30 11:16:37,875][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:16:37,878][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:16:38,183][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:16:38,186][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:16:38,188][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:16:38,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:16:38,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:16:38,193][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:16:38,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:16:38,195][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:16:38,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:16:38,199][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:16:38,200][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:16:38,202][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:16:38,270][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.09it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.092 train/auc:  
                                                              0.645 train/f1:   
                                                              0.600             
                                                              train/precision:  
                                                              0.688             
                                                              train/recall:     
                                                              0.532 train/mre:  
                                                              0.106             
[2024-05-30 11:17:03,416][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/8>
[2024-05-30 11:17:03,417][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:17:03,419][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:17:03,703][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:17:03,706][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:17:03,708][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:17:03,708][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:17:03,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:17:03,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:17:03,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:17:03,714][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:17:03,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:17:03,717][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:17:03,718][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:17:03,720][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:17:03,801][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.43it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.099 train/auc:  
                                                              0.952 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.114             
[2024-05-30 11:17:28,873][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/9>
[2024-05-30 11:17:28,874][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:17:28,877][HYDRA] 	#70 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:17:29,167][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:17:29,169][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:17:29,171][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:17:29,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:17:29,175][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:17:29,176][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:17:29,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:17:29,178][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:17:29,179][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:17:29,181][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:17:29,181][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:17:29,183][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:17:29,225][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.21it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.589  
                                                              train/f1: 0.671   
                                                              train/precision:  
                                                              0.560             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              nan               
[2024-05-30 11:17:53,066][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/0>
[2024-05-30 11:17:53,067][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:17:53,070][HYDRA] 	#71 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:17:53,362][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:17:53,364][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:17:53,366][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:17:53,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:17:53,370][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:17:53,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:17:53,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:17:53,373][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:17:53,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:17:53,377][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:17:53,378][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:17:53,380][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:17:53,426][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.63it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.643  
                                                              train/f1: 0.733   
                                                              train/precision:  
                                                              0.585             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              nan               
[2024-05-30 11:18:16,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/1>
[2024-05-30 11:18:16,236][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:18:16,239][HYDRA] 	#72 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:18:16,518][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:18:16,521][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:18:16,523][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:18:16,523][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:18:16,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:18:16,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:18:16,529][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:18:16,529][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:18:16,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:18:16,533][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:18:16,533][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:18:16,535][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:18:16,618][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.46it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.607  
                                                              train/f1: 0.699   
                                                              train/precision:  
                                                              0.567             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-05-30 11:18:39,295][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/2>
[2024-05-30 11:18:39,295][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:18:39,299][HYDRA] 	#73 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:18:39,579][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:18:39,581][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:18:39,583][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:18:39,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:18:39,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:18:39,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:18:39,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:18:39,589][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:18:39,591][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:18:39,592][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:18:39,592][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:18:39,594][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:18:39,636][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.33it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.607  
                                                              train/f1: 0.662   
                                                              train/precision:  
                                                              0.581             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              nan               
[2024-05-30 11:19:02,861][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/3>
[2024-05-30 11:19:02,862][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:19:02,864][HYDRA] 	#74 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:19:03,147][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:19:03,150][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:19:03,152][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:19:03,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:19:03,156][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:19:03,156][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:19:03,157][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:19:03,158][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:19:03,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:19:03,161][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:19:03,161][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:19:03,163][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:19:03,203][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.20it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.625  
                                                              train/f1: 0.708   
                                                              train/precision:  
                                                              0.580             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-05-30 11:19:26,427][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/4>
[2024-05-30 11:19:26,428][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:19:26,431][HYDRA] 	#75 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:19:26,711][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:19:26,713][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:19:26,715][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:19:26,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:19:26,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:19:26,720][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:19:26,720][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:19:26,721][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:19:26,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:19:26,725][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:19:26,725][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:19:26,727][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:19:26,814][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.53it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.598  
                                                              train/f1: 0.694   
                                                              train/precision:  
                                                              0.560             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-05-30 11:19:50,000][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/5>
[2024-05-30 11:19:50,000][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:19:50,003][HYDRA] 	#76 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:19:50,581][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:19:50,583][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:19:50,585][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:19:50,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:19:50,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:19:50,590][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:19:50,591][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:19:50,591][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:19:50,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:19:50,594][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:19:50,594][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:19:50,596][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:19:50,641][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.05it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.536  
                                                              train/f1: 0.552   
                                                              train/precision:  
                                                              0.533             
                                                              train/recall:     
                                                              0.571 train/mre:  
                                                              nan               
[2024-05-30 11:20:13,593][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/6>
[2024-05-30 11:20:13,594][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:20:13,597][HYDRA] 	#77 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:20:13,879][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:20:13,881][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:20:13,884][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:20:13,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:20:13,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:20:13,888][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:20:13,889][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:20:13,890][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:20:13,891][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:20:13,892][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:20:13,892][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:20:13,895][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:20:13,936][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.64it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.607  
                                                              train/f1: 0.672   
                                                              train/precision:  
                                                              0.577             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              nan               
[2024-05-30 11:20:36,719][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/7>
[2024-05-30 11:20:36,719][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:20:36,722][HYDRA] 	#78 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:20:37,010][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:20:37,013][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:20:37,015][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:20:37,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:20:37,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:20:37,019][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:20:37,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:20:37,021][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:20:37,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:20:37,024][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:20:37,024][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:20:37,026][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:20:37,067][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.88it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.643  
                                                              train/f1: 0.722   
                                                              train/precision:  
                                                              0.591             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              nan               
[2024-05-30 11:21:00,307][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/8>
[2024-05-30 11:21:00,308][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:21:00,311][HYDRA] 	#79 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:21:00,598][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:21:00,600][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:21:00,603][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:21:00,603][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:21:00,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:21:00,607][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:21:00,608][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:21:00,609][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:21:00,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:21:00,612][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:21:00,612][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:21:00,615][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:21:00,657][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.26it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.598  
                                                              train/f1: 0.690   
                                                              train/precision:  
                                                              0.562             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              nan               
[2024-05-30 11:21:23,461][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/9>
[2024-05-30 11:21:23,461][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:21:23,464][HYDRA] 	#80 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:21:23,755][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:21:23,758][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:21:23,760][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:21:23,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:21:23,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:21:23,765][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:21:23,766][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:21:23,766][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:21:23,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:21:23,771][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:21:23,771][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:21:23,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:21:23,817][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.47it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.580 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.553             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.073             
[2024-05-30 11:21:46,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/0>
[2024-05-30 11:21:46,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:21:46,762][HYDRA] 	#81 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:21:47,060][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:21:47,062][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:21:47,065][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:21:47,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:21:47,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:21:47,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:21:47,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:21:47,071][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:21:47,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:21:47,074][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:21:47,075][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:21:47,077][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:21:47,158][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.11it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.580 train/f1:   
                                                              0.647             
                                                              train/precision:  
                                                              0.558             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-05-30 11:22:09,492][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/1>
[2024-05-30 11:22:09,493][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:22:09,496][HYDRA] 	#82 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:22:09,777][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:22:09,779][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:22:09,782][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:22:09,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:22:09,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:22:09,786][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:22:09,787][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:22:09,788][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:22:09,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:22:09,791][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:22:09,791][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:22:09,793][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:22:09,841][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.07it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.571 train/f1:   
                                                              0.647             
                                                              train/precision:  
                                                              0.550             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.075             
[2024-05-30 11:22:33,173][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/2>
[2024-05-30 11:22:33,174][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:22:33,177][HYDRA] 	#83 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:22:33,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:22:33,465][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:22:33,468][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:22:33,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:22:33,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:22:33,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:22:33,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:22:33,474][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:22:33,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:22:33,478][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:22:33,478][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:22:33,480][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:22:33,525][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.40it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.571 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.547             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.078             
[2024-05-30 11:22:56,864][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/3>
[2024-05-30 11:22:56,864][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:22:56,868][HYDRA] 	#84 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:22:57,151][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:22:57,154][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:22:57,156][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:22:57,156][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:22:57,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:22:57,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:22:57,161][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:22:57,162][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:22:57,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:22:57,166][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:22:57,166][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:22:57,168][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:22:57,254][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.07it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.071 train/auc:  
                                                              0.571 train/f1:   
                                                              0.657             
                                                              train/precision:  
                                                              0.548             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.077             
[2024-05-30 11:23:20,674][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/4>
[2024-05-30 11:23:20,675][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:23:20,677][HYDRA] 	#85 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:23:20,962][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:23:20,964][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:23:20,967][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:23:20,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:23:20,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:23:20,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:23:20,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:23:20,973][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:23:20,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:23:20,976][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:23:20,976][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:23:20,978][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:23:21,021][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.70it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.071 train/auc:  
                                                              0.589 train/f1:   
                                                              0.652             
                                                              train/precision:  
                                                              0.566             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.080             
[2024-05-30 11:23:43,847][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/5>
[2024-05-30 11:23:43,848][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:23:43,851][HYDRA] 	#86 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:23:44,126][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:23:44,128][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:23:44,130][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:23:44,131][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:23:44,134][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:23:44,134][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:23:44,135][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:23:44,136][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:23:44,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:23:44,139][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:23:44,139][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:23:44,141][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:23:44,183][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.12it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.067 train/auc:  
                                                              0.536 train/f1:   
                                                              0.567             
                                                              train/precision:  
                                                              0.531             
                                                              train/recall:     
                                                              0.607 train/mre:  
                                                              0.082             
[2024-05-30 11:24:07,577][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/6>
[2024-05-30 11:24:07,578][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:24:07,581][HYDRA] 	#87 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:24:07,871][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:24:07,874][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:24:07,877][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:24:07,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:24:07,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:24:07,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:24:07,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:24:07,884][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:24:07,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:24:07,888][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:24:07,888][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:24:07,890][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:24:07,981][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.28it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.607 train/f1:   
                                                              0.656             
                                                              train/precision:  
                                                              0.583             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.074             
[2024-05-30 11:24:30,974][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/7>
[2024-05-30 11:24:30,975][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:24:30,978][HYDRA] 	#88 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:24:31,262][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:24:31,264][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:24:31,266][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:24:31,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:24:31,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:24:31,271][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:24:31,271][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:24:31,272][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:24:31,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:24:31,275][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:24:31,276][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:24:31,278][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:24:31,321][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.47it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.562 train/f1:   
                                                              0.602             
                                                              train/precision:  
                                                              0.552             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.072             
[2024-05-30 11:24:54,315][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/8>
[2024-05-30 11:24:54,316][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:24:54,319][HYDRA] 	#89 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:24:54,607][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:24:54,609][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:24:54,611][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:24:54,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:24:54,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:24:54,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:24:54,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:24:54,617][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:24:54,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:24:54,621][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:24:54,622][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:24:54,626][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:24:54,671][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.52it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.652 train/f1:   
                                                              0.727             
                                                              train/precision:  
                                                              0.598             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.082             
[2024-05-30 11:25:17,636][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/9>
[2024-05-30 11:25:17,637][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:25:17,641][HYDRA] 	#90 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:25:17,928][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:25:17,931][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:25:17,933][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:25:17,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:25:17,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:25:17,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:25:17,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:25:17,939][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:25:17,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:25:17,943][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:25:17,943][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:25:17,945][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:25:18,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.85it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.598 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.570             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.072             
[2024-05-30 11:25:40,768][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/0>
[2024-05-30 11:25:40,769][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:25:40,772][HYDRA] 	#91 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:25:41,063][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:25:41,066][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:25:41,068][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:25:41,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:25:41,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:25:41,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:25:41,073][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:25:41,074][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:25:41,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:25:41,077][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:25:41,077][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:25:41,081][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:25:41,124][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.91it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.589 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.562             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.073             
[2024-05-30 11:26:04,075][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/1>
[2024-05-30 11:26:04,075][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:26:04,078][HYDRA] 	#92 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:26:04,360][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:26:04,362][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:26:04,364][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:26:04,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:26:04,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:26:04,369][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:26:04,370][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:26:04,370][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:26:04,372][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:26:04,374][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:26:04,375][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:26:04,377][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:26:04,427][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.39it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.554 train/f1:   
                                                              0.632             
                                                              train/precision:  
                                                              0.538             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-05-30 11:26:27,724][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/2>
[2024-05-30 11:26:27,724][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:26:27,727][HYDRA] 	#93 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:26:28,015][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:26:28,018][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:26:28,020][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:26:28,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:26:28,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:26:28,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:26:28,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:26:28,027][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:26:28,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:26:28,038][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:26:28,038][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:26:28,042][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:26:28,124][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.50it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.580 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.554             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.079             
[2024-05-30 11:26:50,962][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/3>
[2024-05-30 11:26:50,962][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:26:50,965][HYDRA] 	#94 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:26:51,384][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:26:51,387][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:26:51,389][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:26:51,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:26:51,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:26:51,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:26:51,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:26:51,396][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:26:51,398][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:26:51,399][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:26:51,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:26:51,402][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:26:51,471][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.52it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.069 train/auc:  
                                                              0.580 train/f1:   
                                                              0.676             
                                                              train/precision:  
                                                              0.551             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.073             
[2024-05-30 11:27:14,306][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/4>
[2024-05-30 11:27:14,307][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:27:14,310][HYDRA] 	#95 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:27:14,591][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:27:14,594][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:27:14,596][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:27:14,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:27:14,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:27:14,600][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:27:14,601][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:27:14,602][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:27:14,604][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:27:14,604][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:27:14,605][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:27:14,608][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:27:14,650][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.35it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.598 train/f1:   
                                                              0.681             
                                                              train/precision:  
                                                              0.565             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.076             
[2024-05-30 11:27:37,581][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/5>
[2024-05-30 11:27:37,582][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:27:37,584][HYDRA] 	#96 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:27:37,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:27:37,870][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:27:37,873][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:27:37,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:27:37,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:27:37,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:27:37,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:27:37,879][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:27:37,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:27:37,882][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:27:37,883][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:27:37,885][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:27:37,995][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.55it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.527 train/f1:   
                                                              0.589             
                                                              train/precision:  
                                                              0.521             
                                                              train/recall:     
                                                              0.679 train/mre:  
                                                              0.081             
[2024-05-30 11:28:01,279][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/6>
[2024-05-30 11:28:01,280][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:28:01,283][HYDRA] 	#97 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:28:01,563][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:28:01,565][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:28:01,568][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:28:01,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:28:01,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:28:01,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:28:01,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:28:01,574][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:28:01,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:28:01,576][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:28:01,577][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:28:01,580][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:28:01,621][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.61it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.598 train/f1:   
                                                              0.646             
                                                              train/precision:  
                                                              0.577             
                                                              train/recall:     
                                                              0.732 train/mre:  
                                                              0.073             
[2024-05-30 11:28:24,398][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/7>
[2024-05-30 11:28:24,399][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:28:24,402][HYDRA] 	#98 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:28:24,690][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:28:24,693][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:28:24,695][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:28:24,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:28:24,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:28:24,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:28:24,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:28:24,701][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:28:24,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:28:24,705][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:28:24,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:28:24,708][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:28:24,753][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.53it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.571 train/f1:   
                                                              0.636             
                                                              train/precision:  
                                                              0.553             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.071             
[2024-05-30 11:28:47,749][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/8>
[2024-05-30 11:28:47,750][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:28:47,752][HYDRA] 	#99 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:28:48,362][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:28:48,364][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:28:48,367][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:28:48,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:28:48,370][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:28:48,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:28:48,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:28:48,372][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:28:48,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:28:48,376][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:28:48,376][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:28:48,378][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-05-30 11:28:48,462][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.05it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.634 train/f1:   
                                                              0.709             
                                                              train/precision:  
                                                              0.588             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.078             
[2024-05-30 11:29:11,847][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/9>
[2024-05-30 11:29:11,847][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:29:11,850][HYDRA] 	#100 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:29:12,133][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:29:12,135][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:29:12,138][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:29:12,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:29:12,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:29:12,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:29:12,145][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:29:12,146][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:29:12,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:29:12,149][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:29:12,149][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:29:12,151][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:29:12,194][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.73it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.634 train/f1:   
                                                              0.696             
                                                              train/precision:  
                                                              0.595             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.071             
[2024-05-30 11:29:35,004][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/0>
[2024-05-30 11:29:35,005][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:29:35,007][HYDRA] 	#101 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:29:35,287][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:29:35,290][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:29:35,292][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:29:35,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:29:35,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:29:35,296][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:29:35,297][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:29:35,298][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:29:35,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:29:35,300][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:29:35,301][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:29:35,303][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:29:35,345][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.25it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.062 train/auc:  
                                                              0.580 train/f1:   
                                                              0.641             
                                                              train/precision:  
                                                              0.560             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.071             
[2024-05-30 11:29:58,283][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/1>
[2024-05-30 11:29:58,284][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:29:58,287][HYDRA] 	#102 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:29:58,593][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:29:58,596][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:29:58,598][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:29:58,598][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:29:58,602][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:29:58,603][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:29:58,603][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:29:58,604][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:29:58,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:29:58,608][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:29:58,608][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:29:58,612][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:29:58,695][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.30it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.554 train/f1:   
                                                              0.627             
                                                              train/precision:  
                                                              0.538             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.072             
[2024-05-30 11:30:21,781][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/2>
[2024-05-30 11:30:21,781][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:30:21,784][HYDRA] 	#103 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:30:22,069][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:30:22,071][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:30:22,074][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:30:22,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:30:22,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:30:22,078][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:30:22,079][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:30:22,080][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:30:22,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:30:22,083][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:30:22,083][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:30:22,085][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:30:22,128][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.06it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.580 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.552             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.081             
[2024-05-30 11:30:45,054][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/3>
[2024-05-30 11:30:45,055][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:30:45,058][HYDRA] 	#104 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:30:45,339][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:30:45,341][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:30:45,344][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:30:45,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:30:45,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:30:45,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:30:45,349][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:30:45,350][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:30:45,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:30:45,352][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:30:45,352][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:30:45,356][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:30:45,401][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.15it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.069 train/auc:  
                                                              0.571 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.544             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.074             
[2024-05-30 11:31:08,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/4>
[2024-05-30 11:31:08,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:31:08,491][HYDRA] 	#105 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:31:08,771][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:31:08,773][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:31:08,775][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:31:08,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:31:08,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:31:08,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:31:08,781][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:31:08,782][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:31:08,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:31:08,784][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:31:08,784][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:31:08,787][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:31:08,827][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.42it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.598 train/f1:   
                                                              0.685             
                                                              train/precision:  
                                                              0.563             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.071             
[2024-05-30 11:31:31,676][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/5>
[2024-05-30 11:31:31,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:31:31,680][HYDRA] 	#106 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:31:31,960][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:31:31,962][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:31:31,964][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:31:31,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:31:31,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:31:31,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:31:31,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:31:31,970][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:31:31,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:31:31,974][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:31:31,974][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:31:31,978][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:31:32,021][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.58it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.536 train/f1:   
                                                              0.574             
                                                              train/precision:  
                                                              0.530             
                                                              train/recall:     
                                                              0.625 train/mre:  
                                                              0.081             
[2024-05-30 11:31:54,944][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/6>
[2024-05-30 11:31:54,945][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:31:54,948][HYDRA] 	#107 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:31:55,253][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:31:55,255][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:31:55,257][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:31:55,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:31:55,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:31:55,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:31:55,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:31:55,263][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:31:55,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:31:55,268][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:31:55,268][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:31:55,271][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:31:55,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.23it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.616 train/f1:   
                                                              0.645             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.072             
[2024-05-30 11:32:18,108][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/7>
[2024-05-30 11:32:18,109][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:32:18,112][HYDRA] 	#108 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:32:18,398][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:32:18,400][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:32:18,403][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:32:18,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:32:18,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:32:18,407][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:32:18,408][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:32:18,409][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:32:18,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:32:18,412][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:32:18,412][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:32:18,416][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:32:18,514][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.69it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.562 train/f1:   
                                                              0.632             
                                                              train/precision:  
                                                              0.545             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.071             
[2024-05-30 11:32:41,643][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/8>
[2024-05-30 11:32:41,643][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:32:41,656][HYDRA] 	#109 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:32:41,954][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:32:41,956][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:32:41,958][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:32:41,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:32:41,962][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:32:41,963][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:32:41,964][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:32:41,964][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:32:41,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:32:41,967][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:32:41,968][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:32:41,970][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:32:42,013][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.08it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.643 train/f1:   
                                                              0.706             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.078             
[2024-05-30 11:33:05,435][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/9>
[2024-05-30 11:33:05,436][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:33:05,439][HYDRA] 	#110 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:33:05,726][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:33:05,729][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:33:05,731][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:33:05,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:33:05,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:33:05,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:33:05,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:33:05,737][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:33:05,739][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:33:05,742][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:33:05,742][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:33:05,746][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:33:05,791][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.81it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.062 train/auc:  
                                                              0.616 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.587             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.069             
[2024-05-30 11:33:28,631][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/0>
[2024-05-30 11:33:28,632][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:33:28,635][HYDRA] 	#111 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:33:28,919][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:33:28,921][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:33:28,924][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:33:28,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:33:28,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:33:28,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:33:28,929][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:33:28,930][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:33:28,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:33:28,933][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:33:28,934][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:33:28,936][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:33:29,016][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.30it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.580 train/f1:   
                                                              0.647             
                                                              train/precision:  
                                                              0.558             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-05-30 11:33:52,128][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/1>
[2024-05-30 11:33:52,128][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:33:52,131][HYDRA] 	#112 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:33:52,748][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:33:52,750][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:33:52,752][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:33:52,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:33:52,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:33:52,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:33:52,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:33:52,758][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:33:52,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:33:52,761][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:33:52,761][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:33:52,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:33:52,807][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.78it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.071 train/auc:  
                                                              0.545 train/f1:   
                                                              0.628             
                                                              train/precision:  
                                                              0.531             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.074             
[2024-05-30 11:34:15,694][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/2>
[2024-05-30 11:34:15,694][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:34:15,698][HYDRA] 	#113 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:34:15,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:34:15,981][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:34:15,983][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:34:15,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:34:15,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:34:15,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:34:15,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:34:15,989][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:34:15,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:34:15,992][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:34:15,992][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:34:15,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:34:16,035][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.18it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.545 train/f1:   
                                                              0.638             
                                                              train/precision:  
                                                              0.529             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.075             
[2024-05-30 11:34:39,196][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/3>
[2024-05-30 11:34:39,196][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:34:39,199][HYDRA] 	#114 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:34:39,486][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:34:39,488][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:34:39,490][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:34:39,491][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:34:39,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:34:39,495][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:34:39,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:34:39,497][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:34:39,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:34:39,500][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:34:39,501][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:34:39,503][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:34:39,583][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.61it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.070 train/auc:  
                                                              0.571 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.545             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.073             
[2024-05-30 11:35:02,626][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/4>
[2024-05-30 11:35:02,627][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:35:02,630][HYDRA] 	#115 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:35:02,921][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:35:02,923][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:35:02,925][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:35:02,926][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:35:02,929][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:35:02,930][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:35:02,931][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:35:02,931][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:35:02,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:35:02,934][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:35:02,935][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:35:02,937][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:35:02,981][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.94it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.580 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.552             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.074             
[2024-05-30 11:35:26,259][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/5>
[2024-05-30 11:35:26,260][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:35:26,262][HYDRA] 	#116 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:35:26,551][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:35:26,553][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:35:26,555][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:35:26,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:35:26,559][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:35:26,560][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:35:26,561][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:35:26,562][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:35:26,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:35:26,566][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:35:26,566][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:35:26,568][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:35:26,613][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.00it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.527 train/f1:   
                                                              0.569             
                                                              train/precision:  
                                                              0.522             
                                                              train/recall:     
                                                              0.625 train/mre:  
                                                              0.082             
[2024-05-30 11:35:49,458][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/6>
[2024-05-30 11:35:49,459][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:35:49,462][HYDRA] 	#117 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:35:49,751][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:35:49,753][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:35:49,756][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:35:49,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:35:49,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:35:49,760][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:35:49,761][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:35:49,762][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:35:49,763][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:35:49,765][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:35:49,765][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:35:49,768][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:35:49,848][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.89it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.634 train/f1:   
                                                              0.650             
                                                              train/precision:  
                                                              0.623             
                                                              train/recall:     
                                                              0.679 train/mre:  
                                                              0.071             
[2024-05-30 11:36:12,807][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/7>
[2024-05-30 11:36:12,808][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:36:12,811][HYDRA] 	#118 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:36:13,093][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:36:13,095][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:36:13,098][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:36:13,098][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:36:13,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:36:13,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:36:13,103][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:36:13,104][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:36:13,106][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:36:13,107][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:36:13,107][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:36:13,109][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:36:13,151][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.43it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.545 train/f1:   
                                                              0.571             
                                                              train/precision:  
                                                              0.540             
                                                              train/recall:     
                                                              0.607 train/mre:  
                                                              0.068             
[2024-05-30 11:36:36,087][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/8>
[2024-05-30 11:36:36,088][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:36:36,091][HYDRA] 	#119 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:36:36,378][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:36:36,380][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:36:36,383][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:36:36,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:36:36,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:36:36,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:36:36,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:36:36,389][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:36:36,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:36:36,393][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:36:36,393][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:36:36,395][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:36:36,441][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.48it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.625 train/f1:   
                                                              0.696             
                                                              train/precision:  
                                                              0.585             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.077             
[2024-05-30 11:36:59,390][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/9>
[2024-05-30 11:36:59,391][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:36:59,394][HYDRA] 	#120 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:36:59,829][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:36:59,832][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:36:59,834][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:36:59,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:36:59,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:36:59,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:36:59,840][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:36:59,841][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:36:59,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:36:59,844][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:36:59,844][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:36:59,846][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:37:00,092][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.51it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.062 train/auc:  
                                                              0.625 train/f1:   
                                                              0.682             
                                                              train/precision:  
                                                              0.592             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.069             
[2024-05-30 11:37:22,922][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/0>
[2024-05-30 11:37:22,923][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:37:22,926][HYDRA] 	#121 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:37:23,214][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:37:23,217][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:37:23,219][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:37:23,219][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:37:23,223][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:37:23,224][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:37:23,224][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:37:23,225][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:37:23,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:37:23,230][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:37:23,231][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:37:23,233][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:37:23,292][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.64it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.554 train/f1:   
                                                              0.632             
                                                              train/precision:  
                                                              0.538             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.074             
[2024-05-30 11:37:46,039][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/1>
[2024-05-30 11:37:46,040][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:37:46,043][HYDRA] 	#122 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:37:46,327][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:37:46,330][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:37:46,332][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:37:46,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:37:46,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:37:46,336][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:37:46,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:37:46,338][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:37:46,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:37:46,343][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:37:46,344][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:37:46,346][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:37:46,391][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.84it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.616 train/f1:   
                                                              0.699             
                                                              train/precision:  
                                                              0.575             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.073             
[2024-05-30 11:38:09,636][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/2>
[2024-05-30 11:38:09,637][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:38:09,640][HYDRA] 	#123 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:38:09,964][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:38:09,966][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:38:09,969][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:38:09,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:38:09,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:38:09,975][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:38:09,976][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:38:09,977][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:38:09,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:38:09,980][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:38:09,980][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:38:09,983][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:38:10,143][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.09it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.589 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.562             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.073             
[2024-05-30 11:38:34,004][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/3>
[2024-05-30 11:38:34,005][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:38:34,008][HYDRA] 	#124 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:38:34,293][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:38:34,295][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:38:34,297][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:38:34,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:38:34,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:38:34,302][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:38:34,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:38:34,304][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:38:34,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:38:34,306][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:38:34,307][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:38:34,309][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:38:34,351][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.68it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.069 train/auc:  
                                                              0.571 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.544             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.073             
[2024-05-30 11:38:57,597][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/4>
[2024-05-30 11:38:57,598][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:38:57,601][HYDRA] 	#125 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:38:57,895][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:38:57,898][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:38:57,900][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:38:57,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:38:57,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:38:57,904][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:38:57,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:38:57,906][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:38:57,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:38:57,910][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:38:57,910][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:38:57,914][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:38:57,958][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.23it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.598 train/f1:   
                                                              0.676             
                                                              train/precision:  
                                                              0.566             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.079             
[2024-05-30 11:39:21,212][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/5>
[2024-05-30 11:39:21,213][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:39:21,216][HYDRA] 	#126 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:39:21,497][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:39:21,500][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:39:21,502][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:39:21,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:39:21,506][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:39:21,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:39:21,507][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:39:21,508][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:39:21,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:39:21,510][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:39:21,511][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:39:21,513][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:39:21,554][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.29it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.500 train/f1:   
                                                              0.541             
                                                              train/precision:  
                                                              0.500             
                                                              train/recall:     
                                                              0.589 train/mre:  
                                                              0.079             
[2024-05-30 11:39:44,899][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/6>
[2024-05-30 11:39:44,900][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:39:44,902][HYDRA] 	#127 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:39:45,185][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:39:45,188][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:39:45,190][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:39:45,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:39:45,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:39:45,195][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:39:45,195][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:39:45,196][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:39:45,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:39:45,199][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:39:45,199][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:39:45,203][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:39:45,245][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.87it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.589 train/f1:   
                                                              0.635             
                                                              train/precision:  
                                                              0.571             
                                                              train/recall:     
                                                              0.714 train/mre:  
                                                              0.072             
[2024-05-30 11:40:08,061][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/7>
[2024-05-30 11:40:08,061][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:40:08,064][HYDRA] 	#128 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:40:08,348][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:40:08,350][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:40:08,352][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:40:08,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:40:08,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:40:08,357][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:40:08,358][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:40:08,359][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:40:08,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:40:08,361][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:40:08,361][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:40:08,364][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:40:08,407][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.47it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.554 train/f1:   
                                                              0.609             
                                                              train/precision:  
                                                              0.542             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.071             
[2024-05-30 11:40:31,377][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/8>
[2024-05-30 11:40:31,378][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:40:31,380][HYDRA] 	#129 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:40:31,676][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:40:31,678][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:40:31,680][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:40:31,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:40:31,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:40:31,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:40:31,686][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:40:31,687][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:40:31,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:40:31,692][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:40:31,692][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:40:31,696][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:40:31,779][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.28it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.616 train/f1:   
                                                              0.686             
                                                              train/precision:  
                                                              0.580             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.077             
[2024-05-30 11:40:54,792][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/9>
[2024-05-30 11:40:54,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:40:54,796][HYDRA] 	#130 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:40:55,082][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:40:55,085][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:40:55,087][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:40:55,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:40:55,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:40:55,092][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:40:55,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:40:55,093][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:40:55,095][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:40:55,096][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:40:55,097][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:40:55,099][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:40:55,142][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.46it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.643 train/f1:   
                                                              0.701             
                                                              train/precision:  
                                                              0.603             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.069             
[2024-05-30 11:41:18,285][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/0>
[2024-05-30 11:41:18,286][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:41:18,288][HYDRA] 	#131 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:41:18,569][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:41:18,572][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:41:18,574][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:41:18,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:41:18,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:41:18,578][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:41:18,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:41:18,580][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:41:18,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:41:18,582][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:41:18,583][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:41:18,587][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:41:18,641][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.36it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.062 train/auc:  
                                                              0.598 train/f1:   
                                                              0.656             
                                                              train/precision:  
                                                              0.573             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-05-30 11:41:41,906][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/1>
[2024-05-30 11:41:41,907][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:41:41,911][HYDRA] 	#132 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:41:42,243][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:41:42,247][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:41:42,250][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:41:42,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:41:42,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:41:42,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:41:42,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:41:42,256][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:41:42,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:41:42,260][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:41:42,260][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:41:42,262][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:41:42,398][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.70it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.067 train/auc:  
                                                              0.598 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.568             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.073             
[2024-05-30 11:42:06,169][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/2>
[2024-05-30 11:42:06,170][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:42:06,172][HYDRA] 	#133 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:42:06,455][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:42:06,457][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:42:06,459][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:42:06,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:42:06,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:42:06,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:42:06,465][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:42:06,465][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:42:06,467][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:42:06,469][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:42:06,470][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:42:06,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:42:06,518][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.78it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.598 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.570             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.071             
[2024-05-30 11:42:29,392][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/3>
[2024-05-30 11:42:29,393][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:42:29,396][HYDRA] 	#134 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:42:29,679][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:42:29,681][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:42:29,684][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:42:29,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:42:29,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:42:29,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:42:29,689][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:42:29,690][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:42:29,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:42:29,692][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:42:29,693][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:42:29,695][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:42:29,738][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.43it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.074 train/auc:  
                                                              0.571 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.545             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.074             
[2024-05-30 11:42:53,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/4>
[2024-05-30 11:42:53,048][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:42:53,050][HYDRA] 	#135 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:42:53,331][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:42:53,333][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:42:53,335][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:42:53,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:42:53,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:42:53,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:42:53,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:42:53,341][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:42:53,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:42:53,344][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:42:53,344][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:42:53,347][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:42:53,389][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.88it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.616 train/f1:   
                                                              0.699             
                                                              train/precision:  
                                                              0.575             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.075             
[2024-05-30 11:43:16,413][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/5>
[2024-05-30 11:43:16,414][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:43:16,417][HYDRA] 	#136 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:43:16,703][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:43:16,706][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:43:16,708][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:43:16,708][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:43:16,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:43:16,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:43:16,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:43:16,714][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:43:16,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:43:16,719][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:43:16,719][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:43:16,721][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:43:16,765][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.42it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.070 train/auc:  
                                                              0.562 train/f1:   
                                                              0.642             
                                                              train/precision:  
                                                              0.543             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.079             
[2024-05-30 11:43:39,858][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/6>
[2024-05-30 11:43:39,859][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:43:39,862][HYDRA] 	#137 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:43:40,139][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:43:40,142][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:43:40,144][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:43:40,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:43:40,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:43:40,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:43:40,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:43:40,150][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:43:40,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:43:40,152][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:43:40,153][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:43:40,156][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:43:40,553][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.02it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.580 train/f1:   
                                                              0.624             
                                                              train/precision:  
                                                              0.565             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.072             
[2024-05-30 11:44:03,628][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/7>
[2024-05-30 11:44:03,629][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:44:03,639][HYDRA] 	#138 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:44:03,923][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:44:03,925][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:44:03,928][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:44:03,929][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:44:03,932][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:44:03,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:44:03,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:44:03,935][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:44:03,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:44:03,939][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:44:03,939][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:44:03,941][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:44:04,022][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.98it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.580 train/f1:   
                                                              0.636             
                                                              train/precision:  
                                                              0.562             
                                                              train/recall:     
                                                              0.732 train/mre:  
                                                              0.071             
[2024-05-30 11:44:27,583][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/8>
[2024-05-30 11:44:27,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:44:27,586][HYDRA] 	#139 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:44:27,880][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:44:27,883][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:44:27,885][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:44:27,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:44:27,888][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:44:27,889][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:44:27,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:44:27,891][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:44:27,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:44:27,895][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:44:27,895][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:44:27,897][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:44:27,941][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.96it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.661 train/f1:   
                                                              0.725             
                                                              train/precision:  
                                                              0.610             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.076             
[2024-05-30 11:44:51,427][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/9>
[2024-05-30 11:44:51,428][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:44:51,433][HYDRA] 	#140 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:44:51,740][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:44:51,742][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:44:51,745][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:44:51,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:44:51,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:44:51,749][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:44:51,750][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:44:51,751][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:44:51,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:44:51,754][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:44:51,754][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:44:51,756][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:44:51,799][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.57it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.879  
                                                              train/f1: 0.874   
                                                              train/precision:  
                                                              0.912             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              nan               
[2024-05-30 11:45:18,593][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/0>
[2024-05-30 11:45:18,594][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:45:18,597][HYDRA] 	#141 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:45:18,884][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:45:18,887][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:45:18,889][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:45:18,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:45:18,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:45:18,894][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:45:18,894][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:45:18,895][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:45:18,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:45:18,899][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:45:18,899][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:45:18,902][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:45:18,985][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.62it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.742  
                                                              train/f1: 0.754   
                                                              train/precision:  
                                                              0.721             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              nan               
[2024-05-30 11:45:44,281][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/1>
[2024-05-30 11:45:44,282][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:45:44,284][HYDRA] 	#142 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:45:44,573][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:45:44,576][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:45:44,578][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:45:44,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:45:44,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:45:44,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:45:44,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:45:44,584][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:45:44,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:45:44,588][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:45:44,589][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:45:44,591][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:45:44,638][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.51it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.726  
                                                              train/f1: 0.773   
                                                              train/precision:  
                                                              0.659             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              nan               
[2024-05-30 11:46:09,699][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/2>
[2024-05-30 11:46:09,700][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:46:09,703][HYDRA] 	#143 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:46:09,985][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:46:09,987][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:46:09,989][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:46:09,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:46:09,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:46:09,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:46:09,995][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:46:09,995][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:46:09,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:46:09,998][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:46:09,998][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:46:10,000][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:46:10,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.96it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.758  
                                                              train/f1: 0.797   
                                                              train/precision:  
                                                              0.686             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 11:46:35,151][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/3>
[2024-05-30 11:46:35,152][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:46:35,154][HYDRA] 	#144 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:46:35,440][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:46:35,443][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:46:35,445][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:46:35,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:46:35,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:46:35,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:46:35,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:46:35,451][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:46:35,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:46:35,455][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:46:35,455][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:46:35,458][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:46:35,539][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.76it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.742  
                                                              train/f1: 0.765   
                                                              train/precision:  
                                                              0.703             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              nan               
[2024-05-30 11:47:00,693][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/4>
[2024-05-30 11:47:00,693][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:47:00,696][HYDRA] 	#145 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:47:00,985][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:47:00,987][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:47:00,989][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:47:00,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:47:00,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:47:00,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:47:00,995][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:47:00,996][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:47:00,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:47:01,003][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:47:01,003][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:47:01,006][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:47:01,053][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.60it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre: nan
                                                              train/auc: 0.718  
                                                              train/f1: 0.755   
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              nan               
[2024-05-30 11:47:26,320][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/5>
[2024-05-30 11:47:26,320][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:47:26,323][HYDRA] 	#146 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:47:26,608][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:47:26,610][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:47:26,613][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:47:26,613][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:47:26,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:47:26,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:47:26,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:47:26,619][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:47:26,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:47:26,621][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:47:26,622][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:47:26,624][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:47:26,667][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.25it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.774  
                                                              train/f1: 0.774   
                                                              train/precision:  
                                                              0.774             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              nan               
[2024-05-30 11:47:51,920][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/6>
[2024-05-30 11:47:51,921][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:47:51,927][HYDRA] 	#147 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:47:52,235][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:47:52,238][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:47:52,241][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:47:52,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:47:52,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:47:52,246][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:47:52,247][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:47:52,248][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:47:52,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:47:52,252][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:47:52,252][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:47:52,254][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:47:52,402][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.06it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.823  
                                                              train/f1: 0.841   
                                                              train/precision:  
                                                              0.763             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              nan               
[2024-05-30 11:48:17,395][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/7>
[2024-05-30 11:48:17,396][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:48:17,399][HYDRA] 	#148 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:48:17,683][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:48:17,686][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:48:17,688][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:48:17,689][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:48:17,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:48:17,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:48:17,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:48:17,694][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:48:17,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:48:17,698][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:48:17,699][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:48:17,701][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:48:17,746][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.10it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.726  
                                                              train/f1: 0.776   
                                                              train/precision:  
                                                              0.656             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 11:48:43,363][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/8>
[2024-05-30 11:48:43,364][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:48:43,366][HYDRA] 	#149 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:48:43,651][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:48:43,654][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:48:43,656][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:48:43,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:48:43,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:48:43,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:48:43,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:48:43,662][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:48:43,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:48:43,664][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:48:43,665][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:48:43,667][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:48:43,708][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.15it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.798  
                                                              train/f1: 0.825   
                                                              train/precision:  
                                                              0.728             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 11:49:09,031][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/9>
[2024-05-30 11:49:09,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:49:09,034][HYDRA] 	#150 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:49:09,316][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:49:09,318][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:49:09,320][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:49:09,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:49:09,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:49:09,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:49:09,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:49:09,326][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:49:09,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:49:09,329][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:49:09,329][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:49:09,332][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:49:09,373][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.60it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre:    
                                                              0.112 train/auc:  
                                                              0.871 train/f1:   
                                                              0.879             
                                                              train/precision:  
                                                              0.829             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.128             
[2024-05-30 11:49:35,066][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/0>
[2024-05-30 11:49:35,067][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:49:35,069][HYDRA] 	#151 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:49:35,355][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:49:35,357][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:49:35,359][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:49:35,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:49:35,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:49:35,364][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:49:35,365][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:49:35,366][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:49:35,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:49:35,370][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:49:35,370][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:49:35,372][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:49:35,417][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.15it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              0.600 val/mre:    
                                                              0.111 train/auc:  
                                                              0.710 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.659             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.126             
[2024-05-30 11:50:00,513][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/1>
[2024-05-30 11:50:00,513][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:50:00,516][HYDRA] 	#152 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:50:00,798][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:50:00,800][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:50:00,803][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:50:00,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:50:00,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:50:00,807][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:50:00,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:50:00,809][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:50:00,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:50:00,811][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:50:00,812][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:50:00,814][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:50:00,855][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.10it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.097 train/auc:  
                                                              0.798 train/f1:   
                                                              0.803             
                                                              train/precision:  
                                                              0.785             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.119             
[2024-05-30 11:50:26,243][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/2>
[2024-05-30 11:50:26,244][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:50:26,247][HYDRA] 	#153 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:50:26,535][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:50:26,537][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:50:26,539][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:50:26,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:50:26,543][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:50:26,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:50:26,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:50:26,545][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:50:26,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:50:26,549][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:50:26,549][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:50:26,552][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:50:26,637][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.64it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.750     
                                                              val/precision:    
                                                              0.643 val/recall: 
                                                              0.900 val/mre:    
                                                              0.100 train/auc:  
                                                              0.710 train/f1:   
                                                              0.746             
                                                              train/precision:  
                                                              0.663             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.124             
[2024-05-30 11:50:52,180][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/3>
[2024-05-30 11:50:52,181][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:50:52,184][HYDRA] 	#154 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:50:52,476][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:50:52,479][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:50:52,481][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:50:52,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:50:52,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:50:52,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:50:52,487][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:50:52,487][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:50:52,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:50:52,492][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:50:52,492][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:50:52,494][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:50:52,541][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.97it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.100 train/auc:  
                                                              0.702 train/f1:   
                                                              0.704             
                                                              train/precision:  
                                                              0.698             
                                                              train/recall:     
                                                              0.710 train/mre:  
                                                              0.120             
[2024-05-30 11:51:17,994][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/4>
[2024-05-30 11:51:17,995][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:51:17,997][HYDRA] 	#155 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:51:18,284][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:51:18,286][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:51:18,288][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:51:18,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:51:18,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:51:18,293][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:51:18,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:51:18,294][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:51:18,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:51:18,297][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:51:18,297][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:51:18,299][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:51:18,341][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.15it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre:    
                                                              0.103 train/auc:  
                                                              0.702 train/f1:   
                                                              0.713             
                                                              train/precision:  
                                                              0.687             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.129             
[2024-05-30 11:51:43,467][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/5>
[2024-05-30 11:51:43,469][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:51:43,474][HYDRA] 	#156 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:51:43,793][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:51:43,795][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:51:43,798][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:51:43,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:51:43,802][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:51:43,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:51:43,803][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:51:43,804][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:51:43,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:51:43,806][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:51:43,807][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:51:43,809][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:51:43,855][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.93it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.103 train/auc:  
                                                              0.815 train/f1:   
                                                              0.835             
                                                              train/precision:  
                                                              0.753             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.122             
[2024-05-30 11:52:09,895][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/6>
[2024-05-30 11:52:09,897][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:52:09,912][HYDRA] 	#157 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:52:10,343][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:52:10,346][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:52:10,349][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:52:10,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:52:10,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:52:10,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:52:10,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:52:10,355][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:52:10,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:52:10,358][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:52:10,359][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:52:10,361][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:52:10,408][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.23it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.114 train/auc:  
                                                              0.750 train/f1:   
                                                              0.760             
                                                              train/precision:  
                                                              0.731             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.130             
[2024-05-30 11:52:35,453][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/7>
[2024-05-30 11:52:35,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:52:35,456][HYDRA] 	#158 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:52:35,746][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:52:35,749][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:52:35,752][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:52:35,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:52:35,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:52:35,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:52:35,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:52:35,759][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:52:35,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:52:35,763][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:52:35,763][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:52:35,765][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:52:35,856][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.71it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.692     
                                                              val/precision:    
                                                              0.562 val/recall: 
                                                              0.900 val/mre:    
                                                              0.104 train/auc:  
                                                              0.637 train/f1:   
                                                              0.615             
                                                              train/precision:  
                                                              0.655             
                                                              train/recall:     
                                                              0.581 train/mre:  
                                                              0.115             
[2024-05-30 11:53:01,465][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/8>
[2024-05-30 11:53:01,465][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:53:01,468][HYDRA] 	#159 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:53:01,788][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:53:01,790][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:53:01,793][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:53:01,793][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:53:01,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:53:01,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:53:01,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:53:01,799][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:53:01,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:53:01,802][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:53:01,802][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:53:01,804][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:53:01,865][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.84it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.107 train/auc:  
                                                              0.766 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.699             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.127             
[2024-05-30 11:53:27,547][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/9>
[2024-05-30 11:53:27,550][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:53:27,565][HYDRA] 	#160 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:53:27,876][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:53:27,878][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:53:27,880][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:53:27,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:53:27,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:53:27,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:53:27,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:53:27,887][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:53:27,888][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:53:27,889][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:53:27,890][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:53:27,892][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:53:27,933][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.03it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.112 train/auc:  
                                                              0.855 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.130             
[2024-05-30 11:53:53,306][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/0>
[2024-05-30 11:53:53,307][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:53:53,309][HYDRA] 	#161 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:53:53,602][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:53:53,604][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:53:53,606][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:53:53,607][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:53:53,610][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:53:53,611][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:53:53,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:53:53,612][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:53:53,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:53:53,616][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:53:53,616][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:53:53,618][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:53:53,699][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.64it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.107 train/auc:  
                                                              0.750 train/f1:   
                                                              0.748             
                                                              train/precision:  
                                                              0.754             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.121             
[2024-05-30 11:54:18,937][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/1>
[2024-05-30 11:54:18,937][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:54:18,941][HYDRA] 	#162 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:54:19,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:54:19,229][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:54:19,231][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:54:19,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:54:19,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:54:19,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:54:19,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:54:19,237][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:54:19,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:54:19,242][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:54:19,242][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:54:19,244][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:54:19,291][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.17it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.101 train/auc:  
                                                              0.887 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.120             
[2024-05-30 11:54:44,505][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/2>
[2024-05-30 11:54:44,505][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:54:44,508][HYDRA] 	#163 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:54:44,790][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:54:44,793][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:54:44,795][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:54:44,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:54:44,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:54:44,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:54:44,800][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:54:44,801][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:54:44,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:54:44,803][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:54:44,804][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:54:44,806][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:54:44,848][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.68it/s v_num: 0.000      
                                                              val/auc: 0.800    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.875 val/recall: 
                                                              0.700 val/mre:    
                                                              0.105 train/auc:  
                                                              0.839 train/f1:   
                                                              0.848             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.126             
[2024-05-30 11:55:10,279][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/3>
[2024-05-30 11:55:10,280][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:55:10,283][HYDRA] 	#164 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:55:10,560][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:55:10,562][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:55:10,564][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:55:10,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:55:10,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:55:10,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:55:10,570][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:55:10,570][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:55:10,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:55:10,573][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:55:10,573][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:55:10,576][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:55:10,617][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.54it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.100 train/auc:  
                                                              0.669 train/f1:   
                                                              0.692             
                                                              train/precision:  
                                                              0.648             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.117             
[2024-05-30 11:55:37,246][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/4>
[2024-05-30 11:55:37,247][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:55:37,250][HYDRA] 	#165 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:55:37,540][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:55:37,542][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:55:37,544][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:55:37,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:55:37,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:55:37,549][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:55:37,550][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:55:37,551][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:55:37,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:55:37,555][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:55:37,555][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:55:37,557][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:55:37,602][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.51it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre:    
                                                              0.103 train/auc:  
                                                              0.831 train/f1:   
                                                              0.840             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.127             
[2024-05-30 11:56:02,953][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/5>
[2024-05-30 11:56:02,953][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:56:02,956][HYDRA] 	#166 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:56:03,252][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:56:03,255][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:56:03,257][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:56:03,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:56:03,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:56:03,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:56:03,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:56:03,263][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:56:03,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:56:03,267][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:56:03,267][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:56:03,269][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:56:03,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.72it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre:    
                                                              0.112 train/auc:  
                                                              0.847 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.124             
[2024-05-30 11:56:28,694][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/6>
[2024-05-30 11:56:28,695][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:56:28,698][HYDRA] 	#167 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:56:28,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:56:28,981][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:56:28,983][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:56:28,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:56:28,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:56:28,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:56:28,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:56:28,990][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:56:28,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:56:28,992][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:56:28,993][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:56:28,995][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:56:29,036][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.66it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.098 train/auc:  
                                                              0.758 train/f1:   
                                                              0.776             
                                                              train/precision:  
                                                              0.722             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.117             
[2024-05-30 11:56:54,044][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/7>
[2024-05-30 11:56:54,044][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:56:54,047][HYDRA] 	#168 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:56:54,327][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:56:54,329][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:56:54,331][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:56:54,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:56:54,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:56:54,336][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:56:54,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:56:54,337][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:56:54,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:56:54,340][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:56:54,340][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:56:54,342][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:56:54,385][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.20it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.692     
                                                              val/precision:    
                                                              0.562 val/recall: 
                                                              0.900 val/mre:    
                                                              0.105 train/auc:  
                                                              0.637 train/f1:   
                                                              0.628             
                                                              train/precision:  
                                                              0.644             
                                                              train/recall:     
                                                              0.613 train/mre:  
                                                              0.116             
[2024-05-30 11:57:19,617][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/8>
[2024-05-30 11:57:19,618][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:57:19,621][HYDRA] 	#169 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:57:19,912][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:57:19,914][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:57:19,916][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:57:19,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:57:19,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:57:19,921][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:57:19,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:57:19,923][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:57:19,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:57:19,926][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:57:19,926][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:57:19,929][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:57:20,110][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.49it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.111 train/auc:  
                                                              0.831 train/f1:   
                                                              0.844             
                                                              train/precision:  
                                                              0.781             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.127             
[2024-05-30 11:57:45,520][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/9>
[2024-05-30 11:57:45,520][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:57:45,523][HYDRA] 	#170 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:57:45,819][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:57:45,822][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:57:45,824][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:57:45,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:57:45,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:57:45,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:57:45,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:57:45,830][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:57:45,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:57:45,835][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:57:45,835][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:57:45,837][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:57:45,882][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.58it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.105 train/auc:  
                                                              0.903 train/f1:   
                                                              0.908             
                                                              train/precision:  
                                                              0.868             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.124             
[2024-05-30 11:58:11,645][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/0>
[2024-05-30 11:58:11,646][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:58:11,650][HYDRA] 	#171 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:58:11,934][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:58:11,936][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:58:11,938][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:58:11,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:58:11,942][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:58:11,943][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:58:11,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:58:11,944][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:58:11,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:58:11,947][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:58:11,947][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:58:11,950][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:58:11,991][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.55it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.106 train/auc:  
                                                              0.815 train/f1:   
                                                              0.803             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.122             
[2024-05-30 11:58:37,638][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/1>
[2024-05-30 11:58:37,639][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:58:37,642][HYDRA] 	#172 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:58:37,928][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:58:37,930][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:58:37,933][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:58:37,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:58:37,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:58:37,937][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:58:37,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:58:37,939][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:58:37,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:58:37,941][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:58:37,942][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:58:37,944][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:58:37,985][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.27it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.116 train/auc:  
                                                              0.871 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.125             
[2024-05-30 11:59:03,326][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/2>
[2024-05-30 11:59:03,327][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:59:03,329][HYDRA] 	#173 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:59:03,611][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:59:03,614][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:59:03,616][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:59:03,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:59:03,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:59:03,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:59:03,622][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:59:03,623][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:59:03,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:59:03,627][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:59:03,627][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:59:03,629][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:59:03,673][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.08it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.101 train/auc:  
                                                              0.839 train/f1:   
                                                              0.841             
                                                              train/precision:  
                                                              0.828             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.124             
[2024-05-30 11:59:29,103][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/3>
[2024-05-30 11:59:29,104][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:59:29,107][HYDRA] 	#174 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:59:29,388][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:59:29,390][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:59:29,392][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:59:29,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:59:29,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:59:29,397][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:59:29,398][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:59:29,398][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:59:29,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:59:29,402][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:59:29,402][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:59:29,404][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:59:29,490][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.37it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.105 train/auc:  
                                                              0.710 train/f1:   
                                                              0.739             
                                                              train/precision:  
                                                              0.671             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.118             
[2024-05-30 11:59:54,923][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/4>
[2024-05-30 11:59:54,923][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:59:54,926][HYDRA] 	#175 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:59:55,208][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:59:55,211][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 11:59:55,213][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:59:55,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:59:55,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:59:55,218][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:59:55,218][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:59:55,219][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:59:55,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:59:55,222][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:59:55,222][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:59:55,224][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:59:55,267][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.67it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre:    
                                                              0.107 train/auc:  
                                                              0.782 train/f1:   
                                                              0.797             
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.137             
[2024-05-30 12:00:20,393][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/5>
[2024-05-30 12:00:20,393][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:00:20,396][HYDRA] 	#176 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:00:20,676][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:00:20,678][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:00:20,680][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:00:20,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:00:20,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:00:20,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:00:20,686][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:00:20,686][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:00:20,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:00:20,689][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:00:20,689][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:00:20,691][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:00:20,732][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.13it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.104 train/auc:  
                                                              0.903 train/f1:   
                                                              0.908             
                                                              train/precision:  
                                                              0.868             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.126             
[2024-05-30 12:00:45,881][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/6>
[2024-05-30 12:00:45,882][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:00:45,884][HYDRA] 	#177 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:00:46,167][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:00:46,169][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:00:46,172][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:00:46,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:00:46,175][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:00:46,176][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:00:46,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:00:46,178][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:00:46,179][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:00:46,182][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:00:46,182][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:00:46,185][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:00:46,267][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.91it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.105 train/auc:  
                                                              0.750 train/f1:   
                                                              0.786             
                                                              train/precision:  
                                                              0.687             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.118             
[2024-05-30 12:01:11,346][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/7>
[2024-05-30 12:01:11,346][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:01:11,349][HYDRA] 	#178 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:01:11,635][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:01:11,637][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:01:11,639][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:01:11,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:01:11,643][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:01:11,644][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:01:11,645][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:01:11,645][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:01:11,647][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:01:11,649][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:01:11,650][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:01:11,652][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:01:11,696][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.63it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.104 train/auc:  
                                                              0.653 train/f1:   
                                                              0.613             
                                                              train/precision:  
                                                              0.694             
                                                              train/recall:     
                                                              0.548 train/mre:  
                                                              0.119             
[2024-05-30 12:01:37,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/8>
[2024-05-30 12:01:37,236][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:01:37,241][HYDRA] 	#179 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:01:37,519][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:01:37,522][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:01:37,524][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:01:37,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:01:37,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:01:37,529][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:01:37,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:01:37,530][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:01:37,532][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:01:37,533][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:01:37,534][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:01:37,536][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:01:37,577][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.70it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.113 train/auc:  
                                                              0.774 train/f1:   
                                                              0.763             
                                                              train/precision:  
                                                              0.804             
                                                              train/recall:     
                                                              0.726 train/mre:  
                                                              0.132             
[2024-05-30 12:02:02,645][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/9>
[2024-05-30 12:02:02,646][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:02:02,648][HYDRA] 	#180 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:02:02,931][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:02:02,934][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:02:02,936][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:02:02,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:02:02,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:02:02,941][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:02:02,942][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:02:02,942][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:02:02,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:02:02,945][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:02:02,945][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:02:02,947][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:02:02,988][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.81it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.104 train/auc:  
                                                              0.887 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.843             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.122             
[2024-05-30 12:02:28,256][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/0>
[2024-05-30 12:02:28,257][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:02:28,260][HYDRA] 	#181 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:02:28,773][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:02:28,778][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:02:28,783][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:02:28,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:02:28,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:02:28,789][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:02:28,790][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:02:28,791][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:02:28,793][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:02:28,795][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:02:28,795][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:02:28,798][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:02:28,981][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.16it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.107 train/auc:  
                                                              0.782 train/f1:   
                                                              0.784             
                                                              train/precision:  
                                                              0.778             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.124             
[2024-05-30 12:02:54,246][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/1>
[2024-05-30 12:02:54,246][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:02:54,249][HYDRA] 	#182 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:02:54,532][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:02:54,534][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:02:54,536][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:02:54,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:02:54,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:02:54,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:02:54,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:02:54,542][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:02:54,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:02:54,546][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:02:54,546][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:02:54,548][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:02:54,643][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.94it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.111 train/auc:  
                                                              0.887 train/f1:   
                                                              0.891             
                                                              train/precision:  
                                                              0.864             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.128             
[2024-05-30 12:03:20,097][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/2>
[2024-05-30 12:03:20,098][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:03:20,101][HYDRA] 	#183 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:03:20,384][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:03:20,387][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:03:20,390][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:03:20,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:03:20,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:03:20,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:03:20,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:03:20,396][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:03:20,398][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:03:20,400][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:03:20,400][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:03:20,402][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:03:20,444][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.48it/s v_num: 0.000      
                                                              val/auc: 0.800    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.875 val/recall: 
                                                              0.700 val/mre:    
                                                              0.100 train/auc:  
                                                              0.879 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.119             
[2024-05-30 12:03:45,723][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/3>
[2024-05-30 12:03:45,724][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:03:45,726][HYDRA] 	#184 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:03:46,006][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:03:46,009][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:03:46,011][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:03:46,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:03:46,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:03:46,015][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:03:46,016][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:03:46,017][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:03:46,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:03:46,019][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:03:46,020][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:03:46,024][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:03:46,064][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.52it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.102 train/auc:  
                                                              0.710 train/f1:   
                                                              0.719             
                                                              train/precision:  
                                                              0.697             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.119             
[2024-05-30 12:04:11,234][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/4>
[2024-05-30 12:04:11,235][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:04:11,238][HYDRA] 	#185 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:04:11,522][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:04:11,524][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:04:11,526][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:04:11,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:04:11,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:04:11,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:04:11,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:04:11,532][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:04:11,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:04:11,535][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:04:11,535][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:04:11,538][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:04:11,579][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.93it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.500 val/mre:    
                                                              0.121 train/auc:  
                                                              0.911 train/f1:   
                                                              0.913             
                                                              train/precision:  
                                                              0.892             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.128             
[2024-05-30 12:04:36,779][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/5>
[2024-05-30 12:04:36,780][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:04:36,783][HYDRA] 	#186 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:04:37,076][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:04:37,079][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:04:37,082][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:04:37,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:04:37,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:04:37,087][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:04:37,088][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:04:37,089][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:04:37,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:04:37,093][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:04:37,093][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:04:37,095][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:04:37,140][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.22it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.500 val/mre:    
                                                              0.104 train/auc:  
                                                              0.863 train/f1:   
                                                              0.876             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.121             
[2024-05-30 12:05:02,133][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/6>
[2024-05-30 12:05:02,133][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:05:02,136][HYDRA] 	#187 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:05:02,421][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:05:02,424][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:05:02,426][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:05:02,426][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:05:02,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:05:02,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:05:02,431][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:05:02,432][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:05:02,434][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:05:02,434][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:05:02,435][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:05:02,437][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:05:02,479][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.12it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.720     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.900 val/mre:    
                                                              0.103 train/auc:  
                                                              0.685 train/f1:   
                                                              0.661             
                                                              train/precision:  
                                                              0.717             
                                                              train/recall:     
                                                              0.613 train/mre:  
                                                              0.117             
[2024-05-30 12:05:27,910][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/7>
[2024-05-30 12:05:27,911][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:05:27,914][HYDRA] 	#188 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:05:28,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:05:28,248][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:05:28,250][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:05:28,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:05:28,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:05:28,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:05:28,256][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:05:28,256][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:05:28,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:05:28,259][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:05:28,259][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:05:28,262][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:05:28,327][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.68it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.103 train/auc:  
                                                              0.702 train/f1:   
                                                              0.689             
                                                              train/precision:  
                                                              0.719             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.115             
[2024-05-30 12:05:53,936][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/8>
[2024-05-30 12:05:53,937][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:05:53,940][HYDRA] 	#189 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:05:54,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:05:54,229][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:05:54,232][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:05:54,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:05:54,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:05:54,237][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:05:54,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:05:54,238][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:05:54,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:05:54,244][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:05:54,245][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:05:54,247][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:05:54,292][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.19it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.110 train/auc:  
                                                              0.831 train/f1:   
                                                              0.842             
                                                              train/precision:  
                                                              0.789             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.123             
[2024-05-30 12:06:19,842][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/9>
[2024-05-30 12:06:19,843][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:06:19,846][HYDRA] 	#190 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:06:20,130][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:06:20,132][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:06:20,135][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:06:20,135][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:06:20,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:06:20,139][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:06:20,140][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:06:20,141][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:06:20,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:06:20,143][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:06:20,144][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:06:20,147][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:06:20,191][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.50it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.115 train/auc:  
                                                              0.806 train/f1:   
                                                              0.786             
                                                              train/precision:  
                                                              0.880             
                                                              train/recall:     
                                                              0.710 train/mre:  
                                                              0.126             
[2024-05-30 12:06:45,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/0>
[2024-05-30 12:06:45,650][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:06:45,653][HYDRA] 	#191 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:06:45,935][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:06:45,937][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:06:45,939][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:06:45,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:06:45,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:06:45,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:06:45,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:06:45,945][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:06:45,947][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:06:45,949][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:06:45,949][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:06:45,951][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:06:46,041][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.80it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.640     
                                                              val/precision:    
                                                              0.533 val/recall: 
                                                              0.800 val/mre:    
                                                              0.111 train/auc:  
                                                              0.847 train/f1:   
                                                              0.848             
                                                              train/precision:  
                                                              0.841             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.124             
[2024-05-30 12:07:10,976][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/1>
[2024-05-30 12:07:10,977][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:07:10,980][HYDRA] 	#192 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:07:11,256][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:07:11,258][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:07:11,261][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:07:11,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:07:11,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:07:11,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:07:11,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:07:11,266][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:07:11,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:07:11,269][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:07:11,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:07:11,271][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:07:11,313][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.02it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.105 train/auc:  
                                                              0.927 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.124             
[2024-05-30 12:07:36,341][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/2>
[2024-05-30 12:07:36,342][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:07:36,344][HYDRA] 	#193 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:07:36,623][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:07:36,625][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:07:36,628][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:07:36,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:07:36,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:07:36,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:07:36,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:07:36,634][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:07:36,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:07:36,636][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:07:36,637][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:07:36,639][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:07:36,680][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.00it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.106 train/auc:  
                                                              0.774 train/f1:   
                                                              0.808             
                                                              train/precision:  
                                                              0.702             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.125             
[2024-05-30 12:08:02,814][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/3>
[2024-05-30 12:08:02,815][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:08:02,817][HYDRA] 	#194 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:08:03,097][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:08:03,100][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:08:03,102][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:08:03,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:08:03,106][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:08:03,106][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:08:03,107][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:08:03,108][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:08:03,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:08:03,111][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:08:03,112][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:08:03,114][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:08:03,195][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.40it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.101 train/auc:  
                                                              0.621 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.593             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.111             
[2024-05-30 12:08:28,679][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/4>
[2024-05-30 12:08:28,680][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:08:28,682][HYDRA] 	#195 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:08:28,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:08:28,981][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:08:28,984][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:08:28,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:08:28,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:08:28,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:08:28,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:08:28,991][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:08:28,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:08:28,996][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:08:28,996][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:08:28,999][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:08:29,044][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.49it/s v_num: 0.000      
                                                              val/auc: 0.850    
                                                              val/f1: 0.842     
                                                              val/precision:    
                                                              0.889 val/recall: 
                                                              0.800 val/mre:    
                                                              0.108 train/auc:  
                                                              0.895 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.126             
[2024-05-30 12:08:54,168][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/5>
[2024-05-30 12:08:54,169][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:08:54,171][HYDRA] 	#196 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:08:54,453][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:08:54,455][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:08:54,458][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:08:54,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:08:54,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:08:54,462][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:08:54,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:08:54,464][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:08:54,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:08:54,466][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:08:54,467][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:08:54,470][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:08:54,511][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.57it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.112 train/auc:  
                                                              0.839 train/f1:   
                                                              0.859             
                                                              train/precision:  
                                                              0.762             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.126             
[2024-05-30 12:09:19,744][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/6>
[2024-05-30 12:09:19,745][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:09:19,748][HYDRA] 	#197 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:09:20,033][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:09:20,035][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:09:20,037][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:09:20,038][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:09:20,041][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:09:20,042][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:09:20,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:09:20,043][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:09:20,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:09:20,047][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:09:20,047][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:09:20,050][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:09:20,147][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.28it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.100 train/auc:  
                                                              0.685 train/f1:   
                                                              0.702             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.114             
[2024-05-30 12:09:45,406][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/7>
[2024-05-30 12:09:45,406][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:09:45,409][HYDRA] 	#198 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:09:45,697][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:09:45,700][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:09:45,702][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:09:45,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:09:45,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:09:45,707][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:09:45,707][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:09:45,708][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:09:45,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:09:45,713][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:09:45,713][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:09:45,715][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:09:45,759][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.57it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.103 train/auc:  
                                                              0.758 train/f1:   
                                                              0.758             
                                                              train/precision:  
                                                              0.758             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.116             
[2024-05-30 12:10:11,180][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/8>
[2024-05-30 12:10:11,181][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:10:11,185][HYDRA] 	#199 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:10:11,494][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:10:11,497][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:10:11,499][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:10:11,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:10:11,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:10:11,503][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:10:11,504][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:10:11,505][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:10:11,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:10:11,508][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:10:11,508][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:10:11,510][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:10:11,552][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.98it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.117 train/auc:  
                                                              0.903 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.129             
[2024-05-30 12:10:37,179][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/9>
[2024-05-30 12:10:37,180][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:10:37,183][HYDRA] 	#200 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:10:37,475][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:10:37,477][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:10:37,480][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:10:37,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:10:37,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:10:37,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:10:37,485][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:10:37,486][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:10:37,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:10:37,488][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:10:37,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:10:37,491][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:10:37,533][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.78it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.102 train/auc:  
                                                              0.839 train/f1:   
                                                              0.851             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.125             
[2024-05-30 12:11:02,886][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/0>
[2024-05-30 12:11:02,887][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:11:02,890][HYDRA] 	#201 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:11:03,184][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:11:03,186][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:11:03,189][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:11:03,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:11:03,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:11:03,193][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:11:03,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:11:03,195][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:11:03,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:11:03,199][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:11:03,199][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:11:03,202][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:11:03,253][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.02it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.111 train/auc:  
                                                              0.911 train/f1:   
                                                              0.915             
                                                              train/precision:  
                                                              0.881             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.120             
[2024-05-30 12:11:28,618][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/1>
[2024-05-30 12:11:28,618][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:11:28,622][HYDRA] 	#202 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:11:28,905][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:11:28,907][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:11:28,909][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:11:28,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:11:28,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:11:28,914][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:11:28,915][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:11:28,915][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:11:28,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:11:28,918][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:11:28,918][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:11:28,922][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:11:28,964][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.73it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.104 train/auc:  
                                                              0.903 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.124             
[2024-05-30 12:11:54,106][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/2>
[2024-05-30 12:11:54,107][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:11:54,110][HYDRA] 	#203 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:11:54,391][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:11:54,393][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:11:54,395][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:11:54,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:11:54,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:11:54,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:11:54,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:11:54,402][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:11:54,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:11:54,404][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:11:54,405][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:11:54,407][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:11:54,454][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.74it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.105 train/auc:  
                                                              0.903 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.120             
[2024-05-30 12:12:19,854][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/3>
[2024-05-30 12:12:19,855][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:12:19,858][HYDRA] 	#204 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:12:20,140][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:12:20,142][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:12:20,145][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:12:20,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:12:20,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:12:20,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:12:20,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:12:20,151][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:12:20,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:12:20,153][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:12:20,154][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:12:20,156][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:12:20,197][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.106 train/auc:  
                                                              0.637 train/f1:   
                                                              0.681             
                                                              train/precision:  
                                                              0.608             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.114             
[2024-05-30 12:12:45,536][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/4>
[2024-05-30 12:12:45,537][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:12:45,539][HYDRA] 	#205 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:12:45,819][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:12:45,821][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:12:45,824][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:12:45,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:12:45,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:12:45,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:12:45,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:12:45,830][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:12:45,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:12:45,833][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:12:45,833][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:12:45,836][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:12:45,915][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.48it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.107 train/auc:  
                                                              0.855 train/f1:   
                                                              0.847             
                                                              train/precision:  
                                                              0.893             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.128             
[2024-05-30 12:13:11,207][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/5>
[2024-05-30 12:13:11,208][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:13:11,211][HYDRA] 	#206 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:13:11,501][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:13:11,504][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:13:11,506][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:13:11,506][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:13:11,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:13:11,510][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:13:11,511][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:13:11,512][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:13:11,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:13:11,515][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:13:11,515][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:13:11,517][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:13:11,559][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.19it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.113 train/auc:  
                                                              0.903 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.130             
[2024-05-30 12:13:36,646][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/6>
[2024-05-30 12:13:36,647][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:13:36,650][HYDRA] 	#207 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:13:36,941][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:13:36,943][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:13:36,945][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:13:36,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:13:36,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:13:36,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:13:36,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:13:36,951][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:13:36,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:13:36,955][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:13:36,956][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:13:36,958][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:13:37,003][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.70it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.692     
                                                              val/precision:    
                                                              0.562 val/recall: 
                                                              0.900 val/mre:    
                                                              0.105 train/auc:  
                                                              0.742 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.727             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.118             
[2024-05-30 12:14:02,283][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/7>
[2024-05-30 12:14:02,284][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:14:02,287][HYDRA] 	#208 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:14:02,575][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:14:02,577][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:14:02,579][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:14:02,580][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:14:02,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:14:02,584][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:14:02,585][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:14:02,585][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:14:02,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:14:02,588][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:14:02,588][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:14:02,592][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:14:02,633][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.58it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.762     
                                                              val/precision:    
                                                              0.727 val/recall: 
                                                              0.800 val/mre:    
                                                              0.106 train/auc:  
                                                              0.742 train/f1:   
                                                              0.778             
                                                              train/precision:  
                                                              0.683             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.124             
[2024-05-30 12:14:27,694][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/8>
[2024-05-30 12:14:27,694][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:14:27,697][HYDRA] 	#209 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:14:28,390][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:14:28,393][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:14:28,395][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:14:28,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:14:28,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:14:28,399][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:14:28,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:14:28,401][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:14:28,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:14:28,403][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:14:28,404][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:14:28,406][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:14:28,447][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.90it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.110 train/auc:  
                                                              0.855 train/f1:   
                                                              0.847             
                                                              train/precision:  
                                                              0.893             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.128             
[2024-05-30 12:14:53,657][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/9>
[2024-05-30 12:14:53,657][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:14:53,660][HYDRA] 	#210 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:14:53,949][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:14:53,951][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:14:53,954][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:14:53,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:14:53,958][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:14:53,958][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:14:53,959][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:14:53,960][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:14:53,962][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:14:53,964][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:14:53,965][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:14:53,967][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:14:54,014][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.20it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.686  
                                                              train/f1: 0.661   
                                                              train/precision:  
                                                              0.720             
                                                              train/recall:     
                                                              0.610 train/mre:  
                                                              nan               
[2024-05-30 12:15:18,922][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/0>
[2024-05-30 12:15:18,923][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:15:18,925][HYDRA] 	#211 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:15:19,225][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:15:19,228][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:15:19,231][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:15:19,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:15:19,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:15:19,235][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:15:19,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:15:19,237][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:15:19,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:15:19,240][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:15:19,241][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:15:19,244][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:15:19,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.74it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.720  
                                                              train/f1: 0.637   
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.492 train/mre:  
                                                              nan               
[2024-05-30 12:15:43,930][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/1>
[2024-05-30 12:15:43,931][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:15:43,933][HYDRA] 	#212 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:15:44,219][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:15:44,221][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:15:44,223][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:15:44,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:15:44,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:15:44,228][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:15:44,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:15:44,229][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:15:44,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:15:44,232][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:15:44,232][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:15:44,235][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:15:44,277][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.33it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.686  
                                                              train/f1: 0.689   
                                                              train/precision:  
                                                              0.683             
                                                              train/recall:     
                                                              0.695 train/mre:  
                                                              nan               
[2024-05-30 12:16:08,760][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/2>
[2024-05-30 12:16:08,760][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:16:08,764][HYDRA] 	#213 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:16:09,069][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:16:09,071][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:16:09,073][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:16:09,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:16:09,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:16:09,079][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:16:09,080][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:16:09,081][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:16:09,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:16:09,084][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:16:09,084][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:16:09,086][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:16:09,207][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.05it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.771  
                                                              train/f1: 0.703   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.542 train/mre:  
                                                              nan               
[2024-05-30 12:16:33,195][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/3>
[2024-05-30 12:16:33,195][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:16:33,198][HYDRA] 	#214 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:16:33,485][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:16:33,488][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:16:33,491][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:16:33,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:16:33,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:16:33,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:16:33,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:16:33,498][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:16:33,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:16:33,501][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:16:33,502][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:16:33,504][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:16:33,588][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.73it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.822  
                                                              train/f1: 0.811   
                                                              train/precision:  
                                                              0.865             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              nan               
[2024-05-30 12:16:57,585][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/4>
[2024-05-30 12:16:57,586][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:16:57,590][HYDRA] 	#215 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:16:57,874][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:16:57,876][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:16:57,878][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:16:57,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:16:57,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:16:57,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:16:57,884][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:16:57,885][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:16:57,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:16:57,887][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:16:57,888][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:16:57,890][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:16:57,932][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.63it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.729  
                                                              train/f1: 0.750   
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              nan               
[2024-05-30 12:17:22,463][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/5>
[2024-05-30 12:17:22,465][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:17:22,480][HYDRA] 	#216 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:17:22,820][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:17:22,822][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:17:22,825][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:17:22,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:17:22,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:17:22,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:17:22,830][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:17:22,831][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:17:22,833][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:17:22,835][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:17:22,835][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:17:22,838][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:17:22,885][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.41it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.678  
                                                              train/f1: 0.612   
                                                              train/precision:  
                                                              0.769             
                                                              train/recall:     
                                                              0.508 train/mre:  
                                                              nan               
[2024-05-30 12:17:46,325][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/6>
[2024-05-30 12:17:46,326][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:17:46,329][HYDRA] 	#217 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:17:46,606][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:17:46,608][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:17:46,610][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:17:46,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:17:46,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:17:46,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:17:46,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:17:46,616][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:17:46,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:17:46,619][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:17:46,619][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:17:46,622][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:17:46,663][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.74it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              1.000 val/mre: nan
                                                              train/auc: 0.669  
                                                              train/f1: 0.711   
                                                              train/precision:  
                                                              0.632             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              nan               
[2024-05-30 12:18:10,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/7>
[2024-05-30 12:18:10,651][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:18:10,655][HYDRA] 	#218 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:18:10,940][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:18:10,943][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:18:10,945][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:18:10,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:18:10,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:18:10,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:18:10,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:18:10,951][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:18:10,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:18:10,954][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:18:10,954][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:18:10,956][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:18:10,999][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.03it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.720  
                                                              train/f1: 0.629   
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.475 train/mre:  
                                                              nan               
[2024-05-30 12:18:35,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/8>
[2024-05-30 12:18:35,113][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:18:35,116][HYDRA] 	#219 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:18:35,401][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:18:35,403][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:18:35,405][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:18:35,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:18:35,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:18:35,410][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:18:35,411][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:18:35,412][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:18:35,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:18:35,416][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:18:35,416][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:18:35,418][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:18:35,472][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.81it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.797  
                                                              train/f1: 0.803   
                                                              train/precision:  
                                                              0.778             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              nan               
[2024-05-30 12:18:59,445][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/9>
[2024-05-30 12:18:59,446][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:18:59,449][HYDRA] 	#220 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:18:59,736][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:18:59,738][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:18:59,740][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:18:59,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:18:59,744][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:18:59,745][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:18:59,746][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:18:59,746][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:18:59,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:18:59,750][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:18:59,750][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:18:59,753][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:18:59,843][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.45it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.071 train/auc:  
                                                              0.771 train/f1:   
                                                              0.791             
                                                              train/precision:  
                                                              0.729             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.081             
[2024-05-30 12:19:23,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/0>
[2024-05-30 12:19:23,489][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:19:23,501][HYDRA] 	#221 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:19:23,922][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:19:23,924][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:19:23,926][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:19:23,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:19:23,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:19:23,932][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:19:23,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:19:23,934][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:19:23,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:19:23,936][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:19:23,937][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:19:23,939][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:19:24,008][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.82it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.078 train/auc:  
                                                              0.669 train/f1:   
                                                              0.693             
                                                              train/precision:  
                                                              0.647             
                                                              train/recall:     
                                                              0.746 train/mre:  
                                                              0.087             
[2024-05-30 12:19:47,986][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/1>
[2024-05-30 12:19:47,986][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:19:47,989][HYDRA] 	#222 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:19:48,284][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:19:48,287][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:19:48,289][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:19:48,290][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:19:48,293][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:19:48,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:19:48,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:19:48,295][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:19:48,297][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:19:48,299][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:19:48,300][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:19:48,302][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:19:48,348][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.29it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.069 train/auc:  
                                                              0.729 train/f1:   
                                                              0.729             
                                                              train/precision:  
                                                              0.729             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.080             
[2024-05-30 12:20:12,682][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/2>
[2024-05-30 12:20:12,682][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:20:12,685][HYDRA] 	#223 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:20:12,967][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:20:12,970][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:20:12,972][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:20:12,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:20:12,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:20:12,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:20:12,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:20:12,978][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:20:12,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:20:12,981][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:20:12,982][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:20:12,985][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:20:13,069][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.49it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.077 train/auc:  
                                                              0.856 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.828             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.085             
[2024-05-30 12:20:37,033][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/3>
[2024-05-30 12:20:37,034][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:20:37,037][HYDRA] 	#224 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:20:37,320][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:20:37,323][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:20:37,325][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:20:37,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:20:37,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:20:37,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:20:37,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:20:37,331][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:20:37,333][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:20:37,334][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:20:37,334][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:20:37,336][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:20:37,379][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.91it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.077 train/auc:  
                                                              0.729 train/f1:   
                                                              0.758             
                                                              train/precision:  
                                                              0.685             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.097             
[2024-05-30 12:21:01,987][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/4>
[2024-05-30 12:21:01,988][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:21:01,991][HYDRA] 	#225 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:21:02,284][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:21:02,286][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:21:02,289][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:21:02,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:21:02,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:21:02,293][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:21:02,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:21:02,295][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:21:02,297][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:21:02,297][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:21:02,298][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:21:02,301][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:21:02,349][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.76it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.080 train/auc:  
                                                              0.814 train/f1:   
                                                              0.810             
                                                              train/precision:  
                                                              0.825             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.090             
[2024-05-30 12:21:26,584][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/5>
[2024-05-30 12:21:26,585][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:21:26,589][HYDRA] 	#226 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:21:26,874][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:21:26,877][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:21:26,879][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:21:26,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:21:26,883][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:21:26,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:21:26,884][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:21:26,885][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:21:26,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:21:26,889][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:21:26,889][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:21:26,891][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:21:26,970][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.07it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.061 train/auc:  
                                                              0.661 train/f1:   
                                                              0.692             
                                                              train/precision:  
                                                              0.634             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.071             
[2024-05-30 12:21:51,152][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/6>
[2024-05-30 12:21:51,153][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:21:51,155][HYDRA] 	#227 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:21:51,442][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:21:51,445][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:21:51,447][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:21:51,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:21:51,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:21:51,451][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:21:51,452][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:21:51,453][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:21:51,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:21:51,456][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:21:51,456][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:21:51,458][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:21:51,500][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.37it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.700 val/mre:    
                                                              0.063 train/auc:  
                                                              0.686 train/f1:   
                                                              0.726             
                                                              train/precision:  
                                                              0.645             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.077             
[2024-05-30 12:22:15,606][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/7>
[2024-05-30 12:22:15,606][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:22:15,609][HYDRA] 	#228 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:22:15,888][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:22:15,890][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:22:15,892][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:22:15,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:22:15,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:22:15,897][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:22:15,897][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:22:15,898][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:22:15,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:22:15,901][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:22:15,901][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:22:15,903][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:22:15,945][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.89it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.800 val/mre:    
                                                              0.083 train/auc:  
                                                              0.653 train/f1:   
                                                              0.624             
                                                              train/precision:  
                                                              0.680             
                                                              train/recall:     
                                                              0.576 train/mre:  
                                                              0.086             
[2024-05-30 12:22:40,251][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/8>
[2024-05-30 12:22:40,251][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:22:40,254][HYDRA] 	#229 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:22:40,541][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:22:40,543][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:22:40,546][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:22:40,546][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:22:40,549][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:22:40,550][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:22:40,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:22:40,552][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:22:40,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:22:40,555][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:22:40,556][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:22:40,558][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:22:40,638][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.38it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.085 train/auc:  
                                                              0.763 train/f1:   
                                                              0.708             
                                                              train/precision:  
                                                              0.919             
                                                              train/recall:     
                                                              0.576 train/mre:  
                                                              0.088             
[2024-05-30 12:23:04,571][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/9>
[2024-05-30 12:23:04,572][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:23:04,574][HYDRA] 	#230 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:23:04,864][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:23:04,866][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:23:04,869][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:23:04,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:23:04,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:23:04,873][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:23:04,874][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:23:04,875][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:23:04,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:23:04,878][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:23:04,878][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:23:04,882][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:23:04,926][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.67it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.072 train/auc:  
                                                              0.746 train/f1:   
                                                              0.766             
                                                              train/precision:  
                                                              0.710             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.091             
[2024-05-30 12:23:29,415][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/0>
[2024-05-30 12:23:29,416][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:23:29,418][HYDRA] 	#231 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:23:29,710][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:23:29,712][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:23:29,715][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:23:29,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:23:29,718][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:23:29,719][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:23:29,720][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:23:29,721][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:23:29,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:23:29,725][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:23:29,725][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:23:29,727][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:23:29,773][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.40it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.067 train/auc:  
                                                              0.780 train/f1:   
                                                              0.768             
                                                              train/precision:  
                                                              0.811             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.085             
[2024-05-30 12:23:53,829][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/1>
[2024-05-30 12:23:53,830][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:23:53,832][HYDRA] 	#232 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:23:54,120][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:23:54,122][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:23:54,124][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:23:54,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:23:54,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:23:54,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:23:54,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:23:54,130][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:23:54,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:23:54,134][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:23:54,134][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:23:54,136][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:23:54,219][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.99it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.069 train/auc:  
                                                              0.678 train/f1:   
                                                              0.694             
                                                              train/precision:  
                                                              0.662             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.089             
[2024-05-30 12:24:18,357][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/2>
[2024-05-30 12:24:18,358][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:24:18,361][HYDRA] 	#233 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:24:18,649][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:24:18,652][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:24:18,654][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:24:18,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:24:18,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:24:18,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:24:18,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:24:18,660][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:24:18,662][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:24:18,663][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:24:18,663][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:24:18,666][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:24:18,708][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.83it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.068 train/auc:  
                                                              0.873 train/f1:   
                                                              0.878             
                                                              train/precision:  
                                                              0.844             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.083             
[2024-05-30 12:24:42,509][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/3>
[2024-05-30 12:24:42,509][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:24:42,512][HYDRA] 	#234 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:24:42,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:24:42,806][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:24:42,808][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:24:42,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:24:42,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:24:42,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:24:42,813][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:24:42,814][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:24:42,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:24:42,818][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:24:42,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:24:42,820][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:24:42,864][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.58it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.088 train/auc:  
                                                              0.763 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.701             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.089             
[2024-05-30 12:25:07,141][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/4>
[2024-05-30 12:25:07,143][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:25:07,150][HYDRA] 	#235 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:25:07,484][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:25:07,487][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:25:07,489][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:25:07,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:25:07,493][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:25:07,494][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:25:07,494][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:25:07,495][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:25:07,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:25:07,498][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:25:07,498][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:25:07,500][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:25:07,559][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.87it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.075 train/auc:  
                                                              0.932 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              0.947             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.090             
[2024-05-30 12:25:31,881][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/5>
[2024-05-30 12:25:31,882][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:25:31,885][HYDRA] 	#236 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:25:32,162][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:25:32,164][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:25:32,166][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:25:32,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:25:32,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:25:32,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:25:32,172][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:25:32,172][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:25:32,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:25:32,175][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:25:32,176][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:25:32,179][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:25:32,222][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.23it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.064 train/auc:  
                                                              0.610 train/f1:   
                                                              0.652             
                                                              train/precision:  
                                                              0.589             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.077             
[2024-05-30 12:25:56,315][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/6>
[2024-05-30 12:25:56,316][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:25:56,319][HYDRA] 	#237 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:25:56,609][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:25:56,611][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:25:56,613][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:25:56,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:25:56,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:25:56,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:25:56,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:25:56,619][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:25:56,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:25:56,624][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:25:56,624][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:25:56,626][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:25:56,671][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.063 train/auc:  
                                                              0.669 train/f1:   
                                                              0.723             
                                                              train/precision:  
                                                              0.622             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.081             
[2024-05-30 12:26:20,489][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/7>
[2024-05-30 12:26:20,490][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:26:20,492][HYDRA] 	#238 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:26:20,778][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:26:20,781][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:26:20,785][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:26:20,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:26:20,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:26:20,789][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:26:20,790][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:26:20,791][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:26:20,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:26:20,793][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:26:20,794][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:26:20,796][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:26:20,836][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.97it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.071 train/auc:  
                                                              0.695 train/f1:   
                                                              0.746             
                                                              train/precision:  
                                                              0.639             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.085             
[2024-05-30 12:26:44,592][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/8>
[2024-05-30 12:26:44,593][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:26:44,597][HYDRA] 	#239 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:26:44,880][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:26:44,882][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:26:44,885][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:26:44,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:26:44,888][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:26:44,889][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:26:44,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:26:44,891][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:26:44,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:26:44,893][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:26:44,893][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:26:44,896][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:26:44,937][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.67it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.400 val/mre:    
                                                              0.073 train/auc:  
                                                              0.720 train/f1:   
                                                              0.723             
                                                              train/precision:  
                                                              0.717             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.083             
[2024-05-30 12:27:09,060][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/9>
[2024-05-30 12:27:09,061][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:27:09,064][HYDRA] 	#240 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:27:09,352][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:27:09,354][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:27:09,356][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:27:09,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:27:09,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:27:09,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:27:09,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:27:09,363][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:27:09,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:27:09,367][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:27:09,367][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:27:09,369][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:27:09,415][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.39it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.066 train/auc:  
                                                              0.712 train/f1:   
                                                              0.726             
                                                              train/precision:  
                                                              0.692             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.081             
[2024-05-30 12:27:34,321][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/0>
[2024-05-30 12:27:34,322][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:27:34,325][HYDRA] 	#241 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:27:34,610][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:27:34,613][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:27:34,616][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:27:34,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:27:34,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:27:34,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:27:34,622][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:27:34,623][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:27:34,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:27:34,633][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:27:34,634][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:27:34,636][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:27:34,720][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.22it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.066 train/auc:  
                                                              0.797 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.769             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.086             
[2024-05-30 12:27:58,817][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/1>
[2024-05-30 12:27:58,818][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:27:58,820][HYDRA] 	#242 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:27:59,109][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:27:59,111][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:27:59,113][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:27:59,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:27:59,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:27:59,118][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:27:59,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:27:59,120][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:27:59,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:27:59,122][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:27:59,122][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:27:59,124][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:27:59,165][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.30it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.300     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.300 val/mre:    
                                                              0.066 train/auc:  
                                                              0.746 train/f1:   
                                                              0.706             
                                                              train/precision:  
                                                              0.837             
                                                              train/recall:     
                                                              0.610 train/mre:  
                                                              0.080             
[2024-05-30 12:28:23,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/2>
[2024-05-30 12:28:23,048][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:28:23,052][HYDRA] 	#243 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:28:23,352][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:28:23,354][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:28:23,356][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:28:23,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:28:23,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:28:23,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:28:23,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:28:23,363][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:28:23,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:28:23,367][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:28:23,367][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:28:23,369][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:28:23,415][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.05it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.081 train/auc:  
                                                              0.864 train/f1:   
                                                              0.860             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.090             
[2024-05-30 12:28:47,603][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/3>
[2024-05-30 12:28:47,603][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:28:47,606][HYDRA] 	#244 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:28:47,891][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:28:47,893][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:28:47,896][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:28:47,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:28:47,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:28:47,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:28:47,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:28:47,902][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:28:47,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:28:47,905][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:28:47,906][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:28:47,908][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:28:47,989][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.67it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.093 train/auc:  
                                                              0.737 train/f1:   
                                                              0.760             
                                                              train/precision:  
                                                              0.700             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.096             
[2024-05-30 12:29:12,204][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/4>
[2024-05-30 12:29:12,205][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:29:12,210][HYDRA] 	#245 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:29:12,495][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:29:12,498][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:29:12,500][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:29:12,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:29:12,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:29:12,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:29:12,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:29:12,506][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:29:12,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:29:12,509][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:29:12,509][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:29:12,513][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:29:12,557][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.10it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.915 train/f1:   
                                                              0.911             
                                                              train/precision:  
                                                              0.962             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.087             
[2024-05-30 12:29:36,607][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/5>
[2024-05-30 12:29:36,607][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:29:36,610][HYDRA] 	#246 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:29:36,891][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:29:36,893][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:29:36,895][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:29:36,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:29:36,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:29:36,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:29:36,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:29:36,901][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:29:36,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:29:36,904][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:29:36,904][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:29:36,907][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:29:36,948][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.16it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.067 train/auc:  
                                                              0.754 train/f1:   
                                                              0.772             
                                                              train/precision:  
                                                              0.721             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.077             
[2024-05-30 12:30:01,134][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/6>
[2024-05-30 12:30:01,135][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:30:01,138][HYDRA] 	#247 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:30:01,434][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:30:01,436][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:30:01,438][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:30:01,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:30:01,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:30:01,443][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:30:01,444][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:30:01,444][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:30:01,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:30:01,448][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:30:01,448][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:30:01,450][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:30:01,553][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.37it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.061 train/auc:  
                                                              0.593 train/f1:   
                                                              0.680             
                                                              train/precision:  
                                                              0.560             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.076             
[2024-05-30 12:30:26,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/7>
[2024-05-30 12:30:26,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:30:26,690][HYDRA] 	#248 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:30:26,982][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:30:26,984][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:30:26,986][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:30:26,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:30:26,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:30:26,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:30:26,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:30:26,993][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:30:26,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:30:26,996][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:30:26,996][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:30:26,998][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:30:27,041][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.83it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.067 train/auc:  
                                                              0.661 train/f1:   
                                                              0.697             
                                                              train/precision:  
                                                              0.630             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.077             
[2024-05-30 12:30:51,064][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/8>
[2024-05-30 12:30:51,065][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:30:51,068][HYDRA] 	#249 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:30:51,358][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:30:51,361][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:30:51,363][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:30:51,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:30:51,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:30:51,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:30:51,368][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:30:51,369][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:30:51,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:30:51,373][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:30:51,374][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:30:51,376][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:30:51,421][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.53it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.822 train/f1:   
                                                              0.824             
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.079             
[2024-05-30 12:31:15,483][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/9>
[2024-05-30 12:31:15,484][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:31:15,487][HYDRA] 	#250 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:31:15,770][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:31:15,772][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:31:15,775][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:31:15,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:31:15,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:31:15,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:31:15,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:31:15,781][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:31:15,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:31:15,783][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:31:15,784][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:31:15,786][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:31:15,828][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.94it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.065 train/auc:  
                                                              0.814 train/f1:   
                                                              0.807             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.083             
[2024-05-30 12:31:39,581][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/0>
[2024-05-30 12:31:39,582][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:31:39,584][HYDRA] 	#251 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:31:39,873][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:31:39,876][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:31:39,878][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:31:39,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:31:39,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:31:39,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:31:39,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:31:39,884][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:31:39,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:31:39,887][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:31:39,887][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:31:39,891][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:31:39,934][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.05it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.070 train/auc:  
                                                              0.805 train/f1:   
                                                              0.813             
                                                              train/precision:  
                                                              0.781             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.089             
[2024-05-30 12:32:03,968][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/1>
[2024-05-30 12:32:03,969][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:32:03,972][HYDRA] 	#252 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:32:04,291][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:32:04,293][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:32:04,295][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:32:04,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:32:04,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:32:04,300][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:32:04,301][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:32:04,302][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:32:04,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:32:04,306][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:32:04,306][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:32:04,308][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:32:04,354][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.74it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.080 train/auc:  
                                                              0.669 train/f1:   
                                                              0.738             
                                                              train/precision:  
                                                              0.611             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.092             
[2024-05-30 12:32:28,515][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/2>
[2024-05-30 12:32:28,515][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:32:28,518][HYDRA] 	#253 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:32:28,805][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:32:28,808][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:32:28,811][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:32:28,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:32:28,815][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:32:28,816][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:32:28,817][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:32:28,817][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:32:28,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:32:28,821][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:32:28,821][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:32:28,823][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:32:28,959][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.02it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.067 train/auc:  
                                                              0.881 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.085             
[2024-05-30 12:32:53,217][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/3>
[2024-05-30 12:32:53,218][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:32:53,221][HYDRA] 	#254 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:32:53,505][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:32:53,507][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:32:53,510][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:32:53,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:32:53,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:32:53,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:32:53,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:32:53,516][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:32:53,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:32:53,519][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:32:53,519][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:32:53,521][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:32:53,564][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.13it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.066 train/auc:  
                                                              0.678 train/f1:   
                                                              0.740             
                                                              train/precision:  
                                                              0.621             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.078             
[2024-05-30 12:33:17,701][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/4>
[2024-05-30 12:33:17,702][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:33:17,705][HYDRA] 	#255 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:33:17,981][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:33:17,983][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:33:17,985][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:33:17,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:33:17,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:33:17,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:33:17,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:33:17,991][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:33:17,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:33:17,993][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:33:17,994][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:33:17,996][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:33:18,040][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.22it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.073 train/auc:  
                                                              0.915 train/f1:   
                                                              0.914             
                                                              train/precision:  
                                                              0.930             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.086             
[2024-05-30 12:33:43,059][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/5>
[2024-05-30 12:33:43,060][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:33:43,063][HYDRA] 	#256 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:33:43,347][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:33:43,350][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:33:43,352][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:33:43,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:33:43,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:33:43,357][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:33:43,357][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:33:43,358][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:33:43,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:33:43,362][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:33:43,362][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:33:43,364][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:33:43,455][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.36it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.064 train/auc:  
                                                              0.686 train/f1:   
                                                              0.722             
                                                              train/precision:  
                                                              0.649             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.074             
[2024-05-30 12:34:07,415][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/6>
[2024-05-30 12:34:07,416][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:34:07,418][HYDRA] 	#257 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:34:07,702][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:34:07,705][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:34:07,707][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:34:07,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:34:07,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:34:07,711][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:34:07,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:34:07,713][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:34:07,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:34:07,717][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:34:07,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:34:07,721][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:34:08,206][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.82it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.062 train/auc:  
                                                              0.653 train/f1:   
                                                              0.713             
                                                              train/precision:  
                                                              0.607             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.075             
[2024-05-30 12:34:32,151][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/7>
[2024-05-30 12:34:32,152][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:34:32,155][HYDRA] 	#258 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:34:32,438][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:34:32,440][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:34:32,442][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:34:32,443][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:34:32,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:34:32,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:34:32,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:34:32,449][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:34:32,450][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:34:32,451][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:34:32,451][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:34:32,453][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:34:32,495][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.75it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.064 train/auc:  
                                                              0.695 train/f1:   
                                                              0.735             
                                                              train/precision:  
                                                              0.649             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.075             
[2024-05-30 12:34:56,764][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/8>
[2024-05-30 12:34:56,765][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:34:56,767][HYDRA] 	#259 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:34:57,049][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:34:57,051][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:34:57,053][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:34:57,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:34:57,057][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:34:57,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:34:57,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:34:57,059][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:34:57,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:34:57,062][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:34:57,062][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:34:57,064][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:34:57,106][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.74it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.435     
                                                              val/precision:    
                                                              0.385 val/recall: 
                                                              0.500 val/mre:    
                                                              0.074 train/auc:  
                                                              0.847 train/f1:   
                                                              0.827             
                                                              train/precision:  
                                                              0.956             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.083             
[2024-05-30 12:35:21,695][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/9>
[2024-05-30 12:35:21,696][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:35:21,699][HYDRA] 	#260 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:35:21,989][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:35:21,991][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:35:21,994][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:35:21,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:35:21,998][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:35:21,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:35:21,999][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:35:22,000][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:35:22,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:35:22,004][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:35:22,005][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:35:22,007][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:35:22,056][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.34it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.066 train/auc:  
                                                              0.805 train/f1:   
                                                              0.763             
                                                              train/precision:  
                                                              0.974             
                                                              train/recall:     
                                                              0.627 train/mre:  
                                                              0.085             
[2024-05-30 12:35:46,320][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/0>
[2024-05-30 12:35:46,320][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:35:46,323][HYDRA] 	#261 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:35:46,606][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:35:46,608][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:35:46,610][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:35:46,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:35:46,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:35:46,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:35:46,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:35:46,616][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:35:46,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:35:46,619][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:35:46,619][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:35:46,621][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:35:46,663][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.25it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.078 train/auc:  
                                                              0.898 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.851             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.083             
[2024-05-30 12:36:10,888][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/1>
[2024-05-30 12:36:10,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:36:10,892][HYDRA] 	#262 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:36:11,186][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:36:11,189][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:36:11,191][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:36:11,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:36:11,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:36:11,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:36:11,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:36:11,197][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:36:11,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:36:11,201][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:36:11,201][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:36:11,203][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:36:11,291][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.81it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.083 train/auc:  
                                                              0.805 train/f1:   
                                                              0.816             
                                                              train/precision:  
                                                              0.773             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.086             
[2024-05-30 12:36:35,361][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/2>
[2024-05-30 12:36:35,362][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:36:35,365][HYDRA] 	#263 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:36:35,647][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:36:35,649][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:36:35,651][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:36:35,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:36:35,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:36:35,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:36:35,657][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:36:35,657][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:36:35,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:36:35,660][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:36:35,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:36:35,664][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:36:35,706][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.03it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.078 train/auc:  
                                                              0.856 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.828             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.085             
[2024-05-30 12:36:59,623][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/3>
[2024-05-30 12:36:59,624][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:36:59,626][HYDRA] 	#264 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:36:59,915][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:36:59,918][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:36:59,920][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:36:59,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:36:59,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:36:59,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:36:59,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:36:59,926][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:36:59,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:36:59,929][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:36:59,929][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:36:59,931][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:36:59,973][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.53it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.455     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.500 val/mre:    
                                                              0.067 train/auc:  
                                                              0.831 train/f1:   
                                                              0.839             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.081             
[2024-05-30 12:37:24,305][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/4>
[2024-05-30 12:37:24,306][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:37:24,309][HYDRA] 	#265 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:37:24,595][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:37:24,598][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:37:24,600][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:37:24,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:37:24,604][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:37:24,604][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:37:24,605][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:37:24,606][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:37:24,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:37:24,609][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:37:24,610][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:37:24,612][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:37:24,692][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.16it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.070 train/auc:  
                                                              0.949 train/f1:   
                                                              0.946             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.085             
[2024-05-30 12:37:48,664][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/5>
[2024-05-30 12:37:48,665][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:37:48,668][HYDRA] 	#266 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:37:48,955][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:37:48,957][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:37:48,959][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:37:48,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:37:48,963][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:37:48,964][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:37:48,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:37:48,966][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:37:48,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:37:48,970][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:37:48,970][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:37:48,973][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:37:49,018][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.70it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.064 train/auc:  
                                                              0.703 train/f1:   
                                                              0.737             
                                                              train/precision:  
                                                              0.662             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.076             
[2024-05-30 12:38:13,130][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/6>
[2024-05-30 12:38:13,130][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:38:13,134][HYDRA] 	#267 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:38:13,416][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:38:13,418][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:38:13,421][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:38:13,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:38:13,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:38:13,425][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:38:13,426][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:38:13,427][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:38:13,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:38:13,429][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:38:13,430][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:38:13,432][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:38:13,479][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.59it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.062 train/auc:  
                                                              0.712 train/f1:   
                                                              0.742             
                                                              train/precision:  
                                                              0.671             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.075             
[2024-05-30 12:38:37,486][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/7>
[2024-05-30 12:38:37,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:38:37,495][HYDRA] 	#268 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:38:37,822][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:38:37,824][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:38:37,827][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:38:37,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:38:37,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:38:37,831][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:38:37,832][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:38:37,833][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:38:37,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:38:37,835][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:38:37,836][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:38:37,838][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:38:37,886][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.25it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.062 train/auc:  
                                                              0.737 train/f1:   
                                                              0.763             
                                                              train/precision:  
                                                              0.694             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.074             
[2024-05-30 12:39:01,820][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/8>
[2024-05-30 12:39:01,821][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:39:01,824][HYDRA] 	#269 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:39:02,116][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:39:02,118][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:39:02,120][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:39:02,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:39:02,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:39:02,125][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:39:02,126][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:39:02,126][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:39:02,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:39:02,131][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:39:02,131][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:39:02,135][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:39:02,182][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.97it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.074 train/auc:  
                                                              0.932 train/f1:   
                                                              0.933             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.084             
[2024-05-30 12:39:26,491][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/9>
[2024-05-30 12:39:26,491][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:39:26,494][HYDRA] 	#270 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:39:26,775][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:39:26,777][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:39:26,779][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:39:26,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:39:26,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:39:26,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:39:26,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:39:26,785][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:39:26,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:39:26,788][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:39:26,788][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:39:26,791][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:39:26,831][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.04it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.864 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.081             
[2024-05-30 12:39:51,232][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/0>
[2024-05-30 12:39:51,232][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:39:51,235][HYDRA] 	#271 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:39:51,520][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:39:51,522][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:39:51,524][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:39:51,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:39:51,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:39:51,529][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:39:51,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:39:51,531][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:39:51,532][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:39:51,534][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:39:51,535][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:39:51,537][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:39:51,625][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.98it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.066 train/auc:  
                                                              0.890 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.859             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.084             
[2024-05-30 12:40:15,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/1>
[2024-05-30 12:40:15,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:40:15,694][HYDRA] 	#272 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:40:15,968][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:40:15,970][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:40:15,972][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:40:15,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:40:15,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:40:15,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:40:15,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:40:15,978][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:40:15,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:40:15,981][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:40:15,981][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:40:15,983][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:40:16,024][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.10it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.078 train/auc:  
                                                              0.805 train/f1:   
                                                              0.827             
                                                              train/precision:  
                                                              0.743             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.088             
[2024-05-30 12:40:41,004][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/2>
[2024-05-30 12:40:41,005][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:40:41,008][HYDRA] 	#273 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:40:41,287][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:40:41,290][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:40:41,292][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:40:41,293][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:40:41,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:40:41,297][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:40:41,297][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:40:41,298][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:40:41,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:40:41,301][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:40:41,301][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:40:41,303][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:40:41,345][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.24it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.071 train/auc:  
                                                              0.949 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.908             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.088             
[2024-05-30 12:41:06,003][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/3>
[2024-05-30 12:41:06,004][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:41:06,007][HYDRA] 	#274 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:41:06,297][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:41:06,299][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:41:06,302][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:41:06,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:41:06,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:41:06,306][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:41:06,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:41:06,308][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:41:06,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:41:06,312][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:41:06,313][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:41:06,315][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:41:06,403][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.16it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.075 train/auc:  
                                                              0.559 train/f1:   
                                                              0.690             
                                                              train/precision:  
                                                              0.532             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.084             
[2024-05-30 12:41:31,142][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/4>
[2024-05-30 12:41:31,143][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:41:31,145][HYDRA] 	#275 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:41:31,426][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:41:31,429][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:41:31,431][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:41:31,431][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:41:31,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:41:31,436][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:41:31,436][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:41:31,437][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:41:31,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:41:31,440][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:41:31,440][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:41:31,442][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:41:31,485][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.07it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.070 train/auc:  
                                                              0.949 train/f1:   
                                                              0.950             
                                                              train/precision:  
                                                              0.934             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.085             
[2024-05-30 12:41:55,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/5>
[2024-05-30 12:41:55,371][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:41:55,374][HYDRA] 	#276 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:41:55,651][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:41:55,653][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:41:55,655][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:41:55,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:41:55,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:41:55,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:41:55,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:41:55,661][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:41:55,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:41:55,664][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:41:55,664][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:41:55,666][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:41:55,708][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.31it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              1.000 val/mre:    
                                                              0.071 train/auc:  
                                                              0.712 train/f1:   
                                                              0.730             
                                                              train/precision:  
                                                              0.687             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.077             
[2024-05-30 12:42:19,736][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/6>
[2024-05-30 12:42:19,736][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:42:19,739][HYDRA] 	#277 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:42:20,031][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:42:20,034][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:42:20,036][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:42:20,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:42:20,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:42:20,040][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:42:20,041][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:42:20,042][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:42:20,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:42:20,047][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:42:20,047][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:42:20,049][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:42:20,135][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.21it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.653 train/f1:   
                                                              0.732             
                                                              train/precision:  
                                                              0.596             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.077             
[2024-05-30 12:42:44,582][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/7>
[2024-05-30 12:42:44,583][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:42:44,586][HYDRA] 	#278 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:42:44,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:42:44,871][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:42:44,873][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:42:44,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:42:44,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:42:44,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:42:44,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:42:44,879][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:42:44,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:42:44,882][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:42:44,882][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:42:44,884][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:42:44,925][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.87it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.063 train/auc:  
                                                              0.712 train/f1:   
                                                              0.761             
                                                              train/precision:  
                                                              0.651             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.076             
[2024-05-30 12:43:09,214][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/8>
[2024-05-30 12:43:09,215][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:43:09,218][HYDRA] 	#279 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:43:09,499][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:43:09,502][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:43:09,504][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:43:09,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:43:09,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:43:09,509][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:43:09,509][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:43:09,510][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:43:09,512][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:43:09,513][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:43:09,513][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:43:09,515][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:43:09,556][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.38it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.067 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.081             
[2024-05-30 12:43:33,962][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/9>
[2024-05-30 12:43:33,962][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:43:33,965][HYDRA] 	#280 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:43:34,253][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:43:34,255][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:43:34,257][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:43:34,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:43:34,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:43:34,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:43:34,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:43:34,263][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:43:34,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:43:34,266][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:43:34,266][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:43:34,268][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:43:34,310][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.50it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.794  
                                                              train/f1: 0.829   
                                                              train/precision:  
                                                              0.708             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-05-30 12:43:59,564][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/0>
[2024-05-30 12:43:59,565][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:43:59,568][HYDRA] 	#281 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:43:59,856][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:43:59,859][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:43:59,861][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:43:59,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:43:59,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:43:59,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:43:59,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:43:59,867][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:43:59,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:43:59,870][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:43:59,870][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:43:59,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:43:59,916][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.18it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.921  
                                                              train/f1: 0.922   
                                                              train/precision:  
                                                              0.908             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              nan               
[2024-05-30 12:44:24,626][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/1>
[2024-05-30 12:44:24,626][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:44:24,629][HYDRA] 	#282 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:44:24,915][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:44:24,918][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:44:24,920][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:44:24,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:44:24,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:44:24,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:44:24,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:44:24,926][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:44:24,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:44:24,929][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:44:24,929][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:44:24,931][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:44:24,974][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.33it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.889  
                                                              train/f1: 0.892   
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-05-30 12:44:48,979][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/2>
[2024-05-30 12:44:48,980][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:44:48,982][HYDRA] 	#283 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:44:49,265][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:44:49,267][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:44:49,269][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:44:49,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:44:49,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:44:49,274][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:44:49,274][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:44:49,275][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:44:49,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:44:49,278][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:44:49,278][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:44:49,280][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:44:49,321][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.38it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.929  
                                                              train/f1: 0.933   
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-05-30 12:45:13,541][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/3>
[2024-05-30 12:45:13,542][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:45:13,545][HYDRA] 	#284 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:45:13,829][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:45:13,832][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:45:13,834][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:45:13,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:45:13,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:45:13,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:45:13,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:45:13,840][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:45:13,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:45:13,843][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:45:13,843][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:45:13,845][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:45:13,894][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.31it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.800 val/mre: nan
                                                              train/auc: 0.833  
                                                              train/f1: 0.826   
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.794 train/mre:  
                                                              nan               
[2024-05-30 12:45:38,104][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/4>
[2024-05-30 12:45:38,105][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:45:38,108][HYDRA] 	#285 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:45:38,410][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:45:38,413][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:45:38,415][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:45:38,415][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:45:38,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:45:38,420][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:45:38,420][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:45:38,421][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:45:38,423][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:45:38,426][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:45:38,427][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:45:38,429][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:45:38,545][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.08it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.905  
                                                              train/f1: 0.906   
                                                              train/precision:  
                                                              0.892             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-05-30 12:46:02,803][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/5>
[2024-05-30 12:46:02,804][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:46:02,807][HYDRA] 	#286 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:46:03,099][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:46:03,101][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:46:03,103][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:46:03,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:46:03,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:46:03,108][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:46:03,109][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:46:03,109][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:46:03,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:46:03,112][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:46:03,112][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:46:03,115][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:46:03,158][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.44it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.865  
                                                              train/f1: 0.864   
                                                              train/precision:  
                                                              0.871             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              nan               
[2024-05-30 12:46:27,663][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/6>
[2024-05-30 12:46:27,663][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:46:27,666][HYDRA] 	#287 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:46:27,944][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:46:27,946][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:46:27,949][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:46:27,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:46:27,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:46:27,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:46:27,954][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:46:27,955][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:46:27,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:46:27,957][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:46:27,958][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:46:27,961][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:46:28,002][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.77it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.937  
                                                              train/f1: 0.935   
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-05-30 12:46:52,135][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/7>
[2024-05-30 12:46:52,136][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:46:52,139][HYDRA] 	#288 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:46:52,425][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:46:52,427][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:46:52,429][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:46:52,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:46:52,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:46:52,434][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:46:52,435][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:46:52,436][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:46:52,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:46:52,441][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:46:52,441][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:46:52,443][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:46:52,530][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.86it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.857  
                                                              train/f1: 0.866   
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-05-30 12:47:16,466][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/8>
[2024-05-30 12:47:16,466][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:47:16,470][HYDRA] 	#289 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:47:16,755][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:47:16,758][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:47:16,760][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:47:16,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:47:16,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:47:16,764][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:47:16,765][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:47:16,766][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:47:16,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:47:16,768][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:47:16,769][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:47:16,771][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:47:16,812][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.99it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.921  
                                                              train/f1: 0.921   
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-05-30 12:47:41,077][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/9>
[2024-05-30 12:47:41,078][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:47:41,081][HYDRA] 	#290 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:47:41,362][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:47:41,365][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:47:41,367][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:47:41,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:47:41,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:47:41,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:47:41,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:47:41,373][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:47:41,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:47:41,376][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:47:41,376][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:47:41,378][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:47:41,421][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.49it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.046 train/auc:  
                                                              0.857 train/f1:   
                                                              0.839             
                                                              train/precision:  
                                                              0.959             
                                                              train/recall:     
                                                              0.746 train/mre:  
                                                              0.065             
[2024-05-30 12:48:05,572][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/0>
[2024-05-30 12:48:05,572][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:48:05,575][HYDRA] 	#291 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:48:05,857][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:48:05,859][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:48:05,861][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:48:05,862][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:48:05,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:48:05,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:48:05,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:48:05,867][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:48:05,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:48:05,872][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:48:05,872][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:48:05,875][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:48:05,959][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.71it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.043 train/auc:  
                                                              0.881 train/f1:   
                                                              0.878             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.067             
[2024-05-30 12:48:30,195][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/1>
[2024-05-30 12:48:30,196][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:48:30,199][HYDRA] 	#292 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:48:30,961][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:48:30,964][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:48:30,966][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:48:30,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:48:30,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:48:30,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:48:30,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:48:30,972][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:48:30,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:48:30,976][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:48:30,976][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:48:30,978][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:48:31,020][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.31it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.048 train/auc:  
                                                              0.913 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              0.861             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.064             
[2024-05-30 12:48:55,038][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/2>
[2024-05-30 12:48:55,039][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:48:55,041][HYDRA] 	#293 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:48:55,318][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:48:55,320][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:48:55,323][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:48:55,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:48:55,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:48:55,327][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:48:55,328][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:48:55,329][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:48:55,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:48:55,331][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:48:55,332][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:48:55,334][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:48:55,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.89it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.052 train/auc:  
                                                              0.905 train/f1:   
                                                              0.913             
                                                              train/precision:  
                                                              0.840             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.061             
[2024-05-30 12:49:20,041][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/3>
[2024-05-30 12:49:20,042][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:49:20,045][HYDRA] 	#294 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:49:20,336][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:49:20,338][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:49:20,340][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:49:20,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:49:20,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:49:20,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:49:20,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:49:20,346][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:49:20,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:49:20,352][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:49:20,353][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:49:20,355][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:49:20,459][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.95it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.046 train/auc:  
                                                              0.960 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.953             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.064             
[2024-05-30 12:49:45,396][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/4>
[2024-05-30 12:49:45,396][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:49:45,399][HYDRA] 	#295 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:49:45,690][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:49:45,693][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:49:45,695][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:49:45,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:49:45,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:49:45,699][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:49:45,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:49:45,701][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:49:45,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:49:45,703][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:49:45,704][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:49:45,706][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:49:45,747][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.23it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.043 train/auc:  
                                                              0.937 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.067             
[2024-05-30 12:50:11,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/5>
[2024-05-30 12:50:11,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:50:11,051][HYDRA] 	#296 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:50:11,342][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:50:11,344][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:50:11,347][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:50:11,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:50:11,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:50:11,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:50:11,352][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:50:11,353][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:50:11,355][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:50:11,356][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:50:11,356][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:50:11,359][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:50:11,417][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.79it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.041 train/auc:  
                                                              0.968 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.954             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.062             
[2024-05-30 12:50:36,563][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/6>
[2024-05-30 12:50:36,564][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:50:36,567][HYDRA] 	#297 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:50:36,899][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:50:36,901][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:50:36,903][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:50:36,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:50:36,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:50:36,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:50:36,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:50:36,910][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:50:36,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:50:36,912][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:50:36,913][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:50:36,915][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:50:36,981][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.43it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.049 train/auc:  
                                                              0.921 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              0.934             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              0.062             
[2024-05-30 12:51:02,100][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/7>
[2024-05-30 12:51:02,101][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:51:02,104][HYDRA] 	#298 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:51:02,403][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:51:02,406][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:51:02,409][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:51:02,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:51:02,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:51:02,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:51:02,415][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:51:02,416][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:51:02,418][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:51:02,419][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:51:02,419][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:51:02,421][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:51:02,464][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.65it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.044 train/auc:  
                                                              0.913 train/f1:   
                                                              0.913             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.060             
[2024-05-30 12:51:28,595][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/8>
[2024-05-30 12:51:28,596][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:51:28,598][HYDRA] 	#299 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:51:28,892][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:51:28,894][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:51:28,896][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:51:28,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:51:28,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:51:28,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:51:28,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:51:28,902][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:51:28,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:51:28,908][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:51:28,908][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:51:28,910][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:51:29,006][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.35it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.040 train/auc:  
                                                              0.913 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.919             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              0.062             
[2024-05-30 12:51:53,956][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/9>
[2024-05-30 12:51:53,957][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:51:53,960][HYDRA] 	#300 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:51:54,253][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:51:54,255][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:51:54,258][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:51:54,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:51:54,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:51:54,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:51:54,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:51:54,264][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:51:54,266][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:51:54,267][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:51:54,267][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:51:54,270][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:51:54,315][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.76it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.045 train/auc:  
                                                              0.905 train/f1:   
                                                              0.897             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.065             
[2024-05-30 12:52:19,116][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/0>
[2024-05-30 12:52:19,117][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:52:19,120][HYDRA] 	#301 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:52:19,405][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:52:19,408][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:52:19,410][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:52:19,411][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:52:19,414][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:52:19,415][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:52:19,416][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:52:19,416][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:52:19,418][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:52:19,419][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:52:19,419][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:52:19,422][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:52:19,467][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.47it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.046 train/auc:  
                                                              0.913 train/f1:   
                                                              0.915             
                                                              train/precision:  
                                                              0.894             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.065             
[2024-05-30 12:52:44,378][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/1>
[2024-05-30 12:52:44,380][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:52:44,383][HYDRA] 	#302 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:52:44,684][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:52:44,687][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:52:44,689][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:52:44,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:52:44,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:52:44,694][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:52:44,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:52:44,696][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:52:44,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:52:44,701][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:52:44,701][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:52:44,703][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:52:44,792][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.10it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.050 train/auc:  
                                                              0.881 train/f1:   
                                                              0.891             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.064             
[2024-05-30 12:53:10,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/2>
[2024-05-30 12:53:10,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:53:10,181][HYDRA] 	#303 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:53:10,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:53:10,466][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:53:10,468][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:53:10,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:53:10,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:53:10,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:53:10,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:53:10,474][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:53:10,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:53:10,477][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:53:10,477][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:53:10,479][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:53:10,523][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.56it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.042 train/auc:  
                                                              0.825 train/f1:   
                                                              0.851             
                                                              train/precision:  
                                                              0.741             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.066             
[2024-05-30 12:53:35,505][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/3>
[2024-05-30 12:53:35,505][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:53:35,508][HYDRA] 	#304 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:53:35,791][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:53:35,794][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:53:35,796][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:53:35,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:53:35,800][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:53:35,801][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:53:35,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:53:35,803][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:53:35,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:53:35,805][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:53:35,806][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:53:35,808][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:53:35,850][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.85it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.064             
[2024-05-30 12:54:00,864][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/4>
[2024-05-30 12:54:00,865][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:54:00,868][HYDRA] 	#305 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:54:01,158][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:54:01,161][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:54:01,163][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:54:01,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:54:01,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:54:01,168][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:54:01,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:54:01,169][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:54:01,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:54:01,175][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:54:01,175][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:54:01,178][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:54:01,264][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.84it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              0.960 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              0.926             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.061             
[2024-05-30 12:54:26,589][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/5>
[2024-05-30 12:54:26,590][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:54:26,592][HYDRA] 	#306 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:54:26,875][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:54:26,878][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:54:26,880][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:54:26,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:54:26,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:54:26,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:54:26,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:54:26,886][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:54:26,888][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:54:26,889][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:54:26,889][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:54:26,891][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:54:26,935][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.85it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.045 train/auc:  
                                                              0.873 train/f1:   
                                                              0.864             
                                                              train/precision:  
                                                              0.927             
                                                              train/recall:     
                                                              0.810 train/mre:  
                                                              0.068             
[2024-05-30 12:54:52,112][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/6>
[2024-05-30 12:54:52,113][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:54:52,116][HYDRA] 	#307 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:54:52,405][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:54:52,407][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:54:52,409][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:54:52,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:54:52,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:54:52,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:54:52,415][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:54:52,415][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:54:52,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:54:52,418][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:54:52,418][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:54:52,421][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:54:52,463][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.05it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.048 train/auc:  
                                                              0.841 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.787             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.062             
[2024-05-30 12:55:17,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/7>
[2024-05-30 12:55:17,840][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:55:17,843][HYDRA] 	#308 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:55:18,132][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:55:18,134][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:55:18,136][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:55:18,137][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:55:18,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:55:18,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:55:18,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:55:18,142][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:55:18,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:55:18,148][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:55:18,148][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:55:18,150][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:55:18,236][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.54it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.043 train/auc:  
                                                              0.905 train/f1:   
                                                              0.913             
                                                              train/precision:  
                                                              0.840             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.063             
[2024-05-30 12:55:43,904][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/8>
[2024-05-30 12:55:43,905][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:55:43,909][HYDRA] 	#309 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:55:44,195][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:55:44,198][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:55:44,200][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:55:44,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:55:44,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:55:44,205][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:55:44,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:55:44,207][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:55:44,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:55:44,209][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:55:44,210][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:55:44,212][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:55:44,259][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.77it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.052 train/auc:  
                                                              0.952 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.913             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.063             
[2024-05-30 12:56:09,360][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/9>
[2024-05-30 12:56:09,361][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:56:09,364][HYDRA] 	#310 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:56:10,165][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:56:10,167][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:56:10,169][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:56:10,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:56:10,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:56:10,174][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:56:10,175][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:56:10,175][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:56:10,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:56:10,178][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:56:10,179][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:56:10,181][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:56:10,223][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.98it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.043 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.064             
[2024-05-30 12:56:35,328][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/0>
[2024-05-30 12:56:35,329][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:56:35,332][HYDRA] 	#311 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:56:35,621][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:56:35,623][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:56:35,626][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:56:35,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:56:35,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:56:35,630][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:56:35,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:56:35,632][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:56:35,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:56:35,635][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:56:35,635][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:56:35,637][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:56:35,685][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.31it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.044 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-05-30 12:57:00,905][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/1>
[2024-05-30 12:57:00,906][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:57:00,908][HYDRA] 	#312 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:57:01,196][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:57:01,199][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:57:01,201][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:57:01,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:57:01,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:57:01,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:57:01,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:57:01,207][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:57:01,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:57:01,210][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:57:01,210][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:57:01,212][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:57:01,255][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.82it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.054 train/auc:  
                                                              0.952 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.065             
[2024-05-30 12:57:26,658][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/2>
[2024-05-30 12:57:26,659][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:57:26,662][HYDRA] 	#313 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:57:26,950][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:57:26,953][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:57:26,955][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:57:26,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:57:26,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:57:26,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:57:26,961][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:57:26,961][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:57:26,963][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:57:26,964][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:57:26,964][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:57:26,967][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:57:27,009][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.28it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.045 train/auc:  
                                                              0.960 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              0.926             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.059             
[2024-05-30 12:57:51,858][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/3>
[2024-05-30 12:57:51,859][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:57:51,864][HYDRA] 	#314 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:57:52,146][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:57:52,149][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:57:52,151][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:57:52,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:57:52,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:57:52,155][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:57:52,156][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:57:52,157][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:57:52,159][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:57:52,159][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:57:52,160][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:57:52,162][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:57:52,204][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.21it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.043 train/auc:  
                                                              0.960 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.067             
[2024-05-30 12:58:17,098][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/4>
[2024-05-30 12:58:17,099][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:58:17,102][HYDRA] 	#315 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:58:17,393][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:58:17,395][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:58:17,398][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:58:17,398][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:58:17,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:58:17,402][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:58:17,403][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:58:17,404][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:58:17,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:58:17,407][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:58:17,407][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:58:17,410][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:58:17,452][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.96it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.039 train/auc:  
                                                              0.976 train/f1:   
                                                              0.977             
                                                              train/precision:  
                                                              0.955             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-05-30 12:58:42,857][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/5>
[2024-05-30 12:58:42,857][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:58:42,860][HYDRA] 	#316 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:58:43,157][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:58:43,160][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:58:43,162][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:58:43,162][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:58:43,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:58:43,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:58:43,168][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:58:43,169][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:58:43,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:58:43,174][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:58:43,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:58:43,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:58:43,263][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.73it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.043 train/auc:  
                                                              0.952 train/f1:   
                                                              0.950             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              0.064             
[2024-05-30 12:59:08,695][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/6>
[2024-05-30 12:59:08,696][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:59:08,699][HYDRA] 	#317 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:59:08,998][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:59:09,001][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:59:09,003][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:59:09,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:59:09,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:59:09,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:59:09,008][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:59:09,009][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:59:09,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:59:09,012][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:59:09,012][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:59:09,014][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:59:09,057][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.49it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              0.952 train/f1:   
                                                              0.953             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.059             
[2024-05-30 12:59:34,361][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/7>
[2024-05-30 12:59:34,362][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:59:34,364][HYDRA] 	#318 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:59:34,647][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:59:34,649][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 12:59:34,651][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:59:34,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:59:34,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:59:34,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:59:34,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:59:34,657][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:59:34,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:59:34,660][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:59:34,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:59:34,662][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:59:34,705][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.42it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.800 val/mre:    
                                                              0.053 train/auc:  
                                                              0.873 train/f1:   
                                                              0.881             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.060             
[2024-05-30 12:59:59,741][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/8>
[2024-05-30 12:59:59,742][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:59:59,745][HYDRA] 	#319 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:00:00,047][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:00:00,050][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:00:00,052][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:00:00,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:00:00,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:00:00,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:00:00,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:00:00,059][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:00:00,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:00:00,064][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:00:00,065][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:00:00,067][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:00:00,160][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.75it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              0.968 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.940             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.065             
[2024-05-30 13:00:25,493][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/9>
[2024-05-30 13:00:25,493][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:00:25,496][HYDRA] 	#320 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:00:25,789][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:00:25,792][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:00:25,795][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:00:25,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:00:25,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:00:25,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:00:25,800][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:00:25,801][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:00:25,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:00:25,804][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:00:25,804][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:00:25,806][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:00:25,849][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.58it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.050 train/auc:  
                                                              0.960 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.066             
[2024-05-30 13:00:51,835][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/0>
[2024-05-30 13:00:51,836][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:00:51,839][HYDRA] 	#321 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:00:52,158][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:00:52,161][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:00:52,164][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:00:52,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:00:52,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:00:52,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:00:52,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:00:52,170][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:00:52,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:00:52,173][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:00:52,173][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:00:52,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:00:52,221][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.79it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.040 train/auc:  
                                                              0.960 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.063             
[2024-05-30 13:01:17,928][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/1>
[2024-05-30 13:01:17,928][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:01:17,931][HYDRA] 	#322 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:01:18,215][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:01:18,218][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:01:18,220][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:01:18,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:01:18,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:01:18,225][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:01:18,226][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:01:18,227][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:01:18,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:01:18,229][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:01:18,230][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:01:18,232][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:01:18,276][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.05it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.077 train/auc:  
                                                              0.905 train/f1:   
                                                              0.913             
                                                              train/precision:  
                                                              0.840             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.073             
[2024-05-30 13:01:43,539][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/2>
[2024-05-30 13:01:43,539][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:01:43,542][HYDRA] 	#323 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:01:43,827][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:01:43,829][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:01:43,831][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:01:43,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:01:43,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:01:43,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:01:43,837][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:01:43,838][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:01:43,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:01:43,841][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:01:43,841][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:01:43,843][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:01:43,885][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.71it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.039 train/auc:  
                                                              0.921 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.062             
[2024-05-30 13:02:08,926][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/3>
[2024-05-30 13:02:08,927][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:02:08,930][HYDRA] 	#324 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:02:09,217][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:02:09,219][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:02:09,222][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:02:09,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:02:09,226][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:02:09,226][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:02:09,227][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:02:09,228][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:02:09,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:02:09,231][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:02:09,231][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:02:09,233][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:02:09,276][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.12it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.044 train/auc:  
                                                              0.937 train/f1:   
                                                              0.932             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.873 train/mre:  
                                                              0.064             
[2024-05-30 13:02:34,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/4>
[2024-05-30 13:02:34,114][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:02:34,117][HYDRA] 	#325 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:02:34,399][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:02:34,401][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:02:34,404][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:02:34,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:02:34,408][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:02:34,408][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:02:34,409][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:02:34,410][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:02:34,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:02:34,413][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:02:34,413][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:02:34,415][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:02:34,457][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.67it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.043 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.061             
[2024-05-30 13:02:59,511][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/5>
[2024-05-30 13:02:59,512][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:02:59,515][HYDRA] 	#326 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:02:59,799][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:02:59,802][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:02:59,804][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:02:59,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:02:59,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:02:59,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:02:59,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:02:59,810][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:02:59,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:02:59,813][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:02:59,813][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:02:59,815][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:02:59,857][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.95it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.045 train/auc:  
                                                              0.905 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.067             
[2024-05-30 13:03:24,965][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/6>
[2024-05-30 13:03:24,966][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:03:24,968][HYDRA] 	#327 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:03:25,255][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:03:25,258][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:03:25,260][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:03:25,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:03:25,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:03:25,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:03:25,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:03:25,266][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:03:25,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:03:25,269][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:03:25,270][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:03:25,272][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:03:25,313][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.15it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.400 val/mre:    
                                                              0.049 train/auc:  
                                                              0.968 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.940             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.061             
[2024-05-30 13:03:51,013][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/7>
[2024-05-30 13:03:51,014][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:03:51,017][HYDRA] 	#328 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:03:51,311][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:03:51,314][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:03:51,316][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:03:51,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:03:51,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:03:51,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:03:51,322][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:03:51,322][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:03:51,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:03:51,328][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:03:51,328][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:03:51,330][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:03:51,414][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.64it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.038 train/auc:  
                                                              0.960 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.059             
[2024-05-30 13:04:17,615][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/8>
[2024-05-30 13:04:17,616][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:04:17,618][HYDRA] 	#329 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:04:17,909][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:04:17,911][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:04:17,913][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:04:17,914][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:04:17,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:04:17,918][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:04:17,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:04:17,919][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:04:17,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:04:17,922][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:04:17,922][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:04:17,924][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:04:17,971][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.26it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.600 val/mre:    
                                                              0.042 train/auc:  
                                                              0.929 train/f1:   
                                                              0.928             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.066             
[2024-05-30 13:04:43,586][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/9>
[2024-05-30 13:04:43,587][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:04:43,590][HYDRA] 	#330 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:04:44,006][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:04:44,009][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:04:44,011][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:04:44,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:04:44,018][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:04:44,019][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:04:44,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:04:44,020][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:04:44,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:04:44,024][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:04:44,024][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:04:44,026][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:04:44,178][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              0.968 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.061             
[2024-05-30 13:05:09,399][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/0>
[2024-05-30 13:05:09,400][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:05:09,402][HYDRA] 	#331 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:05:09,701][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:05:09,703][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:05:09,705][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:05:09,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:05:09,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:05:09,710][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:05:09,711][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:05:09,712][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:05:09,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:05:09,717][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:05:09,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:05:09,720][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:05:09,804][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.79it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.045 train/auc:  
                                                              0.937 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.910             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.063             
[2024-05-30 13:05:35,381][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/1>
[2024-05-30 13:05:35,382][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:05:35,385][HYDRA] 	#332 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:05:35,675][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:05:35,677][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:05:35,680][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:05:35,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:05:35,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:05:35,684][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:05:35,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:05:35,686][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:05:35,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:05:35,689][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:05:35,689][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:05:35,691][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:05:35,734][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.42it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.045 train/auc:  
                                                              0.976 train/f1:   
                                                              0.977             
                                                              train/precision:  
                                                              0.955             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-05-30 13:06:00,943][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/2>
[2024-05-30 13:06:00,944][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:06:00,946][HYDRA] 	#333 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:06:01,234][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:06:01,237][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:06:01,239][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:06:01,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:06:01,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:06:01,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:06:01,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:06:01,245][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:06:01,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:06:01,248][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:06:01,248][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:06:01,250][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:06:01,293][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.27it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.040 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.062             
[2024-05-30 13:06:26,535][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/3>
[2024-05-30 13:06:26,536][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:06:26,539][HYDRA] 	#334 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:06:26,837][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:06:26,839][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:06:26,842][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:06:26,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:06:26,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:06:26,847][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:06:26,848][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:06:26,848][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:06:26,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:06:26,853][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:06:26,854][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:06:26,856][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:06:26,939][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.87it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.048 train/auc:  
                                                              0.929 train/f1:   
                                                              0.933             
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.064             
[2024-05-30 13:06:51,857][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/4>
[2024-05-30 13:06:51,858][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:06:51,866][HYDRA] 	#335 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:06:52,159][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:06:52,161][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:06:52,164][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:06:52,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:06:52,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:06:52,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:06:52,170][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:06:52,171][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:06:52,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:06:52,173][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:06:52,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:06:52,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:06:52,218][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.78it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.064             
[2024-05-30 13:07:17,006][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/5>
[2024-05-30 13:07:17,006][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:07:17,009][HYDRA] 	#336 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:07:17,293][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:07:17,295][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:07:17,297][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:07:17,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:07:17,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:07:17,302][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:07:17,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:07:17,303][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:07:17,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:07:17,306][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:07:17,307][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:07:17,309][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:07:17,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.35it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.042 train/auc:  
                                                              0.960 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.953             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.060             
[2024-05-30 13:07:42,621][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/6>
[2024-05-30 13:07:42,621][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:07:42,625][HYDRA] 	#337 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:07:42,923][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:07:42,925][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:07:42,928][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:07:42,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:07:42,932][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:07:42,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:07:42,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:07:42,934][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:07:42,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:07:42,939][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:07:42,939][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:07:42,941][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:07:43,029][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.12it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.043 train/auc:  
                                                              0.921 train/f1:   
                                                              0.923             
                                                              train/precision:  
                                                              0.896             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.058             
[2024-05-30 13:08:08,713][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/7>
[2024-05-30 13:08:08,714][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:08:08,718][HYDRA] 	#338 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:08:09,007][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:08:09,010][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:08:09,012][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:08:09,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:08:09,016][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:08:09,016][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:08:09,017][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:08:09,018][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:08:09,020][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:08:09,021][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:08:09,021][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:08:09,023][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:08:09,064][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.83it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.049 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.969             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.061             
[2024-05-30 13:08:33,956][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/8>
[2024-05-30 13:08:33,956][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:08:33,959][HYDRA] 	#339 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:08:34,239][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:08:34,242][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:08:34,244][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:08:34,244][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:08:34,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:08:34,249][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:08:34,249][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:08:34,250][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:08:34,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:08:34,253][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:08:34,253][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:08:34,255][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:08:34,296][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.83it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.050 train/auc:  
                                                              0.968 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.940             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.066             
[2024-05-30 13:08:59,373][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/9>
[2024-05-30 13:08:59,374][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:08:59,377][HYDRA] 	#340 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:08:59,672][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:08:59,674][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:08:59,676][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:08:59,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:08:59,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:08:59,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:08:59,682][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:08:59,683][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:08:59,685][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:08:59,688][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:08:59,688][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:08:59,690][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:08:59,777][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.30it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.052 train/auc:  
                                                              0.960 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.066             
[2024-05-30 13:09:24,884][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/0>
[2024-05-30 13:09:24,884][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:09:24,887][HYDRA] 	#341 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:09:25,178][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:09:25,180][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:09:25,182][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:09:25,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:09:25,186][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:09:25,187][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:09:25,187][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:09:25,188][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:09:25,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:09:25,191][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:09:25,192][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:09:25,194][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:09:25,236][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.58it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.042 train/auc:  
                                                              0.929 train/f1:   
                                                              0.928             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.063             
[2024-05-30 13:09:50,304][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/1>
[2024-05-30 13:09:50,305][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:09:50,308][HYDRA] 	#342 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:09:50,594][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:09:50,596][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:09:50,598][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:09:50,599][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:09:50,602][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:09:50,603][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:09:50,604][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:09:50,605][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:09:50,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:09:50,607][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:09:50,608][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:09:50,610][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:09:50,654][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.29it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.062             
[2024-05-30 13:10:15,868][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/2>
[2024-05-30 13:10:15,869][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:10:15,872][HYDRA] 	#343 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:10:16,156][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:10:16,158][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:10:16,160][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:10:16,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:10:16,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:10:16,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:10:16,166][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:10:16,167][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:10:16,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:10:16,169][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:10:16,170][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:10:16,172][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:10:16,214][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.26it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.038 train/auc:  
                                                              0.968 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.954             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.062             
[2024-05-30 13:10:41,701][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/3>
[2024-05-30 13:10:41,702][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:10:41,705][HYDRA] 	#344 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:10:41,998][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:10:42,000][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:10:42,002][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:10:42,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:10:42,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:10:42,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:10:42,008][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:10:42,008][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:10:42,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:10:42,011][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:10:42,012][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:10:42,014][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:10:42,056][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.01it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.044 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.065             
[2024-05-30 13:11:07,428][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/4>
[2024-05-30 13:11:07,429][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:11:07,432][HYDRA] 	#345 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:11:07,727][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:11:07,729][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:11:07,731][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:11:07,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:11:07,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:11:07,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:11:07,737][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:11:07,737][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:11:07,739][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:11:07,742][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:11:07,742][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:11:07,745][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:11:07,830][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.12it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.044 train/auc:  
                                                              0.937 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.062             
[2024-05-30 13:11:32,813][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/5>
[2024-05-30 13:11:32,813][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:11:32,816][HYDRA] 	#346 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:11:33,100][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:11:33,102][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:11:33,104][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:11:33,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:11:33,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:11:33,109][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:11:33,110][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:11:33,111][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:11:33,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:11:33,113][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:11:33,114][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:11:33,116][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:11:33,157][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.65it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.044 train/auc:  
                                                              0.921 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.058             
[2024-05-30 13:11:58,154][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/6>
[2024-05-30 13:11:58,155][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:11:58,159][HYDRA] 	#347 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:11:58,441][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:11:58,443][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:11:58,446][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:11:58,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:11:58,450][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:11:58,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:11:58,451][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:11:58,452][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:11:58,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:11:58,454][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:11:58,455][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:11:58,457][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:11:58,503][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.73it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.050 train/auc:  
                                                              0.937 train/f1:   
                                                              0.934             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              0.068             
[2024-05-30 13:12:24,313][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/7>
[2024-05-30 13:12:24,314][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:12:24,317][HYDRA] 	#348 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:12:24,602][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:12:24,605][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:12:24,607][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:12:24,607][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:12:24,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:12:24,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:12:24,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:12:24,613][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:12:24,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:12:24,619][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:12:24,619][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:12:24,621][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:12:24,707][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.45it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.044 train/auc:  
                                                              0.960 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.953             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.058             
[2024-05-30 13:12:49,906][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/8>
[2024-05-30 13:12:49,906][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:12:49,909][HYDRA] 	#349 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:12:50,198][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:12:50,201][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:12:50,204][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:12:50,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:12:50,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:12:50,210][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:12:50,211][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:12:50,211][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:12:50,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:12:50,214][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:12:50,215][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:12:50,218][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:12:50,262][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.46it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.047 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.067             
[2024-05-30 13:13:15,590][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/9>
[2024-05-30 13:13:15,591][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:13:15,593][HYDRA] 	#350 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:13:15,884][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:13:15,886][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:13:15,889][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:13:15,889][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:13:15,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:13:15,893][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:13:15,894][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:13:15,895][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:13:15,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:13:15,897][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:13:15,898][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:13:15,900][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:13:15,942][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.01it/s v_num: 0.000      
                                                              val/auc: 0.746    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.714 val/mre: nan
                                                              train/auc: 0.789  
                                                              train/f1: 0.786   
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              nan               
[2024-05-30 13:13:39,793][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/0>
[2024-05-30 13:13:39,794][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:13:39,796][HYDRA] 	#351 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:13:40,239][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:13:40,241][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:13:40,243][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:13:40,244][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:13:40,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:13:40,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:13:40,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:13:40,251][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:13:40,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:13:40,254][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:13:40,254][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:13:40,256][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:13:40,405][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.05it/s v_num: 0.000      
                                                              val/auc: 0.405    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 0.895  
                                                              train/f1: 0.903   
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              nan               
[2024-05-30 13:14:04,090][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/1>
[2024-05-30 13:14:04,090][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:14:04,094][HYDRA] 	#352 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:14:04,642][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:14:04,647][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:14:04,649][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:14:04,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:14:04,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:14:04,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:14:04,657][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:14:04,658][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:14:04,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:14:04,661][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:14:04,661][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:14:04,663][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:14:04,804][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.99it/s v_num: 0.000      
                                                              val/auc: 0.778    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              1.000 val/mre: nan
                                                              train/auc: 0.737  
                                                              train/f1: 0.746   
                                                              train/precision:  
                                                              0.721             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              nan               
[2024-05-30 13:14:28,274][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/2>
[2024-05-30 13:14:28,275][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:14:28,278][HYDRA] 	#353 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:14:28,569][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:14:28,572][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:14:28,574][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:14:28,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:14:28,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:14:28,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:14:28,580][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:14:28,580][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:14:28,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:14:28,583][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:14:28,583][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:14:28,586][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:14:28,628][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.62it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 0.886  
                                                              train/f1: 0.887   
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              nan               
[2024-05-30 13:14:52,140][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/3>
[2024-05-30 13:14:52,141][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:14:52,144][HYDRA] 	#354 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:14:52,440][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:14:52,442][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:14:52,444][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:14:52,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:14:52,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:14:52,449][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:14:52,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:14:52,450][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:14:52,452][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:14:52,455][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:14:52,456][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:14:52,458][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:14:52,549][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.60it/s v_num: 0.000      
                                                              val/auc: 0.659    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.833  
                                                              train/f1: 0.848   
                                                              train/precision:  
                                                              0.779             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              nan               
[2024-05-30 13:15:15,804][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/4>
[2024-05-30 13:15:15,805][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:15:15,808][HYDRA] 	#355 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:15:16,095][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:15:16,097][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:15:16,099][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:15:16,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:15:16,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:15:16,104][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:15:16,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:15:16,106][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:15:16,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:15:16,108][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:15:16,109][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:15:16,113][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:15:16,155][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.11it/s v_num: 0.000      
                                                              val/auc: 0.492    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.763  
                                                              train/f1: 0.803   
                                                              train/precision:  
                                                              0.688             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              nan               
[2024-05-30 13:15:39,767][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/5>
[2024-05-30 13:15:39,768][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:15:39,770][HYDRA] 	#356 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:15:40,059][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:15:40,062][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:15:40,064][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:15:40,064][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:15:40,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:15:40,068][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:15:40,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:15:40,070][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:15:40,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:15:40,073][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:15:40,073][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:15:40,075][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:15:40,119][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.08it/s v_num: 0.000      
                                                              val/auc: 0.492    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.807  
                                                              train/f1: 0.833   
                                                              train/precision:  
                                                              0.733             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              nan               
[2024-05-30 13:16:03,301][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/6>
[2024-05-30 13:16:03,302][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:16:03,305][HYDRA] 	#357 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:16:03,590][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:16:03,593][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:16:03,595][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:16:03,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:16:03,599][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:16:03,600][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:16:03,601][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:16:03,601][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:16:03,603][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:16:03,604][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:16:03,604][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:16:03,606][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:16:03,648][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.28it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.711  
                                                              train/f1: 0.680   
                                                              train/precision:  
                                                              0.761             
                                                              train/recall:     
                                                              0.614 train/mre:  
                                                              nan               
[2024-05-30 13:16:27,472][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/7>
[2024-05-30 13:16:27,473][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:16:27,475][HYDRA] 	#358 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:16:27,763][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:16:27,765][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:16:27,767][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:16:27,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:16:27,771][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:16:27,772][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:16:27,773][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:16:27,774][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:16:27,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:16:27,777][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:16:27,777][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:16:27,779][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:16:27,821][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.67it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.781  
                                                              train/f1: 0.806   
                                                              train/precision:  
                                                              0.722             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              nan               
[2024-05-30 13:16:51,078][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/8>
[2024-05-30 13:16:51,079][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:16:51,082][HYDRA] 	#359 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:16:51,369][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:16:51,372][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:16:51,374][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:16:51,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:16:51,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:16:51,379][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:16:51,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:16:51,380][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:16:51,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:16:51,383][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:16:51,383][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:16:51,386][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:16:51,427][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.69it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.807  
                                                              train/f1: 0.817   
                                                              train/precision:  
                                                              0.778             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              nan               
[2024-05-30 13:17:14,636][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/9>
[2024-05-30 13:17:14,637][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:17:14,640][HYDRA] 	#360 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:17:14,929][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:17:14,932][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:17:14,934][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:17:14,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:17:14,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:17:14,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:17:14,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:17:14,941][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:17:14,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:17:14,943][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:17:14,944][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:17:14,946][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:17:14,989][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.52it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.044 train/auc:  
                                                              0.693 train/f1:   
                                                              0.690             
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.684 train/mre:  
                                                              0.048             
[2024-05-30 13:17:38,218][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/0>
[2024-05-30 13:17:38,219][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:17:38,222][HYDRA] 	#361 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:17:38,520][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:17:38,522][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:17:38,525][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:17:38,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:17:38,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:17:38,529][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:17:38,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:17:38,531][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:17:38,533][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:17:38,535][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:17:38,536][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:17:38,540][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:17:38,583][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.36it/s v_num: 0.000      
                                                              val/auc: 0.460    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.047 train/auc:  
                                                              0.860 train/f1:   
                                                              0.864             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.055             
[2024-05-30 13:18:01,874][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/1>
[2024-05-30 13:18:01,875][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:18:01,879][HYDRA] 	#362 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:18:02,168][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:18:02,170][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:18:02,173][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:18:02,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:18:02,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:18:02,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:18:02,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:18:02,179][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:18:02,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:18:02,184][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:18:02,184][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:18:02,187][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:18:02,230][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.49it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.037 train/auc:  
                                                              0.649 train/f1:   
                                                              0.714             
                                                              train/precision:  
                                                              0.602             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.046             
[2024-05-30 13:18:25,738][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/2>
[2024-05-30 13:18:25,738][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:18:25,743][HYDRA] 	#363 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:18:26,033][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:18:26,036][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:18:26,038][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:18:26,038][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:18:26,042][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:18:26,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:18:26,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:18:26,044][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:18:26,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:18:26,047][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:18:26,048][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:18:26,050][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:18:26,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.02it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.033 train/auc:  
                                                              0.561 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.537             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.044             
[2024-05-30 13:18:49,667][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/3>
[2024-05-30 13:18:49,667][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:18:49,670][HYDRA] 	#364 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:18:49,956][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:18:49,958][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:18:49,961][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:18:49,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:18:49,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:18:49,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:18:49,966][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:18:49,967][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:18:49,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:18:49,970][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:18:49,971][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:18:49,973][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:18:50,015][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.19it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.042 train/auc:  
                                                              0.667 train/f1:   
                                                              0.689             
                                                              train/precision:  
                                                              0.646             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.055             
[2024-05-30 13:19:13,648][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/4>
[2024-05-30 13:19:13,648][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:19:13,651][HYDRA] 	#365 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:19:13,940][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:19:13,943][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:19:13,945][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:19:13,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:19:13,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:19:13,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:19:13,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:19:13,952][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:19:13,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:19:13,956][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:19:13,957][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:19:13,959][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:19:14,004][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.90it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.038 train/auc:  
                                                              0.526 train/f1:   
                                                              0.620             
                                                              train/precision:  
                                                              0.518             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.042             
[2024-05-30 13:19:37,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/5>
[2024-05-30 13:19:37,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:19:37,456][HYDRA] 	#366 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:19:37,740][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:19:37,742][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:19:37,744][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:19:37,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:19:37,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:19:37,749][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:19:37,750][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:19:37,751][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:19:37,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:19:37,753][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:19:37,754][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:19:37,756][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:19:37,797][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.16it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.036 train/auc:  
                                                              0.509 train/f1:   
                                                              0.243             
                                                              train/precision:  
                                                              0.529             
                                                              train/recall:     
                                                              0.158 train/mre:  
                                                              0.048             
[2024-05-30 13:20:01,022][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/6>
[2024-05-30 13:20:01,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:20:01,026][HYDRA] 	#367 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:20:01,319][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:20:01,322][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:20:01,324][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:20:01,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:20:01,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:20:01,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:20:01,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:20:01,330][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:20:01,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:20:01,333][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:20:01,334][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:20:01,337][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:20:02,007][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.64it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.048 train/auc:  
                                                              0.693 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.729             
                                                              train/recall:     
                                                              0.614 train/mre:  
                                                              0.046             
[2024-05-30 13:20:26,223][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/7>
[2024-05-30 13:20:26,223][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:20:26,226][HYDRA] 	#368 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:20:26,508][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:20:26,510][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:20:26,512][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:20:26,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:20:26,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:20:26,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:20:26,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:20:26,518][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:20:26,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:20:26,521][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:20:26,521][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:20:26,523][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:20:26,569][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.68it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.037 train/auc:  
                                                              0.728 train/f1:   
                                                              0.730             
                                                              train/precision:  
                                                              0.724             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.043             
[2024-05-30 13:20:49,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/8>
[2024-05-30 13:20:49,841][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:20:49,844][HYDRA] 	#369 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:20:50,133][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:20:50,135][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:20:50,137][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:20:50,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:20:50,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:20:50,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:20:50,143][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:20:50,144][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:20:50,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:20:50,146][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:20:50,147][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:20:50,149][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:20:50,192][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.76it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.438 val/recall: 
                                                              1.000 val/mre:    
                                                              0.047 train/auc:  
                                                              0.544 train/f1:   
                                                              0.600             
                                                              train/precision:  
                                                              0.534             
                                                              train/recall:     
                                                              0.684 train/mre:  
                                                              0.060             
[2024-05-30 13:21:13,675][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/9>
[2024-05-30 13:21:13,676][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:21:13,684][HYDRA] 	#370 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:21:13,977][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:21:13,979][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:21:13,981][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:21:13,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:21:13,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:21:13,986][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:21:13,987][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:21:13,987][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:21:13,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:21:13,991][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:21:13,991][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:21:13,993][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:21:14,086][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.00it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.037 train/auc:  
                                                              0.763 train/f1:   
                                                              0.769             
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.789 train/mre:  
                                                              0.046             
[2024-05-30 13:21:37,841][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/0>
[2024-05-30 13:21:37,842][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:21:37,847][HYDRA] 	#371 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:21:38,152][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:21:38,155][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:21:38,157][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:21:38,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:21:38,162][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:21:38,162][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:21:38,163][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:21:38,164][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:21:38,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:21:38,171][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:21:38,171][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:21:38,173][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:21:38,219][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.92it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.043 train/auc:  
                                                              0.868 train/f1:   
                                                              0.876             
                                                              train/precision:  
                                                              0.828             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.058             
[2024-05-30 13:22:01,927][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/1>
[2024-05-30 13:22:01,927][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:22:01,931][HYDRA] 	#372 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:22:02,220][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:22:02,222][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:22:02,224][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:22:02,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:22:02,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:22:02,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:22:02,230][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:22:02,230][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:22:02,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:22:02,233][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:22:02,233][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:22:02,236][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:22:02,283][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.95it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.047 train/auc:  
                                                              0.693 train/f1:   
                                                              0.711             
                                                              train/precision:  
                                                              0.672             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              0.057             
[2024-05-30 13:22:25,958][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/2>
[2024-05-30 13:22:25,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:22:25,973][HYDRA] 	#373 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:22:26,275][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:22:26,279][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:22:26,281][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:22:26,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:22:26,285][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:22:26,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:22:26,287][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:22:26,287][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:22:26,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:22:26,292][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:22:26,293][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:22:26,297][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:22:26,384][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.64it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.031 train/auc:  
                                                              0.649 train/f1:   
                                                              0.701             
                                                              train/precision:  
                                                              0.610             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.041             
[2024-05-30 13:22:50,021][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/3>
[2024-05-30 13:22:50,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:22:50,025][HYDRA] 	#374 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:22:50,329][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:22:50,333][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:22:50,336][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:22:50,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:22:50,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:22:50,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:22:50,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:22:50,342][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:22:50,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:22:50,347][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:22:50,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:22:50,350][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:22:50,397][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.42it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.046 train/auc:  
                                                              0.798 train/f1:   
                                                              0.819             
                                                              train/precision:  
                                                              0.743             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.056             
[2024-05-30 13:23:13,839][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/4>
[2024-05-30 13:23:13,840][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:23:13,842][HYDRA] 	#375 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:23:14,130][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:23:14,132][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:23:14,134][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:23:14,135][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:23:14,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:23:14,139][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:23:14,140][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:23:14,141][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:23:14,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:23:14,143][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:23:14,143][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:23:14,145][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:23:14,188][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.55it/s v_num: 0.000      
                                                              val/auc: 0.579    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.714 val/mre:    
                                                              0.032 train/auc:  
                                                              0.491 train/f1:   
                                                              0.586             
                                                              train/precision:  
                                                              0.494             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              0.038             
[2024-05-30 13:23:37,412][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/5>
[2024-05-30 13:23:37,413][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:23:37,416][HYDRA] 	#376 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:23:37,703][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:23:37,705][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:23:37,708][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:23:37,708][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:23:37,711][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:23:37,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:23:37,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:23:37,714][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:23:37,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:23:37,717][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:23:37,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:23:37,720][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:23:37,801][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.50it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.034 train/auc:  
                                                              0.719 train/f1:   
                                                              0.758             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.044             
[2024-05-30 13:24:01,041][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/6>
[2024-05-30 13:24:01,042][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:24:01,045][HYDRA] 	#377 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:24:01,339][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:24:01,342][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:24:01,344][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:24:01,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:24:01,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:24:01,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:24:01,349][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:24:01,350][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:24:01,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:24:01,354][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:24:01,355][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:24:01,357][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:24:01,402][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.52it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.037 train/auc:  
                                                              0.588 train/f1:   
                                                              0.598             
                                                              train/precision:  
                                                              0.583             
                                                              train/recall:     
                                                              0.614 train/mre:  
                                                              0.038             
[2024-05-30 13:24:25,046][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/7>
[2024-05-30 13:24:25,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:24:25,050][HYDRA] 	#378 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:24:25,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:24:25,336][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:24:25,338][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:24:25,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:24:25,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:24:25,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:24:25,344][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:24:25,345][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:24:25,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:24:25,347][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:24:25,348][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:24:25,350][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:24:25,391][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.19it/s v_num: 0.000      
                                                              val/auc: 0.492    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre:    
                                                              0.037 train/auc:  
                                                              0.737 train/f1:   
                                                              0.754             
                                                              train/precision:  
                                                              0.708             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.043             
[2024-05-30 13:24:48,715][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/8>
[2024-05-30 13:24:48,716][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:24:48,719][HYDRA] 	#379 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:24:49,010][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:24:49,013][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:24:49,015][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:24:49,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:24:49,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:24:49,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:24:49,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:24:49,021][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:24:49,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:24:49,024][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:24:49,024][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:24:49,028][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:24:49,073][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.86it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              1.000 val/mre:    
                                                              0.047 train/auc:  
                                                              0.711 train/f1:   
                                                              0.769             
                                                              train/precision:  
                                                              0.640             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.058             
[2024-05-30 13:25:12,274][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/9>
[2024-05-30 13:25:12,274][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:25:12,277][HYDRA] 	#380 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:25:12,574][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:25:12,576][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:25:12,578][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:25:12,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:25:12,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:25:12,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:25:12,584][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:25:12,585][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:25:12,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:25:12,587][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:25:12,587][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:25:12,590][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:25:12,635][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.65it/s v_num: 0.000      
                                                              val/auc: 0.714    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.429 val/mre:    
                                                              0.036 train/auc:  
                                                              0.851 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.778             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.044             
[2024-05-30 13:25:36,134][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/0>
[2024-05-30 13:25:36,134][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:25:36,137][HYDRA] 	#381 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:25:36,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:25:36,468][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:25:36,472][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:25:36,473][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:25:36,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:25:36,477][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:25:36,478][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:25:36,479][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:25:36,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:25:36,482][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:25:36,482][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:25:36,484][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:25:36,533][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.96it/s v_num: 0.000      
                                                              val/auc: 0.635    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.714 val/mre:    
                                                              0.045 train/auc:  
                                                              0.939 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.946             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.055             
[2024-05-30 13:26:00,061][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/1>
[2024-05-30 13:26:00,062][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:26:00,066][HYDRA] 	#382 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:26:00,353][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:26:00,356][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:26:00,358][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:26:00,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:26:00,362][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:26:00,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:26:00,364][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:26:00,364][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:26:00,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:26:00,367][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:26:00,367][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:26:00,369][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:26:00,459][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.11it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.038 train/auc:  
                                                              0.789 train/f1:   
                                                              0.793             
                                                              train/precision:  
                                                              0.780             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.046             
[2024-05-30 13:26:23,760][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/2>
[2024-05-30 13:26:23,761][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:26:23,765][HYDRA] 	#383 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:26:24,048][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:26:24,050][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:26:24,053][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:26:24,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:26:24,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:26:24,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:26:24,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:26:24,059][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:26:24,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:26:24,062][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:26:24,062][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:26:24,065][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:26:24,107][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.66it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.033 train/auc:  
                                                              0.737 train/f1:   
                                                              0.762             
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.042             
[2024-05-30 13:26:47,313][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/3>
[2024-05-30 13:26:47,313][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:26:47,316][HYDRA] 	#384 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:26:47,603][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:26:47,606][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:26:47,608][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:26:47,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:26:47,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:26:47,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:26:47,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:26:47,614][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:26:47,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:26:47,617][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:26:47,617][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:26:47,620][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:26:47,664][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.45it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.043 train/auc:  
                                                              0.588 train/f1:   
                                                              0.680             
                                                              train/precision:  
                                                              0.556             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.052             
[2024-05-30 13:27:11,056][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/4>
[2024-05-30 13:27:11,056][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:27:11,059][HYDRA] 	#385 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:27:11,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:27:11,356][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:27:11,359][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:27:11,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:27:11,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:27:11,364][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:27:11,364][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:27:11,365][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:27:11,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:27:11,368][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:27:11,368][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:27:11,372][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:27:11,455][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.94it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.032 train/auc:  
                                                              0.632 train/f1:   
                                                              0.650             
                                                              train/precision:  
                                                              0.619             
                                                              train/recall:     
                                                              0.684 train/mre:  
                                                              0.040             
[2024-05-30 13:27:34,804][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/5>
[2024-05-30 13:27:34,804][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:27:34,807][HYDRA] 	#386 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:27:35,107][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:27:35,109][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:27:35,111][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:27:35,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:27:35,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:27:35,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:27:35,117][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:27:35,118][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:27:35,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:27:35,123][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:27:35,123][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:27:35,125][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:27:35,171][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.12it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              1.000 val/mre:    
                                                              0.037 train/auc:  
                                                              0.746 train/f1:   
                                                              0.797             
                                                              train/precision:  
                                                              0.663             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.046             
[2024-05-30 13:27:58,938][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/6>
[2024-05-30 13:27:58,938][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:27:58,941][HYDRA] 	#387 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:27:59,225][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:27:59,228][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:27:59,230][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:27:59,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:27:59,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:27:59,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:27:59,235][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:27:59,236][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:27:59,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:27:59,239][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:27:59,239][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:27:59,241][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:27:59,284][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.27it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.037 train/auc:  
                                                              0.579 train/f1:   
                                                              0.529             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.474 train/mre:  
                                                              0.036             
[2024-05-30 13:28:23,462][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/7>
[2024-05-30 13:28:23,463][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:28:23,468][HYDRA] 	#388 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:28:23,754][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:28:23,756][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:28:23,758][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:28:23,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:28:23,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:28:23,763][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:28:23,764][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:28:23,764][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:28:23,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:28:23,767][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:28:23,767][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:28:23,769][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:28:23,854][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.42it/s v_num: 0.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.047 train/auc:  
                                                              0.667 train/f1:   
                                                              0.620             
                                                              train/precision:  
                                                              0.721             
                                                              train/recall:     
                                                              0.544 train/mre:  
                                                              0.042             
[2024-05-30 13:28:47,317][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/8>
[2024-05-30 13:28:47,318][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:28:47,321][HYDRA] 	#389 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:28:47,606][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:28:47,609][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:28:47,611][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:28:47,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:28:47,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:28:47,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:28:47,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:28:47,618][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:28:47,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:28:47,623][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:28:47,623][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:28:47,625][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:28:47,670][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.73it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.044 train/auc:  
                                                              0.904 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.883             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.049             
[2024-05-30 13:29:11,267][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/9>
[2024-05-30 13:29:11,267][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:29:11,270][HYDRA] 	#390 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:29:11,571][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:29:11,574][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:29:11,576][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:29:11,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:29:11,580][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:29:11,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:29:11,582][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:29:11,582][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:29:11,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:29:11,585][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:29:11,585][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:29:11,588][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:29:11,634][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.90it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.034 train/auc:  
                                                              0.789 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.044             
[2024-05-30 13:29:36,130][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/0>
[2024-05-30 13:29:36,130][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:29:36,133][HYDRA] 	#391 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:29:36,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:29:36,439][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:29:36,441][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:29:36,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:29:36,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:29:36,446][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:29:36,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:29:36,448][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:29:36,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:29:36,450][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:29:36,451][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:29:36,453][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:29:36,548][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.05it/s v_num: 0.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.043 train/auc:  
                                                              0.921 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.055             
[2024-05-30 13:30:00,024][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/1>
[2024-05-30 13:30:00,025][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:30:00,028][HYDRA] 	#392 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:30:00,330][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:30:00,333][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:30:00,335][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:30:00,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:30:00,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:30:00,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:30:00,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:30:00,341][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:30:00,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:30:00,346][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:30:00,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:30:00,349][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-05-30 13:30:00,392][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.58it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.039 train/auc:  
                                                              0.684 train/f1:   
                                                              0.705             
                                                              train/precision:  
                                                              0.662             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              0.052             
[2024-05-30 13:30:24,011][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/2>
[2024-05-30 13:30:24,012][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:30:24,015][HYDRA] 	#393 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:30:24,302][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:30:24,305][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:30:24,307][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:30:24,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:30:24,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:30:24,311][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:30:24,312][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:30:24,313][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:30:24,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:30:24,315][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:30:24,316][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:30:24,318][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:30:24,359][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.02it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.035 train/auc:  
                                                              0.728 train/f1:   
                                                              0.739             
                                                              train/precision:  
                                                              0.710             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.041             
[2024-05-30 13:30:47,800][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/3>
[2024-05-30 13:30:47,801][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:30:47,807][HYDRA] 	#394 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:30:48,103][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:30:48,106][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:30:48,108][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:30:48,109][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:30:48,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:30:48,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:30:48,114][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:30:48,115][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:30:48,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:30:48,118][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:30:48,118][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:30:48,120][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:30:48,163][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.72it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.037 train/auc:  
                                                              0.772 train/f1:   
                                                              0.780             
                                                              train/precision:  
                                                              0.754             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.049             
[2024-05-30 13:31:11,512][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/4>
[2024-05-30 13:31:11,513][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:31:11,516][HYDRA] 	#395 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:31:11,817][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:31:11,819][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:31:11,822][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:31:11,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:31:11,826][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:31:11,827][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:31:11,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:31:11,828][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:31:11,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:31:11,834][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:31:11,834][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:31:11,836][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:31:11,882][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.50it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.034 train/auc:  
                                                              0.588 train/f1:   
                                                              0.630             
                                                              train/precision:  
                                                              0.571             
                                                              train/recall:     
                                                              0.702 train/mre:  
                                                              0.040             
[2024-05-30 13:31:35,662][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/5>
[2024-05-30 13:31:35,663][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:31:35,666][HYDRA] 	#396 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:31:35,960][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:31:35,962][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:31:35,965][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:31:35,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:31:35,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:31:35,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:31:35,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:31:35,971][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:31:35,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:31:35,974][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:31:35,974][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:31:35,976][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:31:36,019][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.03it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.036 train/auc:  
                                                              0.825 train/f1:   
                                                              0.841             
                                                              train/precision:  
                                                              0.768             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.046             
[2024-05-30 13:31:59,575][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/6>
[2024-05-30 13:31:59,576][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:31:59,578][HYDRA] 	#397 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:31:59,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:31:59,871][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:31:59,873][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:31:59,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:31:59,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:31:59,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:31:59,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:31:59,879][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:31:59,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:31:59,882][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:31:59,882][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:31:59,884][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:31:59,978][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.46it/s v_num: 0.000      
                                                              val/auc: 0.651    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.857 val/mre:    
                                                              0.042 train/auc:  
                                                              0.570 train/f1:   
                                                              0.620             
                                                              train/precision:  
                                                              0.556             
                                                              train/recall:     
                                                              0.702 train/mre:  
                                                              0.042             
[2024-05-30 13:32:23,430][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/7>
[2024-05-30 13:32:23,431][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:32:23,435][HYDRA] 	#398 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:32:23,723][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:32:23,726][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:32:23,728][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:32:23,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:32:23,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:32:23,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:32:23,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:32:23,734][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:32:23,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:32:23,737][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:32:23,737][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:32:23,740][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:32:23,782][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.29it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.037 train/auc:  
                                                              0.684 train/f1:   
                                                              0.654             
                                                              train/precision:  
                                                              0.723             
                                                              train/recall:     
                                                              0.596 train/mre:  
                                                              0.041             
[2024-05-30 13:32:47,778][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/8>
[2024-05-30 13:32:47,779][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:32:47,781][HYDRA] 	#399 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:32:48,082][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:32:48,084][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:32:48,087][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:32:48,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:32:48,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:32:48,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:32:48,092][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:32:48,093][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:32:48,094][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:32:48,095][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:32:48,096][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:32:48,098][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:32:48,147][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.82it/s v_num: 0.000      
                                                              val/auc: 0.460    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.036 train/auc:  
                                                              0.789 train/f1:   
                                                              0.826             
                                                              train/precision:  
                                                              0.704             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.065             
[2024-05-30 13:33:12,482][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/9>
[2024-05-30 13:33:12,483][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:33:12,486][HYDRA] 	#400 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:33:12,781][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:33:12,784][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:33:12,786][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:33:12,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:33:12,790][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:33:12,791][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:33:12,791][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:33:12,792][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:33:12,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:33:12,795][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:33:12,795][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:33:12,797][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:33:12,888][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.50it/s v_num: 0.000      
                                                              val/auc: 0.365    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.286 val/mre:    
                                                              0.039 train/auc:  
                                                              0.754 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.684             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.043             
[2024-05-30 13:33:37,958][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/0>
[2024-05-30 13:33:37,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:33:37,961][HYDRA] 	#401 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:33:38,254][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:33:38,257][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:33:38,259][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:33:38,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:33:38,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:33:38,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:33:38,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:33:38,267][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:33:38,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:33:38,286][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:33:38,287][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:33:38,291][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:33:38,339][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.62it/s v_num: 0.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.043 train/auc:  
                                                              0.930 train/f1:   
                                                              0.927             
                                                              train/precision:  
                                                              0.962             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.049             
[2024-05-30 13:34:01,901][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/1>
[2024-05-30 13:34:01,901][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:34:01,906][HYDRA] 	#402 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:34:02,197][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:34:02,200][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:34:02,202][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:34:02,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:34:02,206][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:34:02,207][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:34:02,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:34:02,208][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:34:02,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:34:02,211][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:34:02,211][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:34:02,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:34:02,258][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.04it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.035 train/auc:  
                                                              0.807 train/f1:   
                                                              0.828             
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.049             
[2024-05-30 13:34:25,567][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/2>
[2024-05-30 13:34:25,568][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:34:25,571][HYDRA] 	#403 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:34:25,867][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:34:25,870][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:34:25,872][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:34:25,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:34:25,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:34:25,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:34:25,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:34:25,878][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:34:25,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:34:25,881][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:34:25,881][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:34:25,883][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:34:25,970][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.39it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre:    
                                                              0.032 train/auc:  
                                                              0.711 train/f1:   
                                                              0.740             
                                                              train/precision:  
                                                              0.671             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.041             
[2024-05-30 13:34:49,593][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/3>
[2024-05-30 13:34:49,594][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:34:49,598][HYDRA] 	#404 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:34:49,899][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:34:49,901][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:34:49,903][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:34:49,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:34:49,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:34:49,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:34:49,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:34:49,910][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:34:49,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:34:49,915][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:34:49,915][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:34:49,918][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:34:49,964][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.51it/s v_num: 0.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.035 train/auc:  
                                                              0.798 train/f1:   
                                                              0.819             
                                                              train/precision:  
                                                              0.743             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.051             
[2024-05-30 13:35:13,860][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/4>
[2024-05-30 13:35:13,861][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:35:13,864][HYDRA] 	#405 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:35:14,150][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:35:14,152][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:35:14,154][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:35:14,155][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:35:14,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:35:14,159][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:35:14,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:35:14,161][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:35:14,162][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:35:14,163][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:35:14,163][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:35:14,166][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:35:14,208][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.50it/s v_num: 0.000      
                                                              val/auc: 0.571    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.143 val/mre:    
                                                              0.040 train/auc:  
                                                              0.579 train/f1:   
                                                              0.593             
                                                              train/precision:  
                                                              0.574             
                                                              train/recall:     
                                                              0.614 train/mre:  
                                                              0.041             
[2024-05-30 13:35:37,843][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/5>
[2024-05-30 13:35:37,843][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:35:37,847][HYDRA] 	#406 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:35:38,137][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:35:38,139][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:35:38,142][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:35:38,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:35:38,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:35:38,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:35:38,147][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:35:38,148][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:35:38,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:35:38,151][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:35:38,151][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:35:38,154][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:35:38,238][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.55it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.033 train/auc:  
                                                              0.482 train/f1:   
                                                              0.272             
                                                              train/precision:  
                                                              0.458             
                                                              train/recall:     
                                                              0.193 train/mre:  
                                                              0.047             
[2024-05-30 13:36:01,959][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/6>
[2024-05-30 13:36:01,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:36:01,962][HYDRA] 	#407 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:36:02,253][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:36:02,255][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:36:02,257][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:36:02,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:36:02,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:36:02,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:36:02,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:36:02,263][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:36:02,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:36:02,268][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:36:02,268][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:36:02,270][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:36:02,315][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.59it/s v_num: 0.000      
                                                              val/auc: 0.651    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.857 val/mre:    
                                                              0.037 train/auc:  
                                                              0.693 train/f1:   
                                                              0.685             
                                                              train/precision:  
                                                              0.704             
                                                              train/recall:     
                                                              0.667 train/mre:  
                                                              0.038             
[2024-05-30 13:36:26,497][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/7>
[2024-05-30 13:36:26,498][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:36:26,501][HYDRA] 	#408 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:36:27,419][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:36:27,422][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:36:27,424][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:36:27,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:36:27,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:36:27,429][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:36:27,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:36:27,430][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:36:27,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:36:27,433][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:36:27,433][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:36:27,435][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:36:27,479][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.41it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.034 train/auc:  
                                                              0.693 train/f1:   
                                                              0.685             
                                                              train/precision:  
                                                              0.704             
                                                              train/recall:     
                                                              0.667 train/mre:  
                                                              0.039             
[2024-05-30 13:36:51,225][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/8>
[2024-05-30 13:36:51,226][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:36:51,228][HYDRA] 	#409 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:36:51,516][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:36:51,518][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:36:51,521][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:36:51,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:36:51,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:36:51,525][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:36:51,526][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:36:51,527][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:36:51,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:36:51,530][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:36:51,530][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:36:51,532][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:36:51,574][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.52it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.036 train/auc:  
                                                              0.851 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.770             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.053             
[2024-05-30 13:37:15,120][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/9>
[2024-05-30 13:37:15,121][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:37:15,124][HYDRA] 	#410 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:37:15,417][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:37:15,419][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:37:15,422][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:37:15,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:37:15,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:37:15,426][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:37:15,427][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:37:15,428][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:37:15,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:37:15,433][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:37:15,433][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:37:15,436][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:37:15,484][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.47it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.035 train/auc:  
                                                              0.860 train/f1:   
                                                              0.869             
                                                              train/precision:  
                                                              0.815             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.043             
[2024-05-30 13:37:39,074][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/0>
[2024-05-30 13:37:39,075][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:37:39,077][HYDRA] 	#411 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:37:39,363][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:37:39,366][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:37:39,368][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:37:39,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:37:39,372][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:37:39,373][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:37:39,373][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:37:39,374][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:37:39,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:37:39,377][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:37:39,377][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:37:39,380][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:37:39,421][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.19it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.037 train/auc:  
                                                              0.974 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.050             
[2024-05-30 13:38:03,548][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/1>
[2024-05-30 13:38:03,549][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:38:03,552][HYDRA] 	#412 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:38:03,848][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:38:03,851][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:38:03,853][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:38:03,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:38:03,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:38:03,858][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:38:03,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:38:03,859][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:38:03,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:38:03,862][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:38:03,863][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:38:03,865][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:38:03,949][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.18it/s v_num: 0.000      
                                                              val/auc: 0.762    
                                                              val/f1: 0.750     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.857 val/mre:    
                                                              0.037 train/auc:  
                                                              0.833 train/f1:   
                                                              0.848             
                                                              train/precision:  
                                                              0.779             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.048             
[2024-05-30 13:38:27,472][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/2>
[2024-05-30 13:38:27,473][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:38:27,476][HYDRA] 	#413 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:38:27,763][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:38:27,766][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:38:27,768][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:38:27,769][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:38:27,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:38:27,773][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:38:27,774][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:38:27,775][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:38:27,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:38:27,777][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:38:27,778][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:38:27,780][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:38:27,823][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.36it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.039 train/auc:  
                                                              0.746 train/f1:   
                                                              0.707             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.614 train/mre:  
                                                              0.042             
[2024-05-30 13:38:51,396][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/3>
[2024-05-30 13:38:51,396][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:38:51,399][HYDRA] 	#414 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:38:51,692][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:38:51,695][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:38:51,697][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:38:51,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:38:51,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:38:51,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:38:51,703][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:38:51,704][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:38:51,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:38:51,707][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:38:51,707][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:38:51,709][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:38:51,753][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.97it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.042 train/auc:  
                                                              0.868 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.904             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.060             
[2024-05-30 13:39:15,668][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/4>
[2024-05-30 13:39:15,668][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:39:15,671][HYDRA] 	#415 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:39:15,965][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:39:15,967][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:39:15,969][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:39:15,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:39:15,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:39:15,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:39:15,975][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:39:15,976][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:39:15,977][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:39:15,978][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:39:15,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:39:15,981][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:39:16,025][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.39it/s v_num: 0.000      
                                                              val/auc: 0.468    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.714 val/mre:    
                                                              0.032 train/auc:  
                                                              0.570 train/f1:   
                                                              0.620             
                                                              train/precision:  
                                                              0.556             
                                                              train/recall:     
                                                              0.702 train/mre:  
                                                              0.038             
[2024-05-30 13:39:39,669][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/5>
[2024-05-30 13:39:39,670][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:39:39,673][HYDRA] 	#416 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:39:39,967][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:39:39,969][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:39:39,971][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:39:39,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:39:39,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:39:39,976][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:39:39,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:39:39,977][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:39:39,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:39:39,984][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:39:39,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:39:39,986][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:39:40,034][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.06it/s v_num: 0.000      
                                                              val/auc: 0.635    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.714 val/mre:    
                                                              0.033 train/auc:  
                                                              0.781 train/f1:   
                                                              0.812             
                                                              train/precision:  
                                                              0.711             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.041             
[2024-05-30 13:40:04,114][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/6>
[2024-05-30 13:40:04,115][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:40:04,118][HYDRA] 	#417 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:40:04,412][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:40:04,415][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:40:04,417][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:40:04,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:40:04,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:40:04,422][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:40:04,422][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:40:04,423][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:40:04,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:40:04,426][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:40:04,426][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:40:04,428][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:40:04,476][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.67it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.036 train/auc:  
                                                              0.754 train/f1:   
                                                              0.791             
                                                              train/precision:  
                                                              0.688             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.060             
[2024-05-30 13:40:28,725][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/7>
[2024-05-30 13:40:28,728][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:40:28,742][HYDRA] 	#418 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:40:29,192][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:40:29,194][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:40:29,196][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:40:29,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:40:29,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:40:29,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:40:29,203][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:40:29,203][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:40:29,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:40:29,206][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:40:29,206][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:40:29,209][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:40:29,440][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.30it/s v_num: 0.000      
                                                              val/auc: 0.746    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.714 val/mre:    
                                                              0.031 train/auc:  
                                                              0.728 train/f1:   
                                                              0.752             
                                                              train/precision:  
                                                              0.691             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.041             
[2024-05-30 13:40:53,373][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/8>
[2024-05-30 13:40:53,374][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:40:53,377][HYDRA] 	#419 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:40:53,681][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:40:53,684][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:40:53,687][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:40:53,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:40:53,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:40:53,692][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:40:53,692][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:40:53,693][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:40:53,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:40:53,699][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:40:53,699][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:40:53,702][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:40:53,748][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.57it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.036 train/auc:  
                                                              0.868 train/f1:   
                                                              0.880             
                                                              train/precision:  
                                                              0.809             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.047             
[2024-05-30 13:41:17,158][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/9>
[2024-05-30 13:41:17,159][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:41:17,162][HYDRA] 	#420 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:41:17,469][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:41:17,471][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:41:17,474][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:41:17,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:41:17,478][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:41:17,478][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:41:17,479][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:41:17,480][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:41:17,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:41:17,483][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:41:17,483][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:41:17,485][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:41:17,527][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.54it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.922  
                                                              train/f1: 0.923   
                                                              train/precision:  
                                                              0.909             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              nan               
[2024-05-30 13:41:44,719][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/0>
[2024-05-30 13:41:44,720][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:41:44,723][HYDRA] 	#421 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:41:45,012][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:41:45,015][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:41:45,017][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:41:45,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:41:45,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:41:45,022][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:41:45,022][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:41:45,023][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:41:45,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:41:45,026][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:41:45,026][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:41:45,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:41:45,119][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.48it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.891  
                                                              train/f1: 0.894   
                                                              train/precision:  
                                                              0.868             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              nan               
[2024-05-30 13:42:11,571][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/1>
[2024-05-30 13:42:11,572][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:42:11,574][HYDRA] 	#422 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:42:11,864][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:42:11,867][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:42:11,869][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:42:11,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:42:11,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:42:11,874][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:42:11,875][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:42:11,875][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:42:11,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:42:11,880][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:42:11,880][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:42:11,883][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:42:11,927][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.81it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.930  
                                                              train/f1: 0.931   
                                                              train/precision:  
                                                              0.910             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              nan               
[2024-05-30 13:42:38,068][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/2>
[2024-05-30 13:42:38,069][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:42:38,072][HYDRA] 	#423 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:42:38,365][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:42:38,367][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:42:38,370][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:42:38,370][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:42:38,373][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:42:38,374][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:42:38,375][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:42:38,376][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:42:38,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:42:38,378][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:42:38,379][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:42:38,381][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:42:38,424][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.88it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.805  
                                                              train/f1: 0.823   
                                                              train/precision:  
                                                              0.753             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              nan               
[2024-05-30 13:43:04,530][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/3>
[2024-05-30 13:43:04,531][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:43:04,536][HYDRA] 	#424 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:43:04,867][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:43:04,870][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:43:04,873][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:43:04,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:43:04,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:43:04,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:43:04,879][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:43:04,880][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:43:04,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:43:04,883][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:43:04,883][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:43:04,886][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:43:04,934][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.56it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.891  
                                                              train/f1: 0.887   
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              nan               
[2024-05-30 13:43:31,203][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/4>
[2024-05-30 13:43:31,204][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:43:31,207][HYDRA] 	#425 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:43:31,504][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:43:31,506][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:43:31,508][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:43:31,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:43:31,512][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:43:31,513][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:43:31,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:43:31,515][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:43:31,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:43:31,520][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:43:31,520][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:43:31,523][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:43:31,566][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.79it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.773  
                                                              train/f1: 0.748   
                                                              train/precision:  
                                                              0.843             
                                                              train/recall:     
                                                              0.672 train/mre:  
                                                              nan               
[2024-05-30 13:43:57,607][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/5>
[2024-05-30 13:43:57,608][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:43:57,612][HYDRA] 	#426 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:43:57,900][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:43:57,902][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:43:57,905][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:43:57,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:43:57,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:43:57,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:43:57,910][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:43:57,911][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:43:57,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:43:57,914][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:43:57,914][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:43:57,916][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:43:58,009][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.44it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.930  
                                                              train/f1: 0.928   
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              nan               
[2024-05-30 13:44:23,950][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/6>
[2024-05-30 13:44:23,951][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:44:23,954][HYDRA] 	#427 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:44:24,241][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:44:24,244][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:44:24,246][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:44:24,246][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:44:24,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:44:24,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:44:24,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:44:24,252][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:44:24,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:44:24,255][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:44:24,255][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:44:24,257][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:44:24,299][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.39it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.867  
                                                              train/f1: 0.876   
                                                              train/precision:  
                                                              0.822             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              nan               
[2024-05-30 13:44:50,428][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/7>
[2024-05-30 13:44:50,429][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:44:50,433][HYDRA] 	#428 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:44:50,725][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:44:50,728][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:44:50,730][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:44:50,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:44:50,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:44:50,735][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:44:50,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:44:50,736][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:44:50,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:44:50,741][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:44:50,742][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:44:50,744][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:44:50,789][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.16it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.828  
                                                              train/f1: 0.807   
                                                              train/precision:  
                                                              0.920             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              nan               
[2024-05-30 13:45:16,992][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/8>
[2024-05-30 13:45:16,993][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:45:16,995][HYDRA] 	#429 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:45:17,285][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:45:17,288][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:45:17,290][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:45:17,290][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:45:17,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:45:17,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:45:17,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:45:17,296][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:45:17,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:45:17,300][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:45:17,300][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:45:17,945][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:45:18,029][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.19it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.812  
                                                              train/f1: 0.821   
                                                              train/precision:  
                                                              0.786             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              nan               
[2024-05-30 13:45:44,281][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/9>
[2024-05-30 13:45:44,281][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:45:44,285][HYDRA] 	#430 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:45:44,584][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:45:44,587][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:45:44,589][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:45:44,590][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:45:44,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:45:44,594][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:45:44,595][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:45:44,596][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:45:44,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:45:44,598][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:45:44,599][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:45:44,601][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:45:44,644][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.057 train/auc:  
                                                              0.891 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.903             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.062             
[2024-05-30 13:46:10,888][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/0>
[2024-05-30 13:46:10,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:46:10,891][HYDRA] 	#431 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:46:11,175][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:46:11,178][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:46:11,180][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:46:11,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:46:11,184][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:46:11,185][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:46:11,185][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:46:11,186][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:46:11,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:46:11,189][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:46:11,189][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:46:11,191][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:46:11,232][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 33.05it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.065 train/auc:  
                                                              0.875 train/f1:   
                                                              0.869             
                                                              train/precision:  
                                                              0.914             
                                                              train/recall:     
                                                              0.828 train/mre:  
                                                              0.064             
[2024-05-30 13:46:37,426][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/1>
[2024-05-30 13:46:37,427][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:46:37,430][HYDRA] 	#432 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:46:37,723][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:46:37,726][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:46:37,728][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:46:37,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:46:37,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:46:37,732][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:46:37,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:46:37,734][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:46:37,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:46:37,737][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:46:37,738][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:46:37,740][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:46:37,834][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.39it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.064 train/auc:  
                                                              0.773 train/f1:   
                                                              0.782             
                                                              train/precision:  
                                                              0.754             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              0.070             
[2024-05-30 13:47:03,977][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/2>
[2024-05-30 13:47:03,977][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:47:03,980][HYDRA] 	#433 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:47:04,266][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:47:04,268][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:47:04,271][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:47:04,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:47:04,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:47:04,276][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:47:04,276][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:47:04,277][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:47:04,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:47:04,280][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:47:04,280][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:47:04,283][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:47:04,325][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.98it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.055 train/auc:  
                                                              0.789 train/f1:   
                                                              0.765             
                                                              train/precision:  
                                                              0.863             
                                                              train/recall:     
                                                              0.688 train/mre:  
                                                              0.062             
[2024-05-30 13:47:30,364][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/3>
[2024-05-30 13:47:30,365][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:47:30,368][HYDRA] 	#434 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:47:30,658][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:47:30,661][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:47:30,663][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:47:30,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:47:30,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:47:30,667][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:47:30,668][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:47:30,669][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:47:30,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:47:30,672][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:47:30,672][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:47:30,674][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:47:30,719][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.83it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.805 train/f1:   
                                                              0.832             
                                                              train/precision:  
                                                              0.729             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.070             
[2024-05-30 13:47:56,793][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/4>
[2024-05-30 13:47:56,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:47:56,797][HYDRA] 	#435 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:47:57,089][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:47:57,091][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:47:57,094][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:47:57,094][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:47:57,098][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:47:57,099][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:47:57,099][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:47:57,100][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:47:57,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:47:57,103][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:47:57,103][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:47:57,105][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:47:57,149][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.03it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.068 train/auc:  
                                                              0.805 train/f1:   
                                                              0.790             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.734 train/mre:  
                                                              0.065             
[2024-05-30 13:48:23,131][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/5>
[2024-05-30 13:48:23,132][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:48:23,134][HYDRA] 	#436 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:48:23,431][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:48:23,434][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:48:23,436][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:48:23,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:48:23,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:48:23,441][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:48:23,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:48:23,443][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:48:23,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:48:23,447][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:48:23,448][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:48:23,450][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:48:23,494][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.48it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.061 train/auc:  
                                                              0.789 train/f1:   
                                                              0.777             
                                                              train/precision:  
                                                              0.825             
                                                              train/recall:     
                                                              0.734 train/mre:  
                                                              0.062             
[2024-05-30 13:48:49,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/6>
[2024-05-30 13:48:49,888][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:48:49,892][HYDRA] 	#437 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:48:50,179][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:48:50,182][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:48:50,184][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:48:50,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:48:50,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:48:50,189][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:48:50,189][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:48:50,190][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:48:50,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:48:50,193][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:48:50,193][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:48:50,195][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:48:50,238][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.95it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.053 train/auc:  
                                                              0.758 train/f1:   
                                                              0.744             
                                                              train/precision:  
                                                              0.789             
                                                              train/recall:     
                                                              0.703 train/mre:  
                                                              0.054             
[2024-05-30 13:49:17,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/7>
[2024-05-30 13:49:17,237][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:49:17,240][HYDRA] 	#438 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:49:17,542][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:49:17,545][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:49:17,547][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:49:17,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:49:17,551][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:49:17,552][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:49:17,553][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:49:17,553][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:49:17,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:49:17,556][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:49:17,557][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:49:17,559][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:49:17,643][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.45it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.056 train/auc:  
                                                              0.719 train/f1:   
                                                              0.705             
                                                              train/precision:  
                                                              0.741             
                                                              train/recall:     
                                                              0.672 train/mre:  
                                                              0.057             
[2024-05-30 13:49:43,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/8>
[2024-05-30 13:49:43,760][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:49:43,762][HYDRA] 	#439 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:49:44,049][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:49:44,052][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:49:44,054][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:49:44,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:49:44,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:49:44,059][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:49:44,059][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:49:44,060][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:49:44,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:49:44,063][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:49:44,063][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:49:44,065][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:49:44,107][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.36it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.061 train/auc:  
                                                              0.875 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.929             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              0.062             
[2024-05-30 13:50:10,281][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/9>
[2024-05-30 13:50:10,282][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:50:10,285][HYDRA] 	#440 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:50:10,577][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:50:10,580][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:50:10,582][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:50:10,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:50:10,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:50:10,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:50:10,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:50:10,588][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:50:10,590][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:50:10,591][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:50:10,591][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:50:10,594][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:50:10,637][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.13it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.056 train/auc:  
                                                              0.883 train/f1:   
                                                              0.884             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.058             
[2024-05-30 13:50:37,303][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/0>
[2024-05-30 13:50:37,303][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:50:37,306][HYDRA] 	#441 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:50:37,603][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:50:37,606][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:50:37,608][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:50:37,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:50:37,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:50:37,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:50:37,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:50:37,614][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:50:37,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:50:37,619][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:50:37,619][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:50:37,622][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:50:37,707][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 32.10it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              0.961 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.062             
[2024-05-30 13:51:03,874][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/1>
[2024-05-30 13:51:03,875][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:51:03,878][HYDRA] 	#442 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:51:04,174][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:51:04,177][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:51:04,180][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:51:04,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:51:04,184][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:51:04,185][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:51:04,186][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:51:04,186][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:51:04,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:51:04,189][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:51:04,189][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:51:04,192][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:51:04,234][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.42it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.057 train/auc:  
                                                              0.906 train/f1:   
                                                              0.910             
                                                              train/precision:  
                                                              0.871             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.063             
[2024-05-30 13:51:30,273][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/2>
[2024-05-30 13:51:30,274][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:51:30,276][HYDRA] 	#443 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:51:30,568][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:51:30,571][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:51:30,573][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:51:30,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:51:30,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:51:30,578][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:51:30,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:51:30,579][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:51:30,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:51:30,582][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:51:30,582][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:51:30,585][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:51:30,626][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.85it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.057 train/auc:  
                                                              0.797 train/f1:   
                                                              0.831             
                                                              train/precision:  
                                                              0.711             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-05-30 13:51:57,392][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/3>
[2024-05-30 13:51:57,392][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:51:57,395][HYDRA] 	#444 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:51:57,686][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:51:57,688][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:51:57,691][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:51:57,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:51:57,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:51:57,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:51:57,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:51:57,697][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:51:57,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:51:57,700][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:51:57,700][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:51:57,702][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:51:57,747][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.74it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.500 val/mre:    
                                                              0.057 train/auc:  
                                                              0.812 train/f1:   
                                                              0.831             
                                                              train/precision:  
                                                              0.756             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.061             
[2024-05-30 13:52:24,321][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/4>
[2024-05-30 13:52:24,322][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:52:24,324][HYDRA] 	#445 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:52:24,611][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:52:24,613][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:52:24,616][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:52:24,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:52:24,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:52:24,620][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:52:24,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:52:24,622][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:52:24,623][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:52:24,624][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:52:24,625][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:52:24,627][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:52:24,669][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.89it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.063 train/auc:  
                                                              0.891 train/f1:   
                                                              0.896             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.064             
[2024-05-30 13:52:50,995][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/5>
[2024-05-30 13:52:50,996][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:52:50,999][HYDRA] 	#446 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:52:51,294][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:52:51,296][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:52:51,298][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:52:51,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:52:51,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:52:51,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:52:51,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:52:51,304][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:52:51,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:52:51,314][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:52:51,314][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:52:51,317][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:52:51,415][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.054 train/auc:  
                                                              0.805 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.775             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.061             
[2024-05-30 13:53:17,512][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/6>
[2024-05-30 13:53:17,512][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:53:17,515][HYDRA] 	#447 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:53:17,804][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:53:17,806][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:53:17,809][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:53:17,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:53:17,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:53:17,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:53:17,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:53:17,815][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:53:17,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:53:17,818][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:53:17,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:53:17,821][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:53:17,863][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.65it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.048 train/auc:  
                                                              0.695 train/f1:   
                                                              0.758             
                                                              train/precision:  
                                                              0.629             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.053             
[2024-05-30 13:53:44,500][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/7>
[2024-05-30 13:53:44,501][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:53:44,504][HYDRA] 	#448 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:53:44,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:53:44,806][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:53:44,808][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:53:44,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:53:44,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:53:44,813][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:53:44,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:53:44,814][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:53:44,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:53:44,817][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:53:44,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:53:44,820][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:53:44,866][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.64it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.060 train/auc:  
                                                              0.727 train/f1:   
                                                              0.706             
                                                              train/precision:  
                                                              0.764             
                                                              train/recall:     
                                                              0.656 train/mre:  
                                                              0.055             
[2024-05-30 13:54:11,561][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/8>
[2024-05-30 13:54:11,562][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:54:11,565][HYDRA] 	#449 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:54:11,866][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:54:11,869][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:54:11,871][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:54:11,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:54:11,875][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:54:11,876][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:54:11,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:54:11,878][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:54:11,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:54:11,882][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:54:11,883][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:54:11,885][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:54:11,984][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.98it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.053 train/auc:  
                                                              0.914 train/f1:   
                                                              0.911             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.061             
[2024-05-30 13:54:39,119][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/9>
[2024-05-30 13:54:39,120][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:54:39,123][HYDRA] 	#450 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:54:39,414][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:54:39,416][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:54:39,419][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:54:39,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:54:39,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:54:39,423][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:54:39,424][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:54:39,425][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:54:39,427][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:54:39,428][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:54:39,428][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:54:39,430][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:54:39,472][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.73it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.053 train/auc:  
                                                              0.938 train/f1:   
                                                              0.941             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.058             
[2024-05-30 13:55:06,221][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/0>
[2024-05-30 13:55:06,221][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:55:06,224][HYDRA] 	#451 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:55:07,200][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:55:07,202][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:55:07,205][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:55:07,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:55:07,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:55:07,210][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:55:07,210][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:55:07,211][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:55:07,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:55:07,214][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:55:07,214][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:55:07,216][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:55:07,260][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.44it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.063 train/auc:  
                                                              0.914 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.063             
[2024-05-30 13:55:33,500][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/1>
[2024-05-30 13:55:33,501][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:55:33,504][HYDRA] 	#452 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:55:33,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:55:33,806][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:55:33,809][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:55:33,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:55:33,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:55:33,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:55:33,815][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:55:33,816][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:55:33,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:55:33,835][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:55:33,835][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:55:33,839][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:55:33,946][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.89it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.056 train/auc:  
                                                              0.844 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.062             
[2024-05-30 13:56:00,287][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/2>
[2024-05-30 13:56:00,287][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:56:00,290][HYDRA] 	#453 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:56:00,580][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:56:00,583][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:56:00,585][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:56:00,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:56:00,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:56:00,590][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:56:00,591][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:56:00,591][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:56:00,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:56:00,594][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:56:00,594][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:56:00,597][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:56:00,641][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.46it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              0.938 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.924             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.065             
[2024-05-30 13:56:27,013][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/3>
[2024-05-30 13:56:27,014][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:56:27,017][HYDRA] 	#454 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:56:27,311][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:56:27,313][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:56:27,315][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:56:27,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:56:27,319][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:56:27,320][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:56:27,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:56:27,321][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:56:27,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:56:27,324][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:56:27,324][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:56:27,326][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:56:27,369][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.67it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.057 train/auc:  
                                                              0.812 train/f1:   
                                                              0.838             
                                                              train/precision:  
                                                              0.738             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.067             
[2024-05-30 13:56:53,894][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/4>
[2024-05-30 13:56:53,895][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:56:53,898][HYDRA] 	#455 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:56:54,198][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:56:54,200][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:56:54,202][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:56:54,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:56:54,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:56:54,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:56:54,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:56:54,209][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:56:54,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:56:54,214][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:56:54,214][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:56:54,217][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:56:54,317][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.77it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.060 train/auc:  
                                                              0.914 train/f1:   
                                                              0.916             
                                                              train/precision:  
                                                              0.896             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.066             
[2024-05-30 13:57:20,681][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/5>
[2024-05-30 13:57:20,682][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:57:20,685][HYDRA] 	#456 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:57:20,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:57:20,982][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:57:20,984][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:57:20,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:57:20,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:57:20,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:57:20,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:57:20,990][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:57:20,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:57:20,994][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:57:20,994][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:57:20,997][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:57:21,041][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.35it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.062 train/auc:  
                                                              0.797 train/f1:   
                                                              0.790             
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.766 train/mre:  
                                                              0.058             
[2024-05-30 13:57:47,590][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/6>
[2024-05-30 13:57:47,591][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:57:47,594][HYDRA] 	#457 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:57:47,880][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:57:47,883][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:57:47,885][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:57:47,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:57:47,889][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:57:47,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:57:47,891][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:57:47,892][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:57:47,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:57:47,894][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:57:47,895][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:57:47,897][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:57:47,940][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.90it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.055 train/auc:  
                                                              0.844 train/f1:   
                                                              0.851             
                                                              train/precision:  
                                                              0.814             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.057             
[2024-05-30 13:58:14,165][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/7>
[2024-05-30 13:58:14,166][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:58:14,168][HYDRA] 	#458 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:58:14,466][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:58:14,469][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:58:14,471][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:58:14,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:58:14,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:58:14,476][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:58:14,476][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:58:14,477][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:58:14,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:58:14,483][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:58:14,483][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:58:14,485][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:58:14,569][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.49it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.057 train/auc:  
                                                              0.758 train/f1:   
                                                              0.752             
                                                              train/precision:  
                                                              0.770             
                                                              train/recall:     
                                                              0.734 train/mre:  
                                                              0.051             
[2024-05-30 13:58:40,820][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/8>
[2024-05-30 13:58:40,820][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:58:40,823][HYDRA] 	#459 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:58:41,116][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:58:41,119][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:58:41,121][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:58:41,122][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:58:41,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:58:41,126][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:58:41,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:58:41,128][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:58:41,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:58:41,131][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:58:41,131][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:58:41,134][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:58:41,179][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.32it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.057 train/auc:  
                                                              0.938 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.063             
[2024-05-30 13:59:07,582][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/9>
[2024-05-30 13:59:07,582][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:59:07,586][HYDRA] 	#460 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:59:07,885][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:59:07,887][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:59:07,890][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:59:07,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:59:07,894][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:59:07,895][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:59:07,896][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:59:07,896][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:59:07,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:59:07,899][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:59:07,900][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:59:07,902][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:59:07,946][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.96it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.057 train/auc:  
                                                              0.953 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.926             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.059             
[2024-05-30 13:59:34,431][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/0>
[2024-05-30 13:59:34,432][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:59:34,435][HYDRA] 	#461 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:59:34,736][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:59:34,738][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 13:59:34,741][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:59:34,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:59:34,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:59:34,745][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:59:34,746][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:59:34,747][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:59:34,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:59:34,750][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:59:34,750][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:59:34,753][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:59:34,795][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.32it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.060 train/auc:  
                                                              0.938 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.062             
[2024-05-30 14:00:01,502][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/1>
[2024-05-30 14:00:01,502][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:00:01,505][HYDRA] 	#462 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:00:01,798][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:00:01,801][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:00:01,803][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:00:01,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:00:01,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:00:01,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:00:01,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:00:01,809][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:00:01,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:00:01,812][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:00:01,812][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:00:01,814][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:00:01,857][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.41it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.063 train/auc:  
                                                              0.875 train/f1:   
                                                              0.884             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.062             
[2024-05-30 14:00:28,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/2>
[2024-05-30 14:00:28,371][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:00:28,374][HYDRA] 	#463 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:00:28,711][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:00:28,714][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:00:28,716][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:00:28,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:00:28,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:00:28,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:00:28,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:00:28,723][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:00:28,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:00:28,729][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:00:28,729][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:00:28,731][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:00:28,818][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.58it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.055 train/auc:  
                                                              0.812 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.061             
[2024-05-30 14:00:54,852][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/3>
[2024-05-30 14:00:54,853][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:00:54,856][HYDRA] 	#464 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:00:55,145][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:00:55,147][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:00:55,150][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:00:55,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:00:55,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:00:55,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:00:55,155][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:00:55,156][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:00:55,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:00:55,159][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:00:55,160][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:00:55,162][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:00:55,206][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.03it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.067 train/auc:  
                                                              0.844 train/f1:   
                                                              0.865             
                                                              train/precision:  
                                                              0.762             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.071             
[2024-05-30 14:01:21,364][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/4>
[2024-05-30 14:01:21,365][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:01:21,370][HYDRA] 	#465 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:01:21,658][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:01:21,661][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:01:21,663][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:01:21,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:01:21,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:01:21,668][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:01:21,668][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:01:21,669][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:01:21,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:01:21,672][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:01:21,672][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:01:21,675][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:01:21,719][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.53it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.061 train/auc:  
                                                              0.898 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.063             
[2024-05-30 14:01:48,125][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/5>
[2024-05-30 14:01:48,126][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:01:48,128][HYDRA] 	#466 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:01:48,423][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:01:48,426][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:01:48,429][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:01:48,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:01:48,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:01:48,434][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:01:48,435][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:01:48,435][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:01:48,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:01:48,438][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:01:48,438][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:01:48,441][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:01:48,483][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.65it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.055 train/auc:  
                                                              0.805 train/f1:   
                                                              0.837             
                                                              train/precision:  
                                                              0.719             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-05-30 14:02:14,979][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/6>
[2024-05-30 14:02:14,980][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:02:14,983][HYDRA] 	#467 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:02:15,266][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:02:15,268][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:02:15,270][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:02:15,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:02:15,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:02:15,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:02:15,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:02:15,276][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:02:15,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:02:15,279][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:02:15,279][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:02:15,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:02:15,323][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.62it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.050 train/auc:  
                                                              0.805 train/f1:   
                                                              0.830             
                                                              train/precision:  
                                                              0.735             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.056             
[2024-05-30 14:02:41,640][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/7>
[2024-05-30 14:02:41,641][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:02:41,644][HYDRA] 	#468 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:02:41,935][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:02:41,937][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:02:41,940][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:02:41,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:02:41,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:02:41,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:02:41,945][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:02:41,946][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:02:41,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:02:41,949][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:02:41,949][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:02:41,952][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:02:41,995][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.86it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.055 train/auc:  
                                                              0.672 train/f1:   
                                                              0.588             
                                                              train/precision:  
                                                              0.789             
                                                              train/recall:     
                                                              0.469 train/mre:  
                                                              0.051             
[2024-05-30 14:03:08,026][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/8>
[2024-05-30 14:03:08,026][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:03:08,029][HYDRA] 	#469 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:03:08,338][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:03:08,340][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:03:08,343][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:03:08,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:03:08,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:03:08,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:03:08,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:03:08,349][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:03:08,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:03:08,354][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:03:08,354][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:03:08,356][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:03:08,443][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.51it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.055 train/auc:  
                                                              0.945 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              0.925             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.061             
[2024-05-30 14:03:34,464][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/9>
[2024-05-30 14:03:34,464][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:03:34,467][HYDRA] 	#470 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:03:34,752][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:03:34,755][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:03:34,757][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:03:34,758][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:03:34,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:03:34,762][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:03:34,763][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:03:34,763][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:03:34,765][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:03:34,766][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:03:34,766][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:03:34,769][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:03:34,810][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.75it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.061 train/auc:  
                                                              0.953 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.914             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.063             
[2024-05-30 14:04:01,301][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/0>
[2024-05-30 14:04:01,302][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:04:01,305][HYDRA] 	#471 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:04:01,595][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:04:01,597][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:04:01,600][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:04:01,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:04:01,603][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:04:01,604][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:04:01,605][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:04:01,606][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:04:01,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:04:01,609][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:04:01,609][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:04:01,611][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:04:01,655][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.75it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.077 train/auc:  
                                                              0.906 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.828 train/mre:  
                                                              0.068             
[2024-05-30 14:04:28,184][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/1>
[2024-05-30 14:04:28,185][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:04:28,187][HYDRA] 	#472 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:04:28,515][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:04:28,518][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:04:28,521][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:04:28,522][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:04:28,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:04:28,526][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:04:28,527][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:04:28,528][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:04:28,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:04:28,534][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:04:28,534][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:04:28,537][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:04:28,649][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.50it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.055 train/auc:  
                                                              0.945 train/f1:   
                                                              0.945             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.061             
[2024-05-30 14:04:55,159][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/2>
[2024-05-30 14:04:55,159][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:04:55,162][HYDRA] 	#473 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:04:55,450][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:04:55,453][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:04:55,455][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:04:55,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:04:55,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:04:55,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:04:55,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:04:55,461][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:04:55,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:04:55,464][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:04:55,464][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:04:55,466][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:04:55,510][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.67it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.055 train/auc:  
                                                              0.938 train/f1:   
                                                              0.937             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.061             
[2024-05-30 14:05:22,565][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/3>
[2024-05-30 14:05:22,566][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:05:22,568][HYDRA] 	#474 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:05:22,855][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:05:22,857][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:05:22,859][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:05:22,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:05:22,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:05:22,864][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:05:22,865][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:05:22,865][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:05:22,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:05:22,868][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:05:22,868][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:05:22,871][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:05:22,916][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.99it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.065 train/auc:  
                                                              0.961 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.954             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.066             
[2024-05-30 14:05:49,214][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/4>
[2024-05-30 14:05:49,215][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:05:49,218][HYDRA] 	#475 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:05:49,522][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:05:49,524][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:05:49,526][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:05:49,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:05:49,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:05:49,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:05:49,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:05:49,532][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:05:49,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:05:49,538][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:05:49,538][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:05:49,540][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:05:49,626][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.20it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              0.945 train/f1:   
                                                              0.946             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.065             
[2024-05-30 14:06:16,049][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/5>
[2024-05-30 14:06:16,049][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:06:16,052][HYDRA] 	#476 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:06:16,338][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:06:16,340][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:06:16,342][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:06:16,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:06:16,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:06:16,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:06:16,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:06:16,349][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:06:16,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:06:16,351][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:06:16,352][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:06:16,354][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:06:16,396][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.18it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.052 train/auc:  
                                                              0.859 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.795             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.058             
[2024-05-30 14:06:42,816][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/6>
[2024-05-30 14:06:42,816][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:06:42,820][HYDRA] 	#477 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:06:43,281][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:06:43,283][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:06:43,285][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:06:43,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:06:43,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:06:43,291][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:06:43,292][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:06:43,293][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:06:43,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:06:43,296][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:06:43,296][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:06:43,298][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:06:43,418][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.18it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.053 train/auc:  
                                                              0.773 train/f1:   
                                                              0.791             
                                                              train/precision:  
                                                              0.733             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.057             
[2024-05-30 14:07:09,869][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/7>
[2024-05-30 14:07:09,870][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:07:09,873][HYDRA] 	#478 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:07:10,160][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:07:10,162][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:07:10,165][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:07:10,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:07:10,169][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:07:10,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:07:10,170][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:07:10,171][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:07:10,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:07:10,174][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:07:10,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:07:10,177][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:07:10,222][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.082 train/auc:  
                                                              0.828 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.781 train/mre:  
                                                              0.062             
[2024-05-30 14:07:36,697][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/8>
[2024-05-30 14:07:36,697][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:07:36,700][HYDRA] 	#479 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:07:36,992][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:07:36,994][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:07:36,996][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:07:36,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:07:37,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:07:37,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:07:37,002][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:07:37,003][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:07:37,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:07:37,006][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:07:37,006][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:07:37,008][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:07:37,050][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.33it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.065 train/auc:  
                                                              0.914 train/f1:   
                                                              0.908             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.844 train/mre:  
                                                              0.066             
[2024-05-30 14:08:03,338][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/9>
[2024-05-30 14:08:03,339][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:08:03,342][HYDRA] 	#480 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:08:03,648][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:08:03,651][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:08:03,653][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:08:03,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:08:03,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:08:03,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:08:03,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:08:03,661][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:08:03,662][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:08:03,666][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:08:03,666][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:08:03,669][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:08:03,773][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.02it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.054 train/auc:  
                                                              0.883 train/f1:   
                                                              0.895             
                                                              train/precision:  
                                                              0.810             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.059             
[2024-05-30 14:08:30,028][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/0>
[2024-05-30 14:08:30,029][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:08:30,032][HYDRA] 	#481 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:08:30,322][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:08:30,325][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:08:30,327][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:08:30,327][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:08:30,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:08:30,332][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:08:30,333][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:08:30,333][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:08:30,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:08:30,336][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:08:30,336][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:08:30,339][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:08:30,381][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.85it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.060 train/auc:  
                                                              0.977 train/f1:   
                                                              0.977             
                                                              train/precision:  
                                                              0.969             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.064             
[2024-05-30 14:08:56,831][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/1>
[2024-05-30 14:08:56,832][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:08:56,835][HYDRA] 	#482 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:08:57,135][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:08:57,138][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:08:57,140][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:08:57,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:08:57,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:08:57,145][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:08:57,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:08:57,147][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:08:57,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:08:57,149][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:08:57,150][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:08:57,152][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:08:57,194][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.17it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.063 train/auc:  
                                                              0.938 train/f1:   
                                                              0.941             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.059             
[2024-05-30 14:09:24,048][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/2>
[2024-05-30 14:09:24,048][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:09:24,051][HYDRA] 	#483 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:09:24,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:09:24,376][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:09:24,378][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:09:24,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:09:24,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:09:24,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:09:24,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:09:24,385][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:09:24,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:09:24,391][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:09:24,392][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:09:24,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:09:24,503][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.79it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.938 train/f1:   
                                                              0.934             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.062             
[2024-05-30 14:09:53,099][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/3>
[2024-05-30 14:09:53,100][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:09:53,103][HYDRA] 	#484 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:09:53,516][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:09:53,518][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:09:53,521][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:09:53,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:09:53,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:09:53,526][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:09:53,526][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:09:53,527][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:09:53,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:09:53,530][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:09:53,530][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:09:53,533][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:09:53,676][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.26it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.058 train/auc:  
                                                              0.883 train/f1:   
                                                              0.892             
                                                              train/precision:  
                                                              0.827             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.065             
[2024-05-30 14:10:23,644][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/4>
[2024-05-30 14:10:23,645][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:10:23,648][HYDRA] 	#485 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:10:23,942][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:10:23,945][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:10:23,947][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:10:23,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:10:23,951][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:10:23,952][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:10:23,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:10:23,954][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:10:23,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:10:23,956][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:10:23,957][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:10:23,959][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:10:24,002][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.84it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.060 train/auc:  
                                                              0.922 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.065             
[2024-05-30 14:10:50,234][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/5>
[2024-05-30 14:10:50,235][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:10:50,237][HYDRA] 	#486 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:10:50,535][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:10:50,538][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:10:50,540][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:10:50,541][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:10:50,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:10:50,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:10:50,546][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:10:50,546][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:10:50,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:10:50,551][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:10:50,552][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:10:50,554][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:10:50,645][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.38it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.061 train/auc:  
                                                              0.898 train/f1:   
                                                              0.896             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.062             
[2024-05-30 14:11:17,030][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/6>
[2024-05-30 14:11:17,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:11:17,034][HYDRA] 	#487 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:11:17,321][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:11:17,323][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:11:17,325][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:11:17,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:11:17,330][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:11:17,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:11:17,331][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:11:17,332][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:11:17,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:11:17,335][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:11:17,335][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:11:17,337][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:11:17,379][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.81it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.054 train/auc:  
                                                              0.828 train/f1:   
                                                              0.849             
                                                              train/precision:  
                                                              0.756             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.058             
[2024-05-30 14:11:44,134][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/7>
[2024-05-30 14:11:44,134][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:11:44,137][HYDRA] 	#488 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:11:44,423][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:11:44,425][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:11:44,428][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:11:44,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:11:44,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:11:44,433][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:11:44,434][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:11:44,434][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:11:44,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:11:44,437][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:11:44,437][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:11:44,440][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:11:44,481][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.22it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.064 train/auc:  
                                                              0.812 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.066             
[2024-05-30 14:12:10,956][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/8>
[2024-05-30 14:12:10,957][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:12:10,960][HYDRA] 	#489 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=transformer callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:12:11,249][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:12:11,251][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-05-30 14:12:11,253][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:12:11,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:12:11,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:12:11,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:12:11,259][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:12:11,260][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:12:11,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:12:11,262][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:12:11,263][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:12:11,265][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:12:11,308][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.86it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.762     
                                                              val/precision:    
                                                              0.727 val/recall: 
                                                              0.800 val/mre:    
                                                              0.054 train/auc:  
                                                              0.953 train/f1:   
                                                              0.953             
                                                              train/precision:  
                                                              0.953             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.060             
[2024-05-30 14:12:37,411][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/9>
[2024-05-30 14:12:37,411][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
