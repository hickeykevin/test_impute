BRITS on last questions, ricardo style evaluation
[2024-06-20 17:54:12,104][HYDRA] Launching 560 jobs locally
[2024-06-20 17:54:12,104][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 17:54:12,289][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:54:13,363][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
2024-06-20 17:54:14 [ERROR]: ❌ No module named 'torch_geometric'
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
2024-06-20 17:54:14 [ERROR]: ❌ name 'MessagePassing' is not defined
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
[2024-06-20 17:54:14,525][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:54:14,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:54:14,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:54:14,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:54:14,533][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:54:14,534][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:54:14,534][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:54:14,534][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:54:14,536][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:54:14,926][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.45it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.884  
                                                              train/f1: 0.883   
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.018             
[2024-06-20 17:55:01,613][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/0>
[2024-06-20 17:55:01,614][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:55:01,617][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 17:55:01,789][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:55:01,791][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 17:55:01,792][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:55:01,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:55:01,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:55:01,794][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:55:01,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:55:01,796][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:55:01,796][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:55:01,796][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:55:01,797][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:55:01,825][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.46it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 17:55:47,812][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/1>
[2024-06-20 17:55:47,813][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:55:47,814][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 17:55:48,005][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:55:48,006][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 17:55:48,007][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:55:48,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:55:48,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:55:48,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:55:48,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:55:48,011][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:55:48,011][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:55:48,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:55:48,012][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:55:48,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.47it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 17:56:34,061][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/2>
[2024-06-20 17:56:34,061][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:56:34,064][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 17:56:34,233][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:56:34,234][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 17:56:34,235][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:56:34,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:56:34,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:56:34,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:56:34,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:56:34,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:56:34,239][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:56:34,240][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:56:34,241][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:56:34,270][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.278    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 17:57:20,940][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/3>
[2024-06-20 17:57:20,941][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:57:20,943][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 17:57:21,110][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:57:21,111][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 17:57:21,112][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:57:21,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:57:21,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:57:21,115][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:57:21,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:57:21,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:57:21,117][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:57:21,117][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:57:21,118][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:57:21,146][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.11it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.021             
[2024-06-20 17:58:09,503][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/4>
[2024-06-20 17:58:09,506][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:58:09,524][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 17:58:09,732][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:58:09,734][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 17:58:09,734][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:58:09,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:58:09,737][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:58:09,737][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:58:09,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:58:09,738][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:58:09,739][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:58:09,739][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:58:09,740][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:58:09,782][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.378    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 17:58:59,551][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/5>
[2024-06-20 17:58:59,552][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:58:59,555][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 17:58:59,735][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:58:59,737][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 17:58:59,738][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:58:59,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:58:59,740][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:58:59,741][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:58:59,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:58:59,742][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:58:59,743][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:58:59,743][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:58:59,744][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:58:59,775][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.278    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.021             
[2024-06-20 17:59:49,385][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/6>
[2024-06-20 17:59:49,386][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:59:49,390][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 17:59:49,571][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:59:49,573][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 17:59:49,574][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:59:49,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:59:49,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:59:49,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:59:49,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:59:49,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:59:49,579][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:59:49,579][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:59:49,580][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:59:49,664][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.019             
[2024-06-20 18:00:38,776][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/7>
[2024-06-20 18:00:38,777][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:00:38,779][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:00:38,958][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:00:38,959][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:00:38,960][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:00:38,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:00:38,963][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:00:38,964][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:00:38,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:00:38,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:00:38,965][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:00:38,965][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:00:38,967][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:00:38,999][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.22it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 18:01:28,977][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/8>
[2024-06-20 18:01:28,978][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:01:28,983][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:01:29,157][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:01:29,159][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:01:29,160][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:01:29,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:01:29,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:01:29,164][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:01:29,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:01:29,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:01:29,166][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:01:29,166][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:01:29,167][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:01:29,201][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.272    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.024             
[2024-06-20 18:02:18,478][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.0/9>
[2024-06-20 18:02:18,479][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:02:18,482][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:02:18,664][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:02:18,666][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:02:18,667][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:02:18,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:02:18,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:02:18,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:02:18,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:02:18,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:02:18,672][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:02:18,672][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:02:18,674][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:02:18,707][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.022             
[2024-06-20 18:03:08,653][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/0>
[2024-06-20 18:03:08,653][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:03:08,656][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:03:08,863][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:03:08,865][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:03:08,866][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:03:08,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:03:08,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:03:08,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:03:08,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:03:08,870][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:03:08,871][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:03:08,871][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:03:08,872][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:03:08,906][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.74it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 18:03:58,768][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/1>
[2024-06-20 18:03:58,769][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:03:58,771][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:03:58,976][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:03:58,977][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:03:58,978][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:03:58,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:03:58,981][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:03:58,981][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:03:58,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:03:58,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:03:58,983][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:03:58,983][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:03:58,984][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:03:59,035][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.86it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 18:04:48,012][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/2>
[2024-06-20 18:04:48,013][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:04:48,016][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:04:48,196][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:04:48,198][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:04:48,199][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:04:48,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:04:48,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:04:48,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:04:48,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:04:48,203][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:04:48,203][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:04:48,203][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:04:48,205][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:04:48,234][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 18:05:37,764][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/3>
[2024-06-20 18:05:37,765][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:05:37,767][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:05:37,939][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:05:37,940][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:05:37,941][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:05:37,942][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:05:37,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:05:37,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:05:37,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:05:37,945][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:05:37,946][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:05:37,946][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:05:37,947][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:05:37,984][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.070 train/auc:  
                                                              0.973 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.024             
[2024-06-20 18:06:27,980][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/4>
[2024-06-20 18:06:27,982][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:06:27,985][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:06:28,242][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:06:28,244][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:06:28,245][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:06:28,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:06:28,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:06:28,248][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:06:28,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:06:28,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:06:28,251][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:06:28,251][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:06:28,252][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:06:28,319][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 18:07:17,984][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/5>
[2024-06-20 18:07:17,984][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:07:17,987][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:07:18,163][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:07:18,165][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:07:18,166][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:07:18,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:07:18,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:07:18,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:07:18,169][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:07:18,170][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:07:18,170][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:07:18,171][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:07:18,172][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:07:18,202][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.261    
                                                              val/f1: 0.300     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.300 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 18:08:07,671][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/6>
[2024-06-20 18:08:07,672][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:08:07,674][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:08:07,847][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:08:07,849][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:08:07,850][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:08:07,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:08:07,852][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:08:07,853][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:08:07,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:08:07,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:08:07,854][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:08:07,855][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:08:07,856][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:08:07,886][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.600 val/mre:    
                                                              0.073 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.021             
[2024-06-20 18:08:57,323][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/7>
[2024-06-20 18:08:57,324][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:08:57,326][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:08:57,525][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:08:57,527][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:08:57,527][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:08:57,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:08:57,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:08:57,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:08:57,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:08:57,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:08:57,532][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:08:57,533][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:08:57,534][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:08:57,563][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.079 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-20 18:09:47,213][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/8>
[2024-06-20 18:09:47,214][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:09:47,217][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:09:47,396][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:09:47,397][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:09:47,399][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:09:47,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:09:47,401][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:09:47,402][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:09:47,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:09:47,403][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:09:47,403][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:09:47,403][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:09:47,405][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:09:47,434][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.84it/s v_num: 0.000      
                                                              val/auc: 0.161    
                                                              val/f1: 0.111     
                                                              val/precision:    
                                                              0.125 val/recall: 
                                                              0.100 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-20 18:10:37,132][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.05/9>
[2024-06-20 18:10:37,132][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:10:37,135][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:10:37,314][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:10:37,316][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:10:37,317][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:10:37,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:10:37,319][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:10:37,320][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:10:37,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:10:37,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:10:37,321][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:10:37,321][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:10:37,323][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:10:37,355][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 18:11:27,011][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/0>
[2024-06-20 18:11:27,012][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:11:27,014][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:11:27,208][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:11:27,210][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:11:27,211][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:11:27,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:11:27,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:11:27,214][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:11:27,214][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:11:27,215][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:11:27,215][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:11:27,216][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:11:27,217][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:11:27,246][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.317    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.300 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 18:12:16,965][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/1>
[2024-06-20 18:12:16,966][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:12:16,968][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:12:17,146][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:12:17,147][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:12:17,148][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:12:17,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:12:17,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:12:17,151][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:12:17,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:12:17,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:12:17,154][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:12:17,154][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:12:17,156][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:12:17,194][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.86it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 18:13:06,515][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/2>
[2024-06-20 18:13:06,516][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:13:06,518][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:13:06,695][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:13:06,696][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:13:06,697][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:13:06,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:13:06,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:13:06,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:13:06,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:13:06,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:13:06,702][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:13:06,702][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:13:06,703][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:13:06,733][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.25it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 18:13:56,678][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/3>
[2024-06-20 18:13:56,681][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:13:56,684][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:13:56,866][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:13:56,867][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:13:56,868][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:13:56,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:13:56,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:13:56,871][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:13:56,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:13:56,872][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:13:56,873][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:13:56,873][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:13:56,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:13:56,902][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.83it/s v_num: 0.000      
                                                              val/auc: 0.322    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-20 18:14:46,612][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/4>
[2024-06-20 18:14:46,613][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:14:46,615][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:14:46,818][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:14:46,820][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:14:46,821][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:14:46,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:14:46,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:14:46,824][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:14:46,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:14:46,825][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:14:46,825][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:14:46,826][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:14:46,827][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:14:46,856][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.19it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-20 18:15:36,518][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/5>
[2024-06-20 18:15:36,518][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:15:36,520][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:15:36,695][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:15:36,697][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:15:36,698][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:15:36,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:15:36,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:15:36,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:15:36,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:15:36,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:15:36,702][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:15:36,702][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:15:36,704][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:15:36,732][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.84it/s v_num: 0.000      
                                                              val/auc: 0.211    
                                                              val/f1: 0.211     
                                                              val/precision:    
                                                              0.222 val/recall: 
                                                              0.200 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 18:16:25,000][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/6>
[2024-06-20 18:16:25,000][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:16:25,002][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:16:25,177][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:16:25,178][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:16:25,179][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:16:25,179][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:16:25,182][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:16:25,182][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:16:25,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:16:25,183][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:16:25,184][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:16:25,184][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:16:25,185][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:16:25,214][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.84it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.082 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.024             
[2024-06-20 18:17:14,677][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/7>
[2024-06-20 18:17:14,678][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:17:14,680][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:17:14,862][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:17:14,864][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:17:14,865][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:17:14,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:17:14,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:17:14,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:17:14,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:17:14,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:17:14,870][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:17:14,870][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:17:14,871][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:17:14,900][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.82it/s v_num: 0.000      
                                                              val/auc: 0.317    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.300 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 18:18:04,946][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/8>
[2024-06-20 18:18:04,947][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:18:04,951][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:18:05,135][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:18:05,136][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:18:05,137][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:18:05,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:18:05,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:18:05,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:18:05,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:18:05,143][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:18:05,143][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:18:05,143][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:18:05,144][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:18:05,207][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.11it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-20 18:18:55,279][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.1/9>
[2024-06-20 18:18:55,280][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:18:55,283][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:18:55,479][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:18:55,480][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:18:55,481][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:18:55,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:18:55,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:18:55,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:18:55,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:18:55,485][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:18:55,485][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:18:55,486][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:18:55,487][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:18:55,518][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.85it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.700 val/mre:    
                                                              0.091 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 18:19:45,237][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/0>
[2024-06-20 18:19:45,238][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:19:45,240][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:19:45,415][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:19:45,416][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:19:45,417][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:19:45,418][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:19:45,420][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:19:45,420][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:19:45,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:19:45,421][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:19:45,422][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:19:45,422][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:19:45,423][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:19:45,453][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.83it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 18:20:35,150][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/1>
[2024-06-20 18:20:35,151][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:20:35,154][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:20:35,330][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:20:35,331][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:20:35,332][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:20:35,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:20:35,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:20:35,336][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:20:35,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:20:35,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:20:35,337][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:20:35,337][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:20:35,338][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:20:35,368][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.86it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 18:21:25,711][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/2>
[2024-06-20 18:21:25,711][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:21:25,714][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:21:25,903][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:21:25,905][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:21:25,906][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:21:25,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:21:25,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:21:25,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:21:25,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:21:25,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:21:25,911][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:21:25,911][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:21:25,912][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:21:25,993][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 18:22:16,222][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/3>
[2024-06-20 18:22:16,223][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:22:16,226][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:22:16,409][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:22:16,410][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:22:16,411][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:22:16,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:22:16,414][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:22:16,415][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:22:16,415][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:22:16,416][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:22:16,416][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:22:16,416][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:22:16,418][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:22:16,449][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.86it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 18:23:06,562][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/4>
[2024-06-20 18:23:06,563][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:23:06,566][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:23:06,744][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:23:06,745][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:23:06,746][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:23:06,747][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:23:06,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:23:06,750][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:23:06,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:23:06,752][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:23:06,752][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:23:06,752][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:23:06,754][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:23:06,787][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-20 18:23:57,303][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/5>
[2024-06-20 18:23:57,303][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:23:57,305][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:23:57,484][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:23:57,486][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:23:57,487][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:23:57,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:23:57,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:23:57,490][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:23:57,491][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:23:57,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:23:57,492][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:23:57,492][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:23:57,493][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:23:57,523][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.455     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.500 val/mre:    
                                                              0.085 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.020             
[2024-06-20 18:24:47,075][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/6>
[2024-06-20 18:24:47,076][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:24:47,078][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:24:47,280][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:24:47,281][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:24:47,283][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:24:47,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:24:47,285][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:24:47,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:24:47,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:24:47,287][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:24:47,287][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:24:47,288][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:24:47,289][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:24:47,320][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.088 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-20 18:25:37,407][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/7>
[2024-06-20 18:25:37,407][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:25:37,410][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:25:37,592][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:25:37,594][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:25:37,595][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:25:37,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:25:37,598][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:25:37,598][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:25:37,598][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:25:37,600][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:25:37,601][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:25:37,601][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:25:37,602][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:25:37,666][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.23it/s v_num: 0.000      
                                                              val/auc: 0.267    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.038             
[2024-06-20 18:26:27,124][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/8>
[2024-06-20 18:26:27,124][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:26:27,127][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:26:27,308][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:26:27,310][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:26:27,311][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:26:27,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:26:27,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:26:27,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:26:27,314][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:26:27,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:26:27,317][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:26:27,317][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:26:27,318][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:26:27,351][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.82it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.028             
[2024-06-20 18:27:16,853][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.15/9>
[2024-06-20 18:27:16,854][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:27:16,857][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:27:17,043][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:27:17,044][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:27:17,045][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:27:17,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:27:17,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:27:17,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:27:17,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:27:17,049][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:27:17,050][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:27:17,050][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:27:17,051][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:27:17,083][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.84it/s v_num: 0.000      
                                                              val/auc: 0.278    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 18:28:06,815][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/0>
[2024-06-20 18:28:06,815][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:28:06,818][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:28:06,994][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:28:06,995][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:28:06,996][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:28:06,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:28:06,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:28:06,999][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:28:07,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:28:07,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:28:07,001][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:28:07,001][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:28:07,003][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:28:07,033][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.378    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 18:28:57,264][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/1>
[2024-06-20 18:28:57,264][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:28:57,269][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:28:57,449][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:28:57,451][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:28:57,452][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:28:57,452][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:28:57,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:28:57,455][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:28:57,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:28:57,456][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:28:57,456][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:28:57,456][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:28:57,458][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:28:57,488][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.75it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.082 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.022             
[2024-06-20 18:29:47,134][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/2>
[2024-06-20 18:29:47,135][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:29:47,137][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:29:47,319][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:29:47,321][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:29:47,322][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:29:47,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:29:47,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:29:47,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:29:47,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:29:47,328][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:29:47,328][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:29:47,328][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:29:47,330][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:29:47,364][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.85it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 18:30:37,107][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/3>
[2024-06-20 18:30:37,108][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:30:37,111][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:30:37,319][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:30:37,321][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:30:37,322][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:30:37,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:30:37,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:30:37,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:30:37,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:30:37,327][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:30:37,327][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:30:37,327][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:30:37,328][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:30:37,366][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.83it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 18:31:27,638][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/4>
[2024-06-20 18:31:27,639][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:31:27,641][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:31:28,033][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:31:28,034][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:31:28,035][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:31:28,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:31:28,038][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:31:28,038][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:31:28,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:31:28,039][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:31:28,040][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:31:28,040][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:31:28,041][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:31:28,071][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.11it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.092 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.017             
[2024-06-20 18:32:17,812][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/5>
[2024-06-20 18:32:17,812][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:32:17,814][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:32:17,986][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:32:17,987][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:32:17,988][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:32:17,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:32:17,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:32:17,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:32:17,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:32:17,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:32:17,993][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:32:17,993][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:32:17,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:32:18,023][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.82it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-20 18:33:08,080][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/6>
[2024-06-20 18:33:08,081][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:33:08,083][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:33:08,264][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:33:08,266][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:33:08,267][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:33:08,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:33:08,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:33:08,270][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:33:08,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:33:08,271][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:33:08,272][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:33:08,272][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:33:08,273][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:33:08,303][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.86it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 18:33:58,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/7>
[2024-06-20 18:33:58,686][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:33:58,689][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:33:58,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:33:58,869][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:33:58,871][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:33:58,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:33:58,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:33:58,874][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:33:58,874][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:33:58,876][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:33:58,877][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:33:58,877][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:33:58,878][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:33:58,913][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.81it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.093 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-20 18:34:48,738][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/8>
[2024-06-20 18:34:48,738][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:34:48,740][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:34:48,917][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:34:48,918][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:34:48,919][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:34:48,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:34:48,922][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:34:48,923][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:34:48,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:34:48,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:34:48,925][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:34:48,925][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:34:48,926][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:34:48,956][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.85it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 18:35:38,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.2/9>
[2024-06-20 18:35:38,425][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:35:38,427][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:35:38,605][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:35:38,607][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:35:38,608][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:35:38,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:35:38,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:35:38,611][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:35:38,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:35:38,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:35:38,613][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:35:38,613][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:35:38,614][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:35:38,645][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.311    
                                                              val/f1: 0.381     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.400 val/mre:    
                                                              0.094 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 18:36:28,255][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/0>
[2024-06-20 18:36:28,256][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:36:28,258][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:36:28,441][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:36:28,443][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:36:28,444][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:36:28,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:36:28,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:36:28,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:36:28,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:36:28,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:36:28,448][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:36:28,449][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:36:28,450][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:36:28,482][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.82it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.088 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 18:37:17,758][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/1>
[2024-06-20 18:37:17,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:37:17,762][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:37:17,937][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:37:17,939][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:37:17,940][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:37:17,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:37:17,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:37:17,943][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:37:17,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:37:17,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:37:17,945][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:37:17,945][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:37:17,946][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:37:17,976][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 18:38:07,435][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/2>
[2024-06-20 18:38:07,436][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:38:07,439][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:38:07,615][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:38:07,617][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:38:07,618][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:38:07,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:38:07,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:38:07,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:38:07,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:38:07,622][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:38:07,623][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:38:07,623][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:38:07,624][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:38:07,653][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 18:38:57,354][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/3>
[2024-06-20 18:38:57,355][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:38:57,357][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:38:57,560][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:38:57,562][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:38:57,563][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:38:57,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:38:57,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:38:57,566][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:38:57,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:38:57,567][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:38:57,567][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:38:57,567][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:38:57,569][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:38:57,606][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.090 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.018             
[2024-06-20 18:39:47,856][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/4>
[2024-06-20 18:39:47,856][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:39:47,860][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:39:48,032][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:39:48,033][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:39:48,034][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:39:48,034][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:39:48,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:39:48,037][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:39:48,037][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:39:48,038][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:39:48,038][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:39:48,038][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:39:48,040][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:39:48,070][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.80it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 18:40:37,755][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/5>
[2024-06-20 18:40:37,755][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:40:37,758][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:40:37,957][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:40:37,959][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:40:37,960][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:40:37,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:40:37,962][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:40:37,963][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:40:37,963][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:40:37,964][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:40:37,964][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:40:37,964][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:40:37,966][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:40:37,997][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.80it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.083 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.015             
[2024-06-20 18:41:27,747][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/6>
[2024-06-20 18:41:27,748][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:41:27,751][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:41:27,933][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:41:27,935][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:41:27,936][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:41:27,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:41:27,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:41:27,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:41:27,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:41:27,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:41:27,940][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:41:27,940][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:41:27,942][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:41:27,972][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.80it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.094 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 18:42:17,816][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/7>
[2024-06-20 18:42:17,817][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:42:17,822][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:42:18,012][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:42:18,013][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:42:18,014][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:42:18,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:42:18,018][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:42:18,019][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:42:18,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:42:18,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:42:18,021][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:42:18,021][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:42:18,022][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:42:18,058][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.455     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.500 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 18:43:07,754][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/8>
[2024-06-20 18:43:07,755][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:43:07,758][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:43:07,949][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:43:07,950][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:43:07,951][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:43:07,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:43:07,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:43:07,954][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:43:07,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:43:07,957][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:43:07,957][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:43:07,957][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:43:07,959][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:43:07,994][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.79it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 18:43:57,850][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.25/9>
[2024-06-20 18:43:57,851][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:43:57,854][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:43:58,276][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:43:58,278][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:43:58,279][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:43:58,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:43:58,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:43:58,282][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:43:58,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:43:58,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:43:58,284][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:43:58,284][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:43:58,287][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:43:58,359][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.84it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 18:44:48,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/0>
[2024-06-20 18:44:48,237][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:44:48,239][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:44:48,417][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:44:48,418][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:44:48,419][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:44:48,420][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:44:48,423][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:44:48,424][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:44:48,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:44:48,425][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:44:48,425][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:44:48,425][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:44:48,427][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:44:48,456][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.700 val/mre:    
                                                              0.088 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 18:45:38,073][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/1>
[2024-06-20 18:45:38,073][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:45:38,075][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:45:38,257][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:45:38,259][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:45:38,260][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:45:38,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:45:38,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:45:38,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:45:38,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:45:38,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:45:38,265][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:45:38,266][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:45:38,267][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:45:38,298][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.75it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 18:46:28,084][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/2>
[2024-06-20 18:46:28,085][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:46:28,088][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:46:28,271][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:46:28,273][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:46:28,274][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:46:28,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:46:28,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:46:28,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:46:28,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:46:28,278][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:46:28,279][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:46:28,279][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:46:28,280][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:46:28,312][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.72it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 18:47:18,812][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/3>
[2024-06-20 18:47:18,813][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:47:18,815][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:47:18,994][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:47:18,996][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:47:18,997][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:47:18,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:47:19,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:47:19,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:47:19,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:47:19,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:47:19,008][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:47:19,008][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:47:19,009][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:47:19,080][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.20it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.093 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 18:48:09,338][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/4>
[2024-06-20 18:48:09,339][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:48:09,341][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:48:09,547][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:48:09,548][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:48:09,549][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:48:09,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:48:09,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:48:09,552][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:48:09,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:48:09,554][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:48:09,554][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:48:09,554][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:48:09,556][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:48:09,585][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.81it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.099 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 18:49:00,319][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/5>
[2024-06-20 18:49:00,320][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:49:00,322][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:49:00,498][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:49:00,499][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:49:00,500][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:49:00,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:49:00,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:49:00,503][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:49:00,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:49:00,504][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:49:00,505][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:49:00,505][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:49:00,506][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:49:00,535][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.74it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.094 train/auc:  
                                                              0.938 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.902             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.018             
[2024-06-20 18:49:50,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/6>
[2024-06-20 18:49:50,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:49:50,455][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:49:50,633][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:49:50,635][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:49:50,636][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:49:50,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:49:50,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:49:50,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:49:50,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:49:50,640][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:49:50,640][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:49:50,640][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:49:50,642][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:49:50,671][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.77it/s v_num: 0.000      
                                                              val/auc: 0.689    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 18:50:41,086][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/7>
[2024-06-20 18:50:41,087][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:50:41,089][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:50:41,267][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:50:41,268][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:50:41,269][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:50:41,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:50:41,272][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:50:41,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:50:41,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:50:41,274][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:50:41,274][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:50:41,274][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:50:41,275][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:50:41,305][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.21it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.088 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 18:51:30,305][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/8>
[2024-06-20 18:51:30,306][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:51:30,308][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:51:30,483][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:51:30,484][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:51:30,485][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:51:30,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:51:30,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:51:30,489][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:51:30,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:51:30,490][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:51:30,491][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:51:30,491][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:51:30,492][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:51:30,521][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.85it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 18:52:19,233][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/dream_job/0.3/9>
[2024-06-20 18:52:19,234][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:52:19,236][HYDRA] 	#70 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:52:19,422][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:52:19,423][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:52:19,424][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:52:19,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:52:19,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:52:19,429][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:52:19,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:52:19,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:52:19,430][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:52:19,431][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:52:19,432][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:52:19,461][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.917  
                                                              train/f1: 0.914   
                                                              train/precision:  
                                                              0.946             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              0.022             
[2024-06-20 18:53:10,627][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/0>
[2024-06-20 18:53:10,628][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:53:10,631][HYDRA] 	#71 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:53:10,808][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:53:10,810][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:53:10,811][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:53:10,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:53:10,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:53:10,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:53:10,814][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:53:10,815][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:53:10,815][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:53:10,815][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:53:10,817][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:53:10,846][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.62it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.892  
                                                              train/f1: 0.879   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.783 train/mre:  
                                                              0.018             
[2024-06-20 18:54:02,372][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/1>
[2024-06-20 18:54:02,372][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:54:02,375][HYDRA] 	#72 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:54:02,563][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:54:02,564][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:54:02,565][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:54:02,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:54:02,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:54:02,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:54:02,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:54:02,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:54:02,571][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:54:02,572][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:54:02,573][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:54:02,615][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.61it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.950  
                                                              train/f1: 0.948   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.018             
[2024-06-20 18:54:54,504][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/2>
[2024-06-20 18:54:54,505][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:54:54,508][HYDRA] 	#73 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:54:54,688][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:54:54,690][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:54:54,691][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:54:54,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:54:54,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:54:54,694][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:54:54,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:54:54,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:54:54,696][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:54:54,697][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:54:54,698][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:54:54,763][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.63it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-20 18:55:46,187][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/3>
[2024-06-20 18:55:46,188][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:55:46,190][HYDRA] 	#74 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:55:46,366][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:55:46,367][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:55:46,368][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:55:46,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:55:46,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:55:46,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:55:46,372][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:55:46,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:55:46,373][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:55:46,373][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:55:46,374][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:55:46,405][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.69it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 18:56:37,787][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/4>
[2024-06-20 18:56:37,787][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:56:37,790][HYDRA] 	#75 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:56:37,967][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:56:37,969][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:56:37,970][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:56:37,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:56:37,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:56:37,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:56:37,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:56:37,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:56:37,974][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:56:37,975][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:56:37,976][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:56:38,008][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.65it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 18:57:29,277][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/5>
[2024-06-20 18:57:29,277][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:57:29,280][HYDRA] 	#76 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:57:29,457][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:57:29,459][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:57:29,460][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:57:29,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:57:29,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:57:29,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:57:29,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:57:29,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:57:29,464][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:57:29,465][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:57:29,466][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:57:29,496][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.64it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 18:58:20,909][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/6>
[2024-06-20 18:58:20,910][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:58:20,912][HYDRA] 	#77 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:58:21,113][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:58:21,115][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:58:21,116][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:58:21,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:58:21,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:58:21,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:58:21,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:58:21,120][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:58:21,120][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:58:21,120][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:58:21,122][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:58:21,152][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-20 18:59:12,326][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/7>
[2024-06-20 18:59:12,327][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:59:12,329][HYDRA] 	#78 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:59:12,512][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:59:12,513][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 18:59:12,514][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:59:12,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:59:12,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:59:12,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:59:12,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:59:12,519][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:59:12,519][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:59:12,519][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:59:12,521][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:59:12,564][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.63it/s v_num: 0.000      
                                                              val/auc: 0.256    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 19:00:04,385][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/8>
[2024-06-20 19:00:04,386][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:00:04,389][HYDRA] 	#79 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:00:04,565][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:00:04,567][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:00:04,568][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:00:04,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:00:04,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:00:04,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:00:04,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:00:04,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:00:04,573][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:00:04,573][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:00:04,574][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:00:04,604][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.62it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:00:55,900][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.0/9>
[2024-06-20 19:00:55,901][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:00:55,903][HYDRA] 	#80 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:00:56,084][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:00:56,085][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:00:56,086][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:00:56,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:00:56,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:00:56,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:00:56,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:00:56,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:00:56,091][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:00:56,091][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:00:56,093][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:00:56,123][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.69it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.067 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.022             
[2024-06-20 19:01:47,796][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/0>
[2024-06-20 19:01:47,796][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:01:47,799][HYDRA] 	#81 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:01:47,977][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:01:47,978][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:01:47,979][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:01:47,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:01:47,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:01:47,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:01:47,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:01:47,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:01:47,984][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:01:47,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:01:47,985][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:01:48,013][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.64it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 19:02:39,767][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/1>
[2024-06-20 19:02:39,768][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:02:39,770][HYDRA] 	#82 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:02:39,962][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:02:39,964][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:02:39,965][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:02:39,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:02:39,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:02:39,968][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:02:39,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:02:39,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:02:39,969][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:02:39,969][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:02:39,971][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:02:39,999][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.63it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.059 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.016             
[2024-06-20 19:03:31,288][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/2>
[2024-06-20 19:03:31,288][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:03:31,291][HYDRA] 	#83 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:03:31,466][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:03:31,468][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:03:31,469][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:03:31,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:03:31,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:03:31,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:03:31,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:03:31,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:03:31,473][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:03:31,473][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:03:31,475][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:03:31,505][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.63it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:04:22,912][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/3>
[2024-06-20 19:04:22,913][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:04:22,915][HYDRA] 	#84 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:04:23,095][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:04:23,097][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:04:23,098][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:04:23,098][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:04:23,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:04:23,101][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:04:23,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:04:23,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:04:23,102][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:04:23,102][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:04:23,104][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:04:23,134][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.72it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:05:14,285][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/4>
[2024-06-20 19:05:14,286][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:05:14,291][HYDRA] 	#85 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:05:14,472][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:05:14,473][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:05:14,474][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:05:14,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:05:14,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:05:14,477][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:05:14,478][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:05:14,478][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:05:14,479][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:05:14,479][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:05:14,480][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:05:14,510][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.59it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 19:06:05,755][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/5>
[2024-06-20 19:06:05,756][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:06:05,762][HYDRA] 	#86 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:06:05,953][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:06:05,955][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:06:05,956][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:06:05,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:06:05,958][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:06:05,959][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:06:05,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:06:05,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:06:05,960][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:06:05,960][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:06:05,962][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:06:05,991][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.68it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.068 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-20 19:06:57,173][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/6>
[2024-06-20 19:06:57,174][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:06:57,177][HYDRA] 	#87 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:06:57,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:06:57,356][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:06:57,357][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:06:57,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:06:57,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:06:57,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:06:57,362][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:06:57,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:06:57,363][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:06:57,363][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:06:57,364][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:06:57,393][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.58it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.070 train/auc:  
                                                              0.983 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 19:07:49,199][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/7>
[2024-06-20 19:07:49,199][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:07:49,202][HYDRA] 	#88 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:07:49,378][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:07:49,379][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:07:49,380][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:07:49,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:07:49,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:07:49,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:07:49,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:07:49,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:07:49,384][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:07:49,385][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:07:49,386][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:07:49,418][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.62it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 19:08:40,578][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/8>
[2024-06-20 19:08:40,580][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:08:40,584][HYDRA] 	#89 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:08:40,822][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:08:40,823][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:08:40,824][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:08:40,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:08:40,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:08:40,827][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:08:40,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:08:40,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:08:40,829][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:08:40,829][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:08:40,830][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:08:40,862][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 19:09:32,807][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.05/9>
[2024-06-20 19:09:32,807][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:09:32,809][HYDRA] 	#90 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:09:32,985][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:09:32,987][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:09:32,988][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:09:32,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:09:32,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:09:32,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:09:32,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:09:32,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:09:32,994][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:09:32,994][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:09:32,996][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:09:33,026][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.64it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.066 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.017             
[2024-06-20 19:10:24,385][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/0>
[2024-06-20 19:10:24,386][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:10:24,391][HYDRA] 	#91 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:10:24,594][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:10:24,596][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:10:24,597][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:10:24,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:10:24,599][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:10:24,600][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:10:24,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:10:24,601][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:10:24,601][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:10:24,601][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:10:24,603][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:10:24,649][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.59it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 19:11:16,479][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/1>
[2024-06-20 19:11:16,479][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:11:16,482][HYDRA] 	#92 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:11:16,657][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:11:16,659][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:11:16,660][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:11:16,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:11:16,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:11:16,664][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:11:16,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:11:16,665][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:11:16,665][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:11:16,665][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:11:16,667][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:11:16,696][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.69it/s v_num: 0.000      
                                                              val/auc: 0.317    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.333 val/mre:    
                                                              0.079 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.024             
[2024-06-20 19:12:07,664][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/2>
[2024-06-20 19:12:07,664][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:12:07,667][HYDRA] 	#93 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:12:07,842][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:12:07,843][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:12:07,844][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:12:07,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:12:07,847][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:12:07,847][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:12:07,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:12:07,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:12:07,849][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:12:07,849][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:12:07,850][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:12:07,881][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.66it/s v_num: 0.000      
                                                              val/auc: 0.317    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.333 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 19:12:59,008][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/3>
[2024-06-20 19:12:59,009][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:12:59,011][HYDRA] 	#94 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:12:59,187][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:12:59,189][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:12:59,190][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:12:59,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:12:59,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:12:59,193][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:12:59,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:12:59,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:12:59,195][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:12:59,195][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:12:59,196][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:12:59,226][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.65it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:13:50,354][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/4>
[2024-06-20 19:13:50,355][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:13:50,357][HYDRA] 	#95 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:13:50,534][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:13:50,536][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:13:50,537][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:13:50,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:13:50,539][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:13:50,540][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:13:50,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:13:50,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:13:50,541][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:13:50,542][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:13:50,543][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:13:50,572][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.56it/s v_num: 0.000      
                                                              val/auc: 0.306    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.111 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:14:42,397][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/5>
[2024-06-20 19:14:42,397][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:14:42,400][HYDRA] 	#96 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:14:42,582][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:14:42,583][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:14:42,584][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:14:42,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:14:42,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:14:42,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:14:42,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:14:42,589][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:14:42,589][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:14:42,589][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:14:42,590][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:14:42,626][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.72it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.078 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.027             
[2024-06-20 19:15:34,192][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/6>
[2024-06-20 19:15:34,193][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:15:34,195][HYDRA] 	#97 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:15:34,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:15:34,375][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:15:34,376][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:15:34,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:15:34,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:15:34,379][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:15:34,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:15:34,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:15:34,381][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:15:34,381][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:15:34,382][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:15:34,414][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.61it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.069 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.018             
[2024-06-20 19:16:24,879][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/7>
[2024-06-20 19:16:24,879][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:16:24,883][HYDRA] 	#98 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:16:25,066][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:16:25,068][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:16:25,069][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:16:25,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:16:25,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:16:25,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:16:25,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:16:25,073][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:16:25,073][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:16:25,073][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:16:25,075][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:16:25,105][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.57it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 19:17:16,867][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/8>
[2024-06-20 19:17:16,868][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:17:16,871][HYDRA] 	#99 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:17:17,054][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:17:17,056][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:17:17,057][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:17:17,057][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:17:17,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:17:17,060][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:17:17,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:17:17,061][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:17:17,062][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:17:17,062][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:17:17,063][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:17:17,093][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 19:18:08,580][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.1/9>
[2024-06-20 19:18:08,581][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:18:08,583][HYDRA] 	#100 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:18:08,807][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:18:08,809][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:18:08,810][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:18:08,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:18:08,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:18:08,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:18:08,814][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:18:08,815][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:18:08,815][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:18:08,815][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:18:08,817][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:18:08,888][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.58it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 19:19:00,374][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/0>
[2024-06-20 19:19:00,375][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:19:00,377][HYDRA] 	#101 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:19:00,552][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:19:00,554][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:19:00,555][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:19:00,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:19:00,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:19:00,558][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:19:00,559][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:19:00,559][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:19:00,560][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:19:00,560][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:19:00,561][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:19:00,591][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.67it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.081 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.021             
[2024-06-20 19:19:52,629][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/1>
[2024-06-20 19:19:52,629][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:19:52,633][HYDRA] 	#102 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:19:52,816][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:19:52,818][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:19:52,819][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:19:52,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:19:52,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:19:52,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:19:52,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:19:52,823][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:19:52,823][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:19:52,823][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:19:52,824][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:19:52,859][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.070 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.018             
[2024-06-20 19:20:44,299][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/2>
[2024-06-20 19:20:44,300][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:20:44,302][HYDRA] 	#103 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:20:44,510][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:20:44,512][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:20:44,513][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:20:44,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:20:44,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:20:44,516][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:20:44,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:20:44,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:20:44,518][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:20:44,518][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:20:44,519][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:20:44,549][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 5.47it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:21:36,797][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/3>
[2024-06-20 19:21:36,798][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:21:36,800][HYDRA] 	#104 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:21:36,974][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:21:36,976][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:21:36,977][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:21:36,977][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:21:36,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:21:36,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:21:36,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:21:36,981][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:21:36,981][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:21:36,981][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:21:36,982][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:21:37,010][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.61it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 19:22:28,160][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/4>
[2024-06-20 19:22:28,160][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:22:28,163][HYDRA] 	#105 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:22:28,343][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:22:28,345][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:22:28,346][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:22:28,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:22:28,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:22:28,349][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:22:28,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:22:28,350][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:22:28,351][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:22:28,351][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:22:28,352][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:22:28,385][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.59it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 19:23:20,122][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/5>
[2024-06-20 19:23:20,123][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:23:20,126][HYDRA] 	#106 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:23:20,301][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:23:20,303][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:23:20,304][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:23:20,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:23:20,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:23:20,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:23:20,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:23:20,308][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:23:20,308][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:23:20,309][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:23:20,310][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:23:20,339][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.073 train/auc:  
                                                              0.975 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 19:24:12,033][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/6>
[2024-06-20 19:24:12,033][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:24:12,036][HYDRA] 	#107 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:24:12,219][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:24:12,221][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:24:12,222][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:24:12,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:24:12,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:24:12,225][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:24:12,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:24:12,226][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:24:12,226][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:24:12,226][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:24:12,228][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:24:12,259][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 19:25:04,621][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/7>
[2024-06-20 19:25:04,621][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:25:04,624][HYDRA] 	#108 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:25:04,801][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:25:04,803][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:25:04,804][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:25:04,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:25:04,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:25:04,807][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:25:04,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:25:04,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:25:04,808][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:25:04,808][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:25:04,810][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:25:04,840][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 19:25:56,422][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/8>
[2024-06-20 19:25:56,423][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:25:56,425][HYDRA] 	#109 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:25:56,629][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:25:56,631][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:25:56,632][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:25:56,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:25:56,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:25:56,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:25:56,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:25:56,636][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:25:56,636][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:25:56,637][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:25:56,638][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:25:56,667][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.62it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 19:26:48,444][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.15/9>
[2024-06-20 19:26:48,445][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:26:48,451][HYDRA] 	#110 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:26:48,631][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:26:48,633][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:26:48,634][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:26:48,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:26:48,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:26:48,637][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:26:48,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:26:48,638][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:26:48,638][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:26:48,639][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:26:48,640][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:26:48,671][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.081 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.022             
[2024-06-20 19:27:39,739][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/0>
[2024-06-20 19:27:39,740][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:27:39,742][HYDRA] 	#111 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:27:39,911][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:27:39,913][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:27:39,914][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:27:39,914][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:27:39,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:27:39,916][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:27:39,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:27:39,917][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:27:39,918][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:27:39,918][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:27:39,919][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:27:39,946][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.69it/s v_num: 0.000      
                                                              val/auc: 0.311    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.222 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 19:28:30,454][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/1>
[2024-06-20 19:28:30,455][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:28:30,457][HYDRA] 	#112 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:28:30,631][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:28:30,633][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:28:30,634][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:28:30,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:28:30,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:28:30,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:28:30,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:28:30,640][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:28:30,640][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:28:30,641][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:28:30,642][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:28:30,670][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.66it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.078 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.018             
[2024-06-20 19:29:21,670][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/2>
[2024-06-20 19:29:21,671][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:29:21,673][HYDRA] 	#113 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:29:21,856][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:29:21,858][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:29:21,859][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:29:21,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:29:21,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:29:21,862][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:29:21,862][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:29:21,863][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:29:21,865][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:29:21,865][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:29:21,866][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:29:21,896][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 19:30:13,251][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/3>
[2024-06-20 19:30:13,252][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:30:13,254][HYDRA] 	#114 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:30:13,440][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:30:13,442][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:30:13,443][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:30:13,443][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:30:13,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:30:13,446][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:30:13,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:30:13,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:30:13,447][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:30:13,448][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:30:13,449][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:30:13,477][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.62it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 19:31:04,789][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/4>
[2024-06-20 19:31:04,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:31:04,793][HYDRA] 	#115 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:31:04,991][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:31:04,992][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:31:04,994][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:31:04,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:31:04,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:31:04,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:31:04,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:31:04,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:31:04,998][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:31:04,998][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:31:04,999][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:31:05,032][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.69it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 19:31:55,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/5>
[2024-06-20 19:31:55,841][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:31:55,843][HYDRA] 	#116 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:31:56,018][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:31:56,020][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:31:56,021][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:31:56,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:31:56,024][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:31:56,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:31:56,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:31:56,025][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:31:56,026][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:31:56,026][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:31:56,027][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:31:56,056][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.65it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 19:32:47,651][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/6>
[2024-06-20 19:32:47,651][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:32:47,654][HYDRA] 	#117 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:32:47,829][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:32:47,831][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:32:47,832][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:32:47,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:32:47,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:32:47,835][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:32:47,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:32:47,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:32:47,836][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:32:47,836][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:32:47,838][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:32:47,867][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.74it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.028             
[2024-06-20 19:33:38,798][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/7>
[2024-06-20 19:33:38,798][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:33:38,801][HYDRA] 	#118 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:33:38,976][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:33:38,978][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:33:38,979][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:33:38,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:33:38,981][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:33:38,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:33:38,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:33:38,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:33:38,983][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:33:38,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:33:38,985][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:33:39,015][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.65it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 19:34:30,375][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/8>
[2024-06-20 19:34:30,376][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:34:30,378][HYDRA] 	#119 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:34:30,550][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:34:30,552][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:34:30,553][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:34:30,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:34:30,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:34:30,556][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:34:30,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:34:30,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:34:30,557][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:34:30,557][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:34:30,559][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:34:30,589][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.61it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 19:35:22,076][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.2/9>
[2024-06-20 19:35:22,076][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:35:22,078][HYDRA] 	#120 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:35:22,256][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:35:22,257][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:35:22,258][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:35:22,259][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:35:22,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:35:22,261][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:35:22,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:35:22,264][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:35:22,265][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:35:22,265][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:35:22,266][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:35:22,331][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.54it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.097 train/auc:  
                                                              0.917 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.023             
[2024-06-20 19:36:13,836][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/0>
[2024-06-20 19:36:13,837][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:36:13,839][HYDRA] 	#121 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:36:14,018][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:36:14,020][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:36:14,021][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:36:14,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:36:14,024][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:36:14,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:36:14,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:36:14,025][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:36:14,026][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:36:14,026][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:36:14,027][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:36:14,059][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.83it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 19:37:05,616][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/1>
[2024-06-20 19:37:05,617][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:37:05,620][HYDRA] 	#122 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:37:05,791][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:37:05,793][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:37:05,794][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:37:05,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:37:05,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:37:05,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:37:05,802][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:37:05,803][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:37:05,803][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:37:05,803][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:37:05,805][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:37:05,839][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.66it/s v_num: 0.000      
                                                              val/auc: 0.317    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.333 val/mre:    
                                                              0.074 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.016             
[2024-06-20 19:37:56,232][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/2>
[2024-06-20 19:37:56,233][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:37:56,235][HYDRA] 	#123 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:37:56,411][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:37:56,413][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:37:56,414][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:37:56,414][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:37:56,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:37:56,417][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:37:56,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:37:56,418][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:37:56,418][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:37:56,418][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:37:56,420][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:37:56,448][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.61it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 19:38:47,962][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/3>
[2024-06-20 19:38:47,963][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:38:47,966][HYDRA] 	#124 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:38:48,147][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:38:48,149][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:38:48,150][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:38:48,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:38:48,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:38:48,152][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:38:48,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:38:48,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:38:48,154][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:38:48,154][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:38:48,155][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:38:48,185][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.57it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 19:39:39,277][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/4>
[2024-06-20 19:39:39,279][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:39:39,282][HYDRA] 	#125 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:39:39,516][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:39:39,518][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:39:39,519][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:39:39,519][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:39:39,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:39:39,522][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:39:39,522][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:39:39,525][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:39:39,525][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:39:39,525][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:39:39,527][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:39:39,599][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:40:32,208][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/5>
[2024-06-20 19:40:32,208][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:40:32,211][HYDRA] 	#126 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:40:32,404][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:40:32,406][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:40:32,407][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:40:32,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:40:32,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:40:32,410][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:40:32,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:40:32,412][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:40:32,412][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:40:32,413][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:40:32,414][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:40:32,523][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.58it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.778 val/mre:    
                                                              0.079 train/auc:  
                                                              0.908 train/f1:   
                                                              0.908             
                                                              train/precision:  
                                                              0.915             
                                                              train/recall:     
                                                              0.900 train/mre:  
                                                              0.015             
[2024-06-20 19:41:24,571][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/6>
[2024-06-20 19:41:24,571][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:41:24,574][HYDRA] 	#127 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:41:24,751][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:41:24,753][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:41:24,754][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:41:24,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:41:24,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:41:24,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:41:24,758][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:41:24,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:41:24,759][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:41:24,759][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:41:24,760][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:41:24,789][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.63it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 19:42:16,533][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/7>
[2024-06-20 19:42:16,533][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:42:16,536][HYDRA] 	#128 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:42:16,712][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:42:16,713][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:42:16,714][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:42:16,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:42:16,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:42:16,717][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:42:16,718][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:42:16,719][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:42:16,719][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:42:16,719][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:42:16,721][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:42:16,750][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.60it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 19:43:08,043][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/8>
[2024-06-20 19:43:08,043][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:43:08,046][HYDRA] 	#129 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:43:08,224][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:43:08,225][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:43:08,226][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:43:08,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:43:08,229][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:43:08,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:43:08,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:43:08,230][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:43:08,231][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:43:08,231][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:43:08,232][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:43:08,261][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.52it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 19:44:00,191][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.25/9>
[2024-06-20 19:44:00,192][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:44:00,194][HYDRA] 	#130 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:44:00,388][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:44:00,390][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:44:00,391][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:44:00,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:44:00,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:44:00,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:44:00,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:44:00,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:44:00,396][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:44:00,396][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:44:00,397][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:44:00,427][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.444 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 19:44:51,538][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/0>
[2024-06-20 19:44:51,538][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:44:51,541][HYDRA] 	#131 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:44:51,712][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:44:51,713][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:44:51,714][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:44:51,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:44:51,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:44:51,717][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:44:51,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:44:51,718][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:44:51,718][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:44:51,718][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:44:51,720][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:44:51,749][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.63it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre:    
                                                              0.087 train/auc:  
                                                              0.900 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.914             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              0.017             
[2024-06-20 19:45:43,345][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/1>
[2024-06-20 19:45:43,346][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:45:43,349][HYDRA] 	#132 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:45:43,785][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:45:43,786][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:45:43,787][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:45:43,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:45:43,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:45:43,790][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:45:43,790][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:45:43,791][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:45:43,791][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:45:43,791][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:45:43,794][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:45:43,823][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.67it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.080 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 19:46:34,959][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/2>
[2024-06-20 19:46:34,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:46:34,962][HYDRA] 	#133 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:46:35,140][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:46:35,142][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:46:35,143][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:46:35,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:46:35,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:46:35,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:46:35,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:46:35,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:46:35,149][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:46:35,149][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:46:35,151][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:46:35,184][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.61it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 19:47:26,133][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/3>
[2024-06-20 19:47:26,134][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:47:26,137][HYDRA] 	#134 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:47:26,315][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:47:26,317][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:47:26,318][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:47:26,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:47:26,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:47:26,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:47:26,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:47:26,322][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:47:26,322][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:47:26,322][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:47:26,324][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:47:26,390][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.59it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.088 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 19:48:18,130][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/4>
[2024-06-20 19:48:18,131][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:48:18,134][HYDRA] 	#135 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:48:18,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:48:18,464][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:48:18,465][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:48:18,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:48:18,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:48:18,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:48:18,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:48:18,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:48:18,472][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:48:18,472][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:48:18,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:48:18,591][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.64it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 19:49:09,731][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/5>
[2024-06-20 19:49:09,732][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:49:09,738][HYDRA] 	#136 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:49:09,915][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:49:09,916][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:49:09,917][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:49:09,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:49:09,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:49:09,920][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:49:09,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:49:09,921][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:49:09,922][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:49:09,922][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:49:09,923][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:49:09,954][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.56it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 19:50:02,131][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/6>
[2024-06-20 19:50:02,132][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:50:02,134][HYDRA] 	#137 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:50:02,308][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:50:02,310][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:50:02,311][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:50:02,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:50:02,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:50:02,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:50:02,314][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:50:02,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:50:02,315][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:50:02,315][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:50:02,317][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:50:02,349][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.61it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.556 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 19:50:53,785][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/7>
[2024-06-20 19:50:53,786][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:50:53,789][HYDRA] 	#138 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:50:53,967][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:50:53,969][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:50:53,970][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:50:53,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:50:53,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:50:53,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:50:53,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:50:53,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:50:53,975][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:50:53,975][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:50:53,976][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:50:54,010][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.84it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 19:51:45,710][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/8>
[2024-06-20 19:51:45,710][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:51:45,712][HYDRA] 	#139 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:51:45,893][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:51:45,894][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:51:45,895][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:51:45,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:51:45,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:51:45,898][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:51:45,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:51:45,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:51:45,911][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:51:45,911][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:51:45,912][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:51:45,983][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 19:52:37,214][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/easy_sleep/0.3/9>
[2024-06-20 19:52:37,214][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:52:37,217][HYDRA] 	#140 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:52:37,422][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:52:37,424][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:52:37,425][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:52:37,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:52:37,427][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:52:37,428][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:52:37,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:52:37,429][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:52:37,429][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:52:37,429][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:52:37,431][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:52:37,463][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.014             
[2024-06-20 19:53:26,598][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/0>
[2024-06-20 19:53:26,598][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:53:26,600][HYDRA] 	#141 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:53:26,780][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:53:26,782][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:53:26,783][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:53:26,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:53:26,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:53:26,786][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:53:26,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:53:26,787][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:53:26,788][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:53:26,788][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:53:26,789][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:53:26,911][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.944  
                                                              train/f1: 0.941   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.020             
[2024-06-20 19:54:14,828][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/1>
[2024-06-20 19:54:14,828][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:54:14,831][HYDRA] 	#142 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:54:15,011][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:54:15,012][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:54:15,013][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:54:15,014][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:54:15,016][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:54:15,016][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:54:15,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:54:15,018][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:54:15,018][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:54:15,018][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:54:15,019][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:54:15,051][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.25it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.981  
                                                              train/f1: 0.981   
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.020             
[2024-06-20 19:55:01,675][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/2>
[2024-06-20 19:55:01,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:55:01,684][HYDRA] 	#143 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:55:01,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:55:01,869][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:55:01,870][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:55:01,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:55:01,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:55:01,873][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:55:01,874][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:55:01,875][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:55:01,875][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:55:01,875][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:55:01,877][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:55:01,907][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.016             
[2024-06-20 19:55:49,696][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/3>
[2024-06-20 19:55:49,697][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:55:49,699][HYDRA] 	#144 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:55:49,874][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:55:49,876][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:55:49,877][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:55:49,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:55:49,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:55:49,880][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:55:49,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:55:49,881][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:55:49,881][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:55:49,882][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:55:49,883][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:55:49,913][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.34it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.935  
                                                              train/f1: 0.937   
                                                              train/precision:  
                                                              0.912             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.020             
[2024-06-20 19:56:36,506][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/4>
[2024-06-20 19:56:36,507][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:56:36,509][HYDRA] 	#145 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:56:36,690][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:56:36,692][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:56:36,693][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:56:36,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:56:36,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:56:36,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:56:36,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:56:36,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:56:36,697][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:56:36,697][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:56:36,699][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:56:36,728][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 19:57:24,024][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/5>
[2024-06-20 19:57:24,024][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:57:24,026][HYDRA] 	#146 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:57:24,234][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:57:24,236][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:57:24,237][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:57:24,237][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:57:24,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:57:24,240][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:57:24,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:57:24,241][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:57:24,241][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:57:24,242][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:57:24,243][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:57:24,273][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.47it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.027             
[2024-06-20 19:58:11,707][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/6>
[2024-06-20 19:58:11,707][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:58:11,710][HYDRA] 	#147 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:58:11,880][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:58:11,881][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:58:11,882][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:58:11,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:58:11,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:58:11,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:58:11,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:58:11,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:58:11,886][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:58:11,886][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:58:11,888][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:58:11,916][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.25it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.907  
                                                              train/f1: 0.898   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.815 train/mre:  
                                                              0.017             
[2024-06-20 19:58:59,075][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/7>
[2024-06-20 19:58:59,076][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:58:59,078][HYDRA] 	#148 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:58:59,289][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:58:59,291][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:58:59,292][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:58:59,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:58:59,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:58:59,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:58:59,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:58:59,296][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:58:59,296][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:58:59,296][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:58:59,297][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:58:59,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.34it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 19:59:46,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/8>
[2024-06-20 19:59:46,424][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:59:46,427][HYDRA] 	#149 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:59:46,609][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:59:46,611][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 19:59:46,612][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:59:46,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:59:46,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:59:46,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:59:46,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:59:46,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:59:46,619][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:59:46,619][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:59:46,620][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:59:46,685][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.256    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.926  
                                                              train/f1: 0.923   
                                                              train/precision:  
                                                              0.960             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.014             
[2024-06-20 20:00:33,643][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.0/9>
[2024-06-20 20:00:33,644][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:00:33,653][HYDRA] 	#150 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:00:33,839][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:00:33,841][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:00:33,842][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:00:33,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:00:33,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:00:33,845][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:00:33,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:00:33,848][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:00:33,848][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:00:33,848][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:00:33,849][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:00:33,890][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 20:01:21,804][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/0>
[2024-06-20 20:01:21,805][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:01:21,807][HYDRA] 	#151 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:01:21,989][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:01:21,991][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:01:21,992][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:01:21,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:01:21,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:01:21,995][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:01:21,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:01:21,996][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:01:21,996][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:01:21,997][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:01:21,998][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:01:22,028][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 20:02:08,899][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/1>
[2024-06-20 20:02:08,899][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:02:08,902][HYDRA] 	#152 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:02:09,082][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:02:09,084][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:02:09,085][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:02:09,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:02:09,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:02:09,088][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:02:09,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:02:09,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:02:09,089][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:02:09,089][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:02:09,091][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:02:09,121][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.30it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 20:02:56,863][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/2>
[2024-06-20 20:02:56,864][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:02:56,866][HYDRA] 	#153 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:02:57,041][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:02:57,043][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:02:57,044][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:02:57,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:02:57,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:02:57,046][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:02:57,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:02:57,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:02:57,048][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:02:57,049][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:02:57,050][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:02:57,081][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.45it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.074 train/auc:  
                                                              0.963 train/f1:   
                                                              0.964             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 20:03:44,092][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/3>
[2024-06-20 20:03:44,092][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:03:44,095][HYDRA] 	#154 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:03:44,264][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:03:44,266][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:03:44,267][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:03:44,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:03:44,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:03:44,269][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:03:44,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:03:44,270][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:03:44,271][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:03:44,271][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:03:44,272][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:03:44,301][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 20:04:32,358][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/4>
[2024-06-20 20:04:32,359][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:04:32,361][HYDRA] 	#155 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:04:32,534][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:04:32,535][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:04:32,536][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:04:32,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:04:32,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:04:32,540][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:04:32,541][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:04:32,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:04:32,542][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:04:32,542][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:04:32,543][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:04:32,572][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.075 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.018             
[2024-06-20 20:05:19,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/5>
[2024-06-20 20:05:19,841][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:05:19,844][HYDRA] 	#156 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:05:20,022][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:05:20,024][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:05:20,025][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:05:20,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:05:20,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:05:20,028][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:05:20,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:05:20,029][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:05:20,029][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:05:20,029][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:05:20,031][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:05:20,059][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.080 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.017             
[2024-06-20 20:06:06,983][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/6>
[2024-06-20 20:06:06,983][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:06:06,985][HYDRA] 	#157 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:06:07,159][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:06:07,160][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:06:07,161][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:06:07,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:06:07,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:06:07,164][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:06:07,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:06:07,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:06:07,166][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:06:07,166][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:06:07,167][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:06:07,195][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.073 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.012             
[2024-06-20 20:06:54,718][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/7>
[2024-06-20 20:06:54,718][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:06:54,720][HYDRA] 	#158 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:06:54,893][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:06:54,895][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:06:54,896][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:06:54,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:06:54,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:06:54,899][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:06:54,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:06:54,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:06:54,900][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:06:54,901][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:06:54,902][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:06:54,930][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.12it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:07:41,605][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/8>
[2024-06-20 20:07:41,606][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:07:41,609][HYDRA] 	#159 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:07:41,784][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:07:41,785][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:07:41,786][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:07:41,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:07:41,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:07:41,789][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:07:41,790][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:07:41,790][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:07:41,791][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:07:41,791][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:07:41,792][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:07:41,820][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.074 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.018             
[2024-06-20 20:08:29,172][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.05/9>
[2024-06-20 20:08:29,173][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:08:29,175][HYDRA] 	#160 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:08:29,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:08:29,356][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:08:29,357][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:08:29,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:08:29,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:08:29,360][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:08:29,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:08:29,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:08:29,361][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:08:29,361][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:08:29,363][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:08:29,395][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:09:16,426][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/0>
[2024-06-20 20:09:16,427][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:09:16,429][HYDRA] 	#161 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:09:16,652][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:09:16,653][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:09:16,654][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:09:16,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:09:16,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:09:16,658][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:09:16,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:09:16,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:09:16,659][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:09:16,659][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:09:16,661][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:09:16,690][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 20:10:03,960][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/1>
[2024-06-20 20:10:03,961][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:10:03,963][HYDRA] 	#162 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:10:04,139][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:10:04,140][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:10:04,141][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:10:04,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:10:04,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:10:04,144][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:10:04,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:10:04,145][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:10:04,146][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:10:04,146][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:10:04,147][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:10:04,178][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 20:10:51,639][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/2>
[2024-06-20 20:10:51,640][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:10:51,657][HYDRA] 	#163 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:10:51,834][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:10:51,835][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:10:51,836][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:10:51,837][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:10:51,839][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:10:51,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:10:51,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:10:51,840][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:10:51,841][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:10:51,841][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:10:51,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:10:51,872][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.311    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.222 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 20:11:38,526][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/3>
[2024-06-20 20:11:38,526][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:11:38,528][HYDRA] 	#164 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:11:38,703][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:11:38,705][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:11:38,706][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:11:38,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:11:38,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:11:38,709][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:11:38,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:11:38,710][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:11:38,711][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:11:38,711][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:11:38,712][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:11:38,741][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 20:12:26,254][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/4>
[2024-06-20 20:12:26,254][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:12:26,256][HYDRA] 	#165 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:12:26,432][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:12:26,433][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:12:26,434][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:12:26,434][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:12:26,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:12:26,437][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:12:26,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:12:26,438][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:12:26,438][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:12:26,439][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:12:26,440][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:12:26,469][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.46it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 20:13:13,217][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/5>
[2024-06-20 20:13:13,217][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:13:13,220][HYDRA] 	#166 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:13:13,390][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:13:13,391][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:13:13,392][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:13:13,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:13:13,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:13:13,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:13:13,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:13:13,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:13:13,396][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:13:13,397][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:13:13,398][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:13:13,428][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.088 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 20:14:01,013][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/6>
[2024-06-20 20:14:01,014][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:14:01,016][HYDRA] 	#167 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:14:01,189][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:14:01,191][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:14:01,192][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:14:01,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:14:01,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:14:01,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:14:01,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:14:01,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:14:01,196][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:14:01,196][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:14:01,197][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:14:01,226][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.30it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:14:48,577][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/7>
[2024-06-20 20:14:48,577][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:14:48,580][HYDRA] 	#168 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:14:48,756][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:14:48,757][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:14:48,758][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:14:48,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:14:48,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:14:48,762][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:14:48,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:14:48,763][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:14:48,763][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:14:48,763][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:14:48,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:14:48,794][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-20 20:15:35,527][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/8>
[2024-06-20 20:15:35,527][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:15:35,529][HYDRA] 	#169 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:15:35,947][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:15:35,948][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:15:35,949][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:15:35,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:15:35,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:15:35,952][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:15:35,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:15:35,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:15:35,954][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:15:35,954][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:15:35,955][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:15:35,983][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.32it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 20:16:23,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.1/9>
[2024-06-20 20:16:23,162][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:16:23,170][HYDRA] 	#170 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:16:23,373][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:16:23,375][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:16:23,376][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:16:23,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:16:23,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:16:23,379][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:16:23,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:16:23,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:16:23,381][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:16:23,381][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:16:23,382][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:16:23,429][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.55it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-20 20:17:10,163][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/0>
[2024-06-20 20:17:10,163][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:17:10,166][HYDRA] 	#171 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:17:10,339][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:17:10,341][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:17:10,342][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:17:10,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:17:10,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:17:10,344][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:17:10,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:17:10,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:17:10,346][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:17:10,346][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:17:10,347][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:17:10,375][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 20:17:57,689][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/1>
[2024-06-20 20:17:57,689][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:17:57,691][HYDRA] 	#172 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:17:57,913][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:17:57,915][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:17:57,916][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:17:57,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:17:57,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:17:57,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:17:57,919][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:17:57,920][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:17:57,920][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:17:57,920][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:17:57,921][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:17:57,951][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:18:45,214][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/2>
[2024-06-20 20:18:45,214][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:18:45,217][HYDRA] 	#173 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:18:45,391][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:18:45,393][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:18:45,394][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:18:45,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:18:45,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:18:45,397][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:18:45,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:18:45,398][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:18:45,398][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:18:45,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:18:45,400][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:18:45,430][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.32it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.122 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 20:19:32,142][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/3>
[2024-06-20 20:19:32,143][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:19:32,145][HYDRA] 	#174 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:19:32,318][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:19:32,320][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:19:32,321][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:19:32,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:19:32,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:19:32,323][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:19:32,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:19:32,324][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:19:32,325][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:19:32,325][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:19:32,326][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:19:32,354][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 20:20:19,491][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/4>
[2024-06-20 20:20:19,492][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:20:19,494][HYDRA] 	#175 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:20:19,669][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:20:19,670][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:20:19,671][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:20:19,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:20:19,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:20:19,676][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:20:19,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:20:19,677][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:20:19,677][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:20:19,677][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:20:19,678][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:20:19,708][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.093 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:21:06,897][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/5>
[2024-06-20 20:21:06,898][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:21:06,900][HYDRA] 	#176 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:21:07,087][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:21:07,089][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:21:07,090][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:21:07,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:21:07,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:21:07,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:21:07,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:21:07,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:21:07,094][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:21:07,095][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:21:07,096][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:21:07,126][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.095 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.020             
[2024-06-20 20:21:54,867][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/6>
[2024-06-20 20:21:54,868][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:21:54,872][HYDRA] 	#177 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:21:55,079][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:21:55,081][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:21:55,082][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:21:55,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:21:55,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:21:55,085][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:21:55,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:21:55,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:21:55,087][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:21:55,087][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:21:55,088][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:21:55,140][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.095 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.014             
[2024-06-20 20:22:42,330][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/7>
[2024-06-20 20:22:42,330][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:22:42,333][HYDRA] 	#178 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:22:42,508][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:22:42,509][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:22:42,510][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:22:42,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:22:42,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:22:42,513][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:22:42,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:22:42,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:22:42,515][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:22:42,515][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:22:42,516][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:22:42,545][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.378    
                                                              val/f1: 0.455     
                                                              val/precision:    
                                                              0.385 val/recall: 
                                                              0.556 val/mre:    
                                                              0.090 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.018             
[2024-06-20 20:23:29,712][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/8>
[2024-06-20 20:23:29,713][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:23:29,715][HYDRA] 	#179 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:23:29,892][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:23:29,893][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:23:29,894][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:23:29,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:23:29,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:23:29,897][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:23:29,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:23:29,898][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:23:29,899][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:23:29,899][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:23:29,900][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:23:29,928][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.087 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.015             
[2024-06-20 20:24:17,126][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.15/9>
[2024-06-20 20:24:17,127][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:24:17,131][HYDRA] 	#180 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:24:17,340][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:24:17,342][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:24:17,343][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:24:17,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:24:17,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:24:17,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:24:17,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:24:17,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:24:17,348][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:24:17,348][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:24:17,349][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:24:17,380][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.25it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.103 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.014             
[2024-06-20 20:25:04,719][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/0>
[2024-06-20 20:25:04,719][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:25:04,722][HYDRA] 	#181 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:25:04,899][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:25:04,900][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:25:04,901][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:25:04,901][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:25:04,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:25:04,904][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:25:04,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:25:04,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:25:04,905][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:25:04,906][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:25:04,907][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:25:04,936][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.30it/s v_num: 0.000      
                                                              val/auc: 0.261    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.222 val/recall: 
                                                              0.222 val/mre:    
                                                              0.096 train/auc:  
                                                              0.963 train/f1:   
                                                              0.964             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 20:25:52,552][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/1>
[2024-06-20 20:25:52,553][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:25:52,555][HYDRA] 	#182 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:25:52,730][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:25:52,732][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:25:52,733][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:25:52,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:25:52,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:25:52,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:25:52,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:25:52,737][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:25:52,737][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:25:52,738][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:25:52,739][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:25:52,768][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:26:40,374][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/2>
[2024-06-20 20:26:40,375][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:26:40,379][HYDRA] 	#183 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:26:40,562][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:26:40,564][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:26:40,564][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:26:40,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:26:40,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:26:40,567][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:26:40,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:26:40,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:26:40,569][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:26:40,569][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:26:40,571][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:26:40,604][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.22it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.096 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:27:28,247][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/3>
[2024-06-20 20:27:28,248][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:27:28,250][HYDRA] 	#184 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:27:28,434][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:27:28,435][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:27:28,436][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:27:28,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:27:28,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:27:28,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:27:28,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:27:28,440][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:27:28,440][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:27:28,441][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:27:28,442][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:27:28,471][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.096 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:28:15,656][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/4>
[2024-06-20 20:28:15,656][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:28:15,658][HYDRA] 	#185 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:28:15,833][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:28:15,834][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:28:15,835][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:28:15,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:28:15,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:28:15,838][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:28:15,839][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:28:15,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:28:15,840][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:28:15,840][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:28:15,841][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:28:15,870][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:29:02,527][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/5>
[2024-06-20 20:29:02,528][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:29:02,530][HYDRA] 	#186 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:29:02,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:29:02,706][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:29:02,707][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:29:02,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:29:02,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:29:02,710][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:29:02,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:29:02,711][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:29:02,711][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:29:02,711][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:29:02,713][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:29:02,741][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.095 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 20:29:50,478][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/6>
[2024-06-20 20:29:50,478][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:29:50,481][HYDRA] 	#187 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:29:50,657][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:29:50,659][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:29:50,660][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:29:50,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:29:50,662][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:29:50,663][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:29:50,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:29:50,664][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:29:50,664][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:29:50,664][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:29:50,666][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:29:50,694][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.35it/s v_num: 0.000      
                                                              val/auc: 0.306    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.111 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:30:37,663][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/7>
[2024-06-20 20:30:37,663][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:30:37,666][HYDRA] 	#188 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:30:37,839][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:30:37,840][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:30:37,841][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:30:37,841][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:30:37,844][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:30:37,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:30:37,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:30:37,845][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:30:37,845][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:30:37,846][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:30:37,847][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:30:37,875][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:31:25,324][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/8>
[2024-06-20 20:31:25,325][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:31:25,327][HYDRA] 	#189 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:31:25,504][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:31:25,506][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:31:25,507][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:31:25,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:31:25,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:31:25,510][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:31:25,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:31:25,511][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:31:25,511][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:31:25,512][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:31:25,513][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:31:25,545][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.097 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:32:12,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.2/9>
[2024-06-20 20:32:12,840][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:32:12,843][HYDRA] 	#190 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:32:13,020][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:32:13,022][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:32:13,023][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:32:13,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:32:13,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:32:13,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:32:13,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:32:13,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:32:13,027][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:32:13,027][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:32:13,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:32:13,060][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.096 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-06-20 20:33:00,244][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/0>
[2024-06-20 20:33:00,245][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:33:00,247][HYDRA] 	#191 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:33:00,456][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:33:00,458][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:33:00,459][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:33:00,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:33:00,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:33:00,462][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:33:00,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:33:00,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:33:00,463][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:33:00,463][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:33:00,465][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:33:00,494][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.104 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.018             
[2024-06-20 20:33:48,066][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/1>
[2024-06-20 20:33:48,066][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:33:48,069][HYDRA] 	#192 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:33:48,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:33:48,246][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:33:48,247][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:33:48,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:33:48,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:33:48,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:33:48,251][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:33:48,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:33:48,252][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:33:48,252][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:33:48,253][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:33:48,283][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.51it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.101 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:34:35,192][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/2>
[2024-06-20 20:34:35,193][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:34:35,195][HYDRA] 	#193 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:34:35,365][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:34:35,366][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:34:35,367][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:34:35,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:34:35,370][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:34:35,370][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:34:35,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:34:35,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:34:35,372][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:34:35,372][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:34:35,373][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:34:35,401][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.31it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.444 val/mre:    
                                                              0.098 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 20:35:22,696][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/3>
[2024-06-20 20:35:22,697][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:35:22,700][HYDRA] 	#194 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:35:22,876][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:35:22,878][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:35:22,879][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:35:22,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:35:22,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:35:22,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:35:22,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:35:22,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:35:22,883][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:35:22,884][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:35:22,885][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:35:22,914][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.157 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:36:10,009][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/4>
[2024-06-20 20:36:10,010][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:36:10,012][HYDRA] 	#195 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:36:10,190][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:36:10,192][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:36:10,193][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:36:10,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:36:10,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:36:10,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:36:10,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:36:10,197][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:36:10,197][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:36:10,198][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:36:10,199][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:36:10,228][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.31it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.102 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 20:36:57,288][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/5>
[2024-06-20 20:36:57,288][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:36:57,291][HYDRA] 	#196 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:36:57,469][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:36:57,471][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:36:57,472][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:36:57,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:36:57,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:36:57,475][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:36:57,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:36:57,476][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:36:57,476][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:36:57,476][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:36:57,478][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:36:57,506][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.23it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.112 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 20:37:45,101][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/6>
[2024-06-20 20:37:45,102][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:37:45,104][HYDRA] 	#197 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:37:45,279][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:37:45,281][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:37:45,282][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:37:45,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:37:45,285][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:37:45,285][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:37:45,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:37:45,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:37:45,287][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:37:45,287][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:37:45,288][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:37:45,319][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.094 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 20:38:31,918][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/7>
[2024-06-20 20:38:31,918][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:38:31,921][HYDRA] 	#198 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:38:32,096][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:38:32,097][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:38:32,098][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:38:32,099][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:38:32,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:38:32,101][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:38:32,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:38:32,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:38:32,103][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:38:32,103][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:38:32,104][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:38:32,141][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.095 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:39:19,554][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/8>
[2024-06-20 20:39:19,557][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:39:19,573][HYDRA] 	#199 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:39:19,940][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:39:19,942][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:39:19,943][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:39:19,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:39:19,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:39:19,947][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:39:19,947][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:39:19,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:39:19,948][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:39:19,948][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:39:19,949][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:39:20,049][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.36it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.096 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.011             
[2024-06-20 20:40:07,189][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.25/9>
[2024-06-20 20:40:07,190][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:40:07,192][HYDRA] 	#200 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:40:07,372][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:40:07,374][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:40:07,375][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:40:07,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:40:07,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:40:07,378][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:40:07,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:40:07,381][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:40:07,381][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:40:07,382][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:40:07,383][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:40:07,448][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.31it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.114 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 20:40:54,038][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/0>
[2024-06-20 20:40:54,039][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:40:54,041][HYDRA] 	#201 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:40:54,215][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:40:54,216][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:40:54,217][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:40:54,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:40:54,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:40:54,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:40:54,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:40:54,221][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:40:54,222][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:40:54,222][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:40:54,223][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:40:54,251][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.34it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.111 train/auc:  
                                                              0.917 train/f1:   
                                                              0.922             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.019             
[2024-06-20 20:41:41,676][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/1>
[2024-06-20 20:41:41,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:41:41,679][HYDRA] 	#202 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:41:41,865][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:41:41,867][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:41:41,868][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:41:41,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:41:41,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:41:41,871][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:41:41,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:41:41,872][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:41:41,872][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:41:41,872][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:41:41,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:41:41,905][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.097 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 20:42:28,993][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/2>
[2024-06-20 20:42:28,993][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:42:28,996][HYDRA] 	#203 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:42:29,217][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:42:29,218][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:42:29,219][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:42:29,219][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:42:29,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:42:29,222][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:42:29,223][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:42:29,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:42:29,224][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:42:29,224][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:42:29,225][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:42:29,276][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.101 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 20:43:16,682][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/3>
[2024-06-20 20:43:16,682][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:43:16,684][HYDRA] 	#204 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:43:16,864][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:43:16,865][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:43:16,866][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:43:16,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:43:16,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:43:16,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:43:16,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:43:16,870][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:43:16,871][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:43:16,871][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:43:16,872][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:43:16,904][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.30it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.556 val/mre:    
                                                              0.127 train/auc:  
                                                              0.972 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              0.947             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-20 20:44:04,006][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/4>
[2024-06-20 20:44:04,006][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:44:04,009][HYDRA] 	#205 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:44:04,189][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:44:04,190][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:44:04,191][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:44:04,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:44:04,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:44:04,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:44:04,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:44:04,195][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:44:04,196][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:44:04,196][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:44:04,197][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:44:04,251][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.111 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 20:44:51,205][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/5>
[2024-06-20 20:44:51,206][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:44:51,208][HYDRA] 	#206 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:44:51,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:44:51,387][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:44:51,388][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:44:51,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:44:51,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:44:51,391][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:44:51,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:44:51,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:44:51,392][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:44:51,392][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:44:51,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:44:51,422][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.23it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.101 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 20:45:39,392][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/6>
[2024-06-20 20:45:39,393][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:45:39,395][HYDRA] 	#207 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:45:39,625][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:45:39,626][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:45:39,627][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:45:39,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:45:39,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:45:39,630][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:45:39,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:45:39,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:45:39,632][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:45:39,632][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:45:39,633][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:45:39,663][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.23it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.108 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 20:46:26,792][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/7>
[2024-06-20 20:46:26,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:46:26,795][HYDRA] 	#208 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:46:26,971][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:46:26,973][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:46:26,974][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:46:26,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:46:26,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:46:26,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:46:26,977][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:46:26,978][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:46:26,978][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:46:26,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:46:26,980][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:46:27,008][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.306    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.111 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 20:47:15,056][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/8>
[2024-06-20 20:47:15,057][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:47:15,062][HYDRA] 	#209 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:47:15,257][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:47:15,258][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:47:15,259][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:47:15,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:47:15,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:47:15,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:47:15,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:47:15,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:47:15,267][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:47:15,267][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:47:15,268][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:47:15,334][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.38it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.096 train/auc:  
                                                              0.954 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.930             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.011             
[2024-06-20 20:48:03,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/feeling_lately/0.3/9>
[2024-06-20 20:48:03,425][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:48:03,430][HYDRA] 	#210 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:48:03,610][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:48:03,611][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:48:03,612][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:48:03,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:48:03,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:48:03,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:48:03,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:48:03,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:48:03,616][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:48:03,617][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:48:03,618][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:48:03,650][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.70it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.023             
[2024-06-20 20:48:52,627][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/0>
[2024-06-20 20:48:52,627][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:48:52,629][HYDRA] 	#211 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:48:52,813][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:48:52,814][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:48:52,815][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:48:52,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:48:52,818][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:48:52,819][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:48:52,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:48:52,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:48:52,822][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:48:52,822][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:48:52,824][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:48:52,893][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 20:49:42,608][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/1>
[2024-06-20 20:49:42,608][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:49:42,610][HYDRA] 	#212 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:49:42,788][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:49:42,789][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:49:42,790][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:49:42,790][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:49:42,793][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:49:42,793][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:49:42,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:49:42,794][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:49:42,795][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:49:42,795][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:49:42,796][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:49:42,824][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.018             
[2024-06-20 20:50:31,941][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/2>
[2024-06-20 20:50:31,942][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:50:31,945][HYDRA] 	#213 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:50:32,124][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:50:32,126][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:50:32,127][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:50:32,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:50:32,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:50:32,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:50:32,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:50:32,131][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:50:32,131][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:50:32,132][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:50:32,133][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:50:32,163][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 20:51:22,030][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/3>
[2024-06-20 20:51:22,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:51:22,033][HYDRA] 	#214 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:51:22,222][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:51:22,224][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:51:22,225][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:51:22,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:51:22,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:51:22,228][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:51:22,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:51:22,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:51:22,231][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:51:22,232][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:51:22,233][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:51:22,300][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 20:52:11,859][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/4>
[2024-06-20 20:52:11,859][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:52:11,862][HYDRA] 	#215 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:52:12,042][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:52:12,043][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:52:12,044][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:52:12,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:52:12,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:52:12,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:52:12,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:52:12,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:52:12,049][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:52:12,049][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:52:12,050][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:52:12,080][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 20:53:01,542][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/5>
[2024-06-20 20:53:01,543][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:53:01,545][HYDRA] 	#216 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:53:01,732][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:53:01,734][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:53:01,735][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:53:01,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:53:01,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:53:01,738][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:53:01,739][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:53:01,741][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:53:01,742][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:53:01,742][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:53:01,743][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:53:01,809][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.521    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 20:53:51,435][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/6>
[2024-06-20 20:53:51,436][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:53:51,439][HYDRA] 	#217 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:53:51,797][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:53:51,799][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:53:51,800][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:53:51,800][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:53:51,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:53:51,805][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:53:51,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:53:51,806][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:53:51,806][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:53:51,807][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:53:51,808][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:53:51,926][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.027             
[2024-06-20 20:54:41,072][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/7>
[2024-06-20 20:54:41,073][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:54:41,075][HYDRA] 	#218 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:54:41,259][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:54:41,261][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:54:41,262][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:54:41,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:54:41,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:54:41,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:54:41,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:54:41,268][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:54:41,269][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:54:41,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:54:41,270][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:54:41,337][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.021             
[2024-06-20 20:55:30,886][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/8>
[2024-06-20 20:55:30,886][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:55:30,889][HYDRA] 	#219 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:55:31,066][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:55:31,067][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:55:31,068][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:55:31,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:55:31,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:55:31,071][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:55:31,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:55:31,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:55:31,073][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:55:31,073][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:55:31,074][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:55:31,103][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.571    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.018             
[2024-06-20 20:56:20,491][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.0/9>
[2024-06-20 20:56:20,492][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:56:20,494][HYDRA] 	#220 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:56:20,673][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:56:20,675][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:56:20,676][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:56:20,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:56:20,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:56:20,679][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:56:20,679][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:56:20,680][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:56:20,680][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:56:20,681][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:56:20,682][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:56:20,710][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.056 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.027             
[2024-06-20 20:57:10,280][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/0>
[2024-06-20 20:57:10,280][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:57:10,284][HYDRA] 	#221 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:57:10,477][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:57:10,478][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:57:10,479][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:57:10,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:57:10,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:57:10,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:57:10,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:57:10,488][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:57:10,489][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:57:10,489][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:57:10,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:57:10,572][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.11it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-06-20 20:58:00,187][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/1>
[2024-06-20 20:58:00,188][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:58:00,194][HYDRA] 	#222 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:58:00,392][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:58:00,393][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:58:00,394][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:58:00,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:58:00,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:58:00,398][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:58:00,398][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:58:00,399][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:58:00,399][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:58:00,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:58:00,400][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:58:00,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.052 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.021             
[2024-06-20 20:58:50,124][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/2>
[2024-06-20 20:58:50,124][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:58:50,127][HYDRA] 	#223 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:58:50,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:58:50,336][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:58:50,337][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:58:50,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:58:50,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:58:50,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:58:50,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:58:50,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:58:50,343][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:58:50,344][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:58:50,345][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:58:50,413][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.443    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.286 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 20:59:39,484][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/3>
[2024-06-20 20:59:39,485][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:59:39,487][HYDRA] 	#224 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:59:39,665][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:59:39,667][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 20:59:39,668][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:59:39,668][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:59:39,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:59:39,671][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:59:39,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:59:39,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:59:39,672][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:59:39,673][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:59:39,674][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:59:39,703][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.042             
[2024-06-20 21:00:29,239][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/4>
[2024-06-20 21:00:29,240][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:00:29,242][HYDRA] 	#225 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:00:29,419][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:00:29,420][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:00:29,421][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:00:29,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:00:29,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:00:29,424][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:00:29,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:00:29,425][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:00:29,426][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:00:29,426][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:00:29,427][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:00:29,456][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 21:01:18,933][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/5>
[2024-06-20 21:01:18,934][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:01:18,936][HYDRA] 	#226 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:01:19,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:01:19,123][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:01:19,124][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:01:19,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:01:19,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:01:19,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:01:19,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:01:19,131][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:01:19,132][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:01:19,132][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:01:19,133][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:01:19,198][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.514    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre:    
                                                              0.072 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.024             
[2024-06-20 21:02:08,411][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/6>
[2024-06-20 21:02:08,412][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:02:08,414][HYDRA] 	#227 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:02:08,612][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:02:08,614][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:02:08,615][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:02:08,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:02:08,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:02:08,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:02:08,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:02:08,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:02:08,620][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:02:08,620][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:02:08,621][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:02:08,653][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.033             
[2024-06-20 21:02:58,928][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/7>
[2024-06-20 21:02:58,928][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:02:58,931][HYDRA] 	#228 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:02:59,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:02:59,122][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:02:59,123][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:02:59,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:02:59,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:02:59,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:02:59,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:02:59,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:02:59,130][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:02:59,130][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:02:59,132][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:02:59,200][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.058 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.023             
[2024-06-20 21:03:48,510][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/8>
[2024-06-20 21:03:48,510][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:03:48,513][HYDRA] 	#229 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:03:48,686][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:03:48,688][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:03:48,689][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:03:48,689][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:03:48,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:03:48,692][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:03:48,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:03:48,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:03:48,693][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:03:48,693][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:03:48,695][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:03:48,722][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.521    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre:    
                                                              0.058 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.020             
[2024-06-20 21:04:38,211][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.05/9>
[2024-06-20 21:04:38,211][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:04:38,214][HYDRA] 	#230 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:04:38,397][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:04:38,399][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:04:38,400][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:04:38,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:04:38,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:04:38,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:04:38,405][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:04:38,407][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:04:38,408][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:04:38,408][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:04:38,409][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:04:38,478][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.061 train/auc:  
                                                              0.942 train/f1:   
                                                              0.941             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.016             
[2024-06-20 21:05:28,456][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/0>
[2024-06-20 21:05:28,456][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:05:28,459][HYDRA] 	#231 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:05:28,646][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:05:28,647][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:05:28,648][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:05:28,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:05:28,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:05:28,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:05:28,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:05:28,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:05:28,653][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:05:28,653][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:05:28,655][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:05:28,685][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 21:06:17,737][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/1>
[2024-06-20 21:06:17,737][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:06:17,741][HYDRA] 	#232 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:06:17,918][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:06:17,920][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:06:17,921][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:06:17,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:06:17,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:06:17,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:06:17,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:06:17,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:06:17,926][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:06:17,926][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:06:17,927][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:06:17,957][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.72it/s v_num: 0.000      
                                                              val/auc: 0.271    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.143 val/mre:    
                                                              0.064 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.025             
[2024-06-20 21:07:07,743][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/2>
[2024-06-20 21:07:07,743][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:07:07,746][HYDRA] 	#233 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:07:07,951][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:07:07,952][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:07:07,953][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:07:07,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:07:07,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:07:07,956][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:07:07,957][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:07:07,963][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:07:07,964][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:07:07,964][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:07:07,965][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:07:08,058][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.058 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.018             
[2024-06-20 21:07:57,353][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/3>
[2024-06-20 21:07:57,354][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:07:57,356][HYDRA] 	#234 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:07:57,534][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:07:57,535][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:07:57,537][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:07:57,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:07:57,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:07:57,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:07:57,541][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:07:57,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:07:57,542][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:07:57,542][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:07:57,544][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:07:57,574][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.067 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.025             
[2024-06-20 21:08:46,718][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/4>
[2024-06-20 21:08:46,718][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:08:46,721][HYDRA] 	#235 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:08:46,905][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:08:46,907][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:08:46,908][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:08:46,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:08:46,911][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:08:46,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:08:46,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:08:46,914][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:08:46,915][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:08:46,915][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:08:46,916][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:08:46,987][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.271    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.143 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 21:09:36,715][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/5>
[2024-06-20 21:09:36,716][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:09:36,718][HYDRA] 	#236 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:09:36,895][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:09:36,896][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:09:36,897][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:09:36,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:09:36,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:09:36,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:09:36,901][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:09:36,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:09:36,902][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:09:36,902][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:09:36,904][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:09:36,932][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.86it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.061 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.023             
[2024-06-20 21:10:26,610][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/6>
[2024-06-20 21:10:26,610][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:10:26,613][HYDRA] 	#237 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:10:26,797][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:10:26,799][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:10:26,800][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:10:26,800][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:10:26,802][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:10:26,803][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:10:26,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:10:26,806][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:10:26,806][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:10:26,807][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:10:26,808][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:10:26,909][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.271    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.143 val/mre:    
                                                              0.070 train/auc:  
                                                              0.967 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.041             
[2024-06-20 21:11:18,768][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/7>
[2024-06-20 21:11:18,769][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:11:18,771][HYDRA] 	#238 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:11:18,963][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:11:18,964][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:11:18,965][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:11:18,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:11:18,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:11:18,968][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:11:18,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:11:18,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:11:18,970][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:11:18,970][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:11:18,972][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:11:19,003][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.85it/s v_num: 0.000      
                                                              val/auc: 0.386    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.571 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 21:12:08,422][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/8>
[2024-06-20 21:12:08,424][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:12:08,426][HYDRA] 	#239 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:12:08,602][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:12:08,604][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:12:08,605][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:12:08,605][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:12:08,607][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:12:08,608][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:12:08,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:12:08,609][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:12:08,609][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:12:08,609][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:12:08,611][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:12:08,659][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.543    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.095 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-06-20 21:12:58,009][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.1/9>
[2024-06-20 21:12:58,009][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:12:58,012][HYDRA] 	#240 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:12:58,196][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:12:58,197][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:12:58,198][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:12:58,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:12:58,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:12:58,201][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:12:58,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:12:58,227][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:12:58,228][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:12:58,228][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:12:58,229][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:12:58,343][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.08it/s v_num: 0.000      
                                                              val/auc: 0.221    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.125 val/recall: 
                                                              0.143 val/mre:    
                                                              0.059 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.015             
[2024-06-20 21:13:47,881][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/0>
[2024-06-20 21:13:47,882][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:13:47,884][HYDRA] 	#241 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:13:48,057][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:13:48,059][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:13:48,060][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:13:48,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:13:48,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:13:48,062][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:13:48,063][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:13:48,063][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:13:48,064][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:13:48,064][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:13:48,065][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:13:48,093][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 21:14:37,094][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/1>
[2024-06-20 21:14:37,095][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:14:37,098][HYDRA] 	#242 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:14:37,343][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:14:37,345][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:14:37,346][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:14:37,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:14:37,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:14:37,349][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:14:37,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:14:37,358][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:14:37,358][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:14:37,358][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:14:37,360][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:14:37,468][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.593    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.073 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.027             
[2024-06-20 21:15:27,003][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/2>
[2024-06-20 21:15:27,005][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:15:27,021][HYDRA] 	#243 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:15:27,227][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:15:27,228][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:15:27,230][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:15:27,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:15:27,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:15:27,233][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:15:27,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:15:27,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:15:27,235][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:15:27,235][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:15:27,236][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:15:27,267][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.386    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.571 val/mre:    
                                                              0.083 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.032             
[2024-06-20 21:16:17,088][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/3>
[2024-06-20 21:16:17,089][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:16:17,091][HYDRA] 	#244 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:16:17,273][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:16:17,275][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:16:17,276][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:16:17,276][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:16:17,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:16:17,279][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:16:17,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:16:17,280][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:16:17,280][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:16:17,280][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:16:17,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:16:17,322][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.081 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.030             
[2024-06-20 21:17:06,950][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/4>
[2024-06-20 21:17:06,951][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:17:06,953][HYDRA] 	#245 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:17:07,143][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:17:07,144][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:17:07,145][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:17:07,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:17:07,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:17:07,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:17:07,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:17:07,152][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:17:07,152][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:17:07,153][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:17:07,154][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:17:07,270][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 21:17:56,721][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/5>
[2024-06-20 21:17:56,721][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:17:56,724][HYDRA] 	#246 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:17:56,900][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:17:56,901][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:17:56,902][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:17:56,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:17:56,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:17:56,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:17:56,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:17:56,907][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:17:56,907][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:17:56,907][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:17:56,908][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:17:56,937][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.221    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.125 val/recall: 
                                                              0.143 val/mre:    
                                                              0.066 train/auc:  
                                                              0.900 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.800 train/mre:  
                                                              0.018             
[2024-06-20 21:18:46,507][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/6>
[2024-06-20 21:18:46,507][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:18:46,509][HYDRA] 	#247 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:18:46,695][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:18:46,697][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:18:46,698][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:18:46,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:18:46,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:18:46,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:18:46,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:18:46,704][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:18:46,705][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:18:46,705][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:18:46,706][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:18:46,774][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.075 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.023             
[2024-06-20 21:19:35,953][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/7>
[2024-06-20 21:19:35,953][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:19:35,956][HYDRA] 	#248 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:19:36,272][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:19:36,273][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:19:36,274][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:19:36,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:19:36,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:19:36,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:19:36,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:19:36,278][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:19:36,279][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:19:36,279][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:19:36,280][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:19:36,342][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 21:20:25,473][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/8>
[2024-06-20 21:20:25,473][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:20:25,476][HYDRA] 	#249 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:20:25,664][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:20:25,666][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:20:25,667][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:20:25,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:20:25,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:20:25,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:20:25,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:20:25,673][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:20:25,674][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:20:25,674][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:20:25,675][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:20:25,742][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-06-20 21:21:16,745][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.15/9>
[2024-06-20 21:21:16,748][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:21:16,751][HYDRA] 	#250 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:21:17,119][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:21:17,121][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:21:17,122][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:21:17,122][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:21:17,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:21:17,126][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:21:17,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:21:17,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:21:17,129][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:21:17,129][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:21:17,130][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:21:17,170][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.343    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.286 val/mre:    
                                                              0.065 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.019             
[2024-06-20 21:22:06,793][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/0>
[2024-06-20 21:22:06,794][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:22:06,796][HYDRA] 	#251 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:22:06,975][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:22:06,976][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:22:06,977][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:22:06,977][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:22:06,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:22:06,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:22:06,981][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:22:06,981][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:22:06,982][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:22:06,982][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:22:06,983][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:22:07,013][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-06-20 21:22:56,318][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/1>
[2024-06-20 21:22:56,319][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:22:56,321][HYDRA] 	#252 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:22:56,505][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:22:56,507][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:22:56,508][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:22:56,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:22:56,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:22:56,511][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:22:56,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:22:56,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:22:56,514][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:22:56,515][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:22:56,516][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:22:56,585][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.068 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.021             
[2024-06-20 21:23:46,308][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/2>
[2024-06-20 21:23:46,309][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:23:46,311][HYDRA] 	#253 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:23:46,490][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:23:46,492][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:23:46,493][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:23:46,493][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:23:46,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:23:46,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:23:46,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:23:46,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:23:46,497][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:23:46,497][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:23:46,499][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:23:46,530][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.070 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.025             
[2024-06-20 21:24:36,183][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/3>
[2024-06-20 21:24:36,184][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:24:36,186][HYDRA] 	#254 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:24:36,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:24:36,375][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:24:36,376][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:24:36,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:24:36,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:24:36,379][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:24:36,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:24:36,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:24:36,383][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:24:36,384][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:24:36,385][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:24:36,456][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.074 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.024             
[2024-06-20 21:25:27,338][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/4>
[2024-06-20 21:25:27,338][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:25:27,340][HYDRA] 	#255 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:25:27,519][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:25:27,521][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:25:27,522][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:25:27,522][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:25:27,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:25:27,524][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:25:27,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:25:27,526][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:25:27,526][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:25:27,526][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:25:27,528][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:25:27,558][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.457    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.385 val/recall: 
                                                              0.714 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 21:26:17,112][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/5>
[2024-06-20 21:26:17,112][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:26:17,115][HYDRA] 	#256 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:26:17,300][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:26:17,302][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:26:17,303][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:26:17,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:26:17,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:26:17,306][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:26:17,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:26:17,308][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:26:17,308][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:26:17,309][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:26:17,310][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:26:17,376][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.343    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.286 val/mre:    
                                                              0.066 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.017             
[2024-06-20 21:27:06,888][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/6>
[2024-06-20 21:27:06,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:27:06,891][HYDRA] 	#257 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:27:07,070][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:27:07,071][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:27:07,072][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:27:07,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:27:07,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:27:07,075][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:27:07,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:27:07,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:27:07,077][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:27:07,077][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:27:07,079][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:27:07,107][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.243    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.286 val/mre:    
                                                              0.077 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.033             
[2024-06-20 21:27:56,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/7>
[2024-06-20 21:27:56,560][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:27:56,562][HYDRA] 	#258 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:27:56,748][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:27:56,749][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:27:56,750][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:27:56,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:27:56,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:27:56,753][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:27:56,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:27:56,754][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:27:56,755][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:27:56,755][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:27:56,756][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:27:56,786][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.068 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.022             
[2024-06-20 21:28:46,133][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/8>
[2024-06-20 21:28:46,133][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:28:46,136][HYDRA] 	#259 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:28:46,320][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:28:46,321][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:28:46,322][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:28:46,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:28:46,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:28:46,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:28:46,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:28:46,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:28:46,329][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:28:46,329][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:28:46,331][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:28:46,400][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.084 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.031             
[2024-06-20 21:29:35,836][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.2/9>
[2024-06-20 21:29:35,837][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:29:35,839][HYDRA] 	#260 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:29:36,014][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:29:36,016][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:29:36,017][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:29:36,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:29:36,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:29:36,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:29:36,020][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:29:36,021][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:29:36,021][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:29:36,021][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:29:36,023][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:29:36,051][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.066 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.017             
[2024-06-20 21:30:26,212][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/0>
[2024-06-20 21:30:26,212][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:30:26,215][HYDRA] 	#261 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:30:26,403][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:30:26,405][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:30:26,406][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:30:26,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:30:26,408][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:30:26,409][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:30:26,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:30:26,411][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:30:26,412][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:30:26,412][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:30:26,413][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:30:26,484][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.88it/s v_num: 0.000      
                                                              val/auc: 0.414    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.429 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 21:31:16,497][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/1>
[2024-06-20 21:31:16,498][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:31:16,501][HYDRA] 	#262 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:31:16,683][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:31:16,685][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:31:16,686][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:31:16,686][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:31:16,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:31:16,689][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:31:16,689][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:31:16,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:31:16,693][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:31:16,693][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:31:16,695][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:31:16,728][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.493    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-20 21:32:05,856][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/2>
[2024-06-20 21:32:05,857][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:32:05,859][HYDRA] 	#263 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:32:06,037][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:32:06,039][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:32:06,040][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:32:06,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:32:06,042][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:32:06,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:32:06,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:32:06,044][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:32:06,044][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:32:06,044][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:32:06,046][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:32:06,074][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.80it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.075 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.021             
[2024-06-20 21:32:55,880][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/3>
[2024-06-20 21:32:55,880][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:32:55,883][HYDRA] 	#264 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:32:56,072][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:32:56,074][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:32:56,075][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:32:56,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:32:56,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:32:56,078][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:32:56,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:32:56,080][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:32:56,081][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:32:56,081][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:32:56,082][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:32:56,153][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.493    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.071 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.020             
[2024-06-20 21:33:45,446][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/4>
[2024-06-20 21:33:45,446][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:33:45,449][HYDRA] 	#265 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:33:45,628][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:33:45,630][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:33:45,631][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:33:45,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:33:45,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:33:45,634][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:33:45,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:33:45,647][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:33:45,647][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:33:45,647][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:33:45,649][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:33:45,690][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.074 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.018             
[2024-06-20 21:34:35,132][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/5>
[2024-06-20 21:34:35,132][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:34:35,134][HYDRA] 	#266 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:34:35,319][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:34:35,321][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:34:35,322][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:34:35,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:34:35,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:34:35,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:34:35,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:34:35,327][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:34:35,328][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:34:35,328][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:34:35,329][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:34:35,395][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.05it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.070 train/auc:  
                                                              0.942 train/f1:   
                                                              0.939             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.900 train/mre:  
                                                              0.030             
[2024-06-20 21:35:26,346][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/6>
[2024-06-20 21:35:26,346][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:35:26,348][HYDRA] 	#267 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:35:26,524][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:35:26,526][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:35:26,527][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:35:26,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:35:26,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:35:26,529][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:35:26,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:35:26,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:35:26,531][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:35:26,531][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:35:26,532][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:35:26,561][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.068 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.020             
[2024-06-20 21:36:15,652][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/7>
[2024-06-20 21:36:15,653][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:36:15,655][HYDRA] 	#268 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:36:15,855][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:36:15,857][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:36:15,858][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:36:15,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:36:15,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:36:15,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:36:15,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:36:15,863][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:36:15,863][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:36:15,864][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:36:15,865][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:36:15,932][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 3.73it/s v_num: 0.000      
                                                              val/auc: 0.393    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.286 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 21:37:05,933][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/8>
[2024-06-20 21:37:05,934][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:37:05,936][HYDRA] 	#269 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:37:06,111][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:37:06,112][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:37:06,113][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:37:06,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:37:06,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:37:06,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:37:06,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:37:06,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:37:06,119][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:37:06,119][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:37:06,120][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:37:06,151][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-20 21:37:55,314][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.25/9>
[2024-06-20 21:37:55,314][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:37:55,317][HYDRA] 	#270 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:37:55,496][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:37:55,497][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:37:55,498][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:37:55,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:37:55,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:37:55,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:37:55,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:37:55,503][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:37:55,503][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:37:55,503][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:37:55,505][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:37:55,534][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 21:38:44,630][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/0>
[2024-06-20 21:38:44,631][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:38:44,633][HYDRA] 	#271 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:38:44,815][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:38:44,817][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:38:44,818][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:38:44,818][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:38:44,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:38:44,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:38:44,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:38:44,824][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:38:44,824][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:38:44,824][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:38:44,826][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:38:44,892][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.200    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 21:39:34,176][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/1>
[2024-06-20 21:39:34,177][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:39:34,179][HYDRA] 	#272 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:39:34,372][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:39:34,373][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:39:34,374][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:39:34,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:39:34,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:39:34,377][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:39:34,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:39:34,378][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:39:34,379][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:39:34,379][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:39:34,380][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:39:34,408][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.74it/s v_num: 0.000      
                                                              val/auc: 0.543    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 21:40:24,702][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/2>
[2024-06-20 21:40:24,702][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:40:24,705][HYDRA] 	#273 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:40:24,897][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:40:24,899][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:40:24,900][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:40:24,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:40:24,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:40:24,903][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:40:24,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:40:24,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:40:24,907][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:40:24,907][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:40:24,908][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:40:24,978][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 21:41:14,073][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/3>
[2024-06-20 21:41:14,074][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:41:14,077][HYDRA] 	#274 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:41:14,278][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:41:14,279][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:41:14,280][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:41:14,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:41:14,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:41:14,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:41:14,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:41:14,285][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:41:14,285][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:41:14,285][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:41:14,286][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:41:14,317][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.493    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.076 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.015             
[2024-06-20 21:42:03,764][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/4>
[2024-06-20 21:42:03,765][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:42:03,767][HYDRA] 	#275 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:42:03,952][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:42:03,953][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:42:03,954][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:42:03,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:42:03,957][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:42:03,957][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:42:03,958][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:42:03,959][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:42:03,960][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:42:03,960][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:42:03,961][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:42:04,028][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 21:42:52,620][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/5>
[2024-06-20 21:42:52,620][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:42:52,623][HYDRA] 	#276 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:42:52,797][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:42:52,798][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:42:52,799][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:42:52,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:42:52,802][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:42:52,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:42:52,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:42:52,805][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:42:52,805][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:42:52,806][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:42:52,807][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:42:52,837][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.076 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.023             
[2024-06-20 21:43:41,578][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/6>
[2024-06-20 21:43:41,578][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:43:41,581][HYDRA] 	#277 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:43:41,755][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:43:41,757][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:43:41,758][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:43:41,758][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:43:41,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:43:41,761][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:43:41,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:43:41,762][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:43:41,762][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:43:41,763][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:43:41,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:43:41,792][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.01it/s v_num: 0.000      
                                                              val/auc: 0.271    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.143 val/mre:    
                                                              0.071 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.013             
[2024-06-20 21:44:31,165][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/7>
[2024-06-20 21:44:31,165][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:44:31,167][HYDRA] 	#278 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:44:31,348][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:44:31,350][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:44:31,351][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:44:31,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:44:31,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:44:31,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:44:31,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:44:31,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:44:31,356][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:44:31,357][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:44:31,358][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:44:31,424][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 21:45:20,433][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/8>
[2024-06-20 21:45:20,433][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:45:20,436][HYDRA] 	#279 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:45:20,768][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:45:20,769][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:45:20,770][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:45:20,771][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:45:20,773][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:45:20,773][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:45:20,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:45:20,774][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:45:20,775][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:45:20,775][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:45:20,776][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:45:20,809][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 21:46:10,172][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/friend_describe/0.3/9>
[2024-06-20 21:46:10,173][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:46:10,176][HYDRA] 	#280 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:46:10,357][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:46:10,359][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:46:10,360][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:46:10,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:46:10,362][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:46:10,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:46:10,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:46:10,365][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:46:10,365][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:46:10,365][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:46:10,367][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:46:10,433][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.967  
                                                              train/f1: 0.967   
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.019             
[2024-06-20 21:46:59,900][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/0>
[2024-06-20 21:46:59,900][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:46:59,902][HYDRA] 	#281 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:47:00,087][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:47:00,088][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:47:00,089][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:47:00,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:47:00,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:47:00,092][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:47:00,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:47:00,095][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:47:00,095][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:47:00,095][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:47:00,097][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:47:00,126][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.562    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.019             
[2024-06-20 21:47:49,095][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/1>
[2024-06-20 21:47:49,096][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:47:49,098][HYDRA] 	#282 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:47:49,275][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:47:49,276][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:47:49,277][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:47:49,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:47:49,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:47:49,282][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:47:49,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:47:49,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:47:49,284][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:47:49,284][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:47:49,286][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:47:49,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.892  
                                                              train/f1: 0.879   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.783 train/mre:  
                                                              0.014             
[2024-06-20 21:48:38,876][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/2>
[2024-06-20 21:48:38,877][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:48:38,879][HYDRA] 	#283 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:48:39,063][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:48:39,064][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:48:39,065][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:48:39,066][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:48:39,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:48:39,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:48:39,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:48:39,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:48:39,070][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:48:39,070][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:48:39,071][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:48:39,101][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.018             
[2024-06-20 21:49:28,172][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/3>
[2024-06-20 21:49:28,173][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:49:28,175][HYDRA] 	#284 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:49:28,370][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:49:28,371][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:49:28,372][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:49:28,373][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:49:28,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:49:28,376][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:49:28,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:49:28,378][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:49:28,378][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:49:28,378][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:49:28,380][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:49:28,419][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.983  
                                                              train/f1: 0.983   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.017             
[2024-06-20 21:50:17,506][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/4>
[2024-06-20 21:50:17,506][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:50:17,509][HYDRA] 	#285 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:50:17,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:50:17,718][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:50:17,719][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:50:17,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:50:17,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:50:17,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:50:17,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:50:17,724][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:50:17,725][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:50:17,725][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:50:17,726][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:50:17,916][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.016             
[2024-06-20 21:51:07,722][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/5>
[2024-06-20 21:51:07,725][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:51:07,738][HYDRA] 	#286 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:51:07,920][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:51:07,921][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:51:07,922][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:51:07,922][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:51:07,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:51:07,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:51:07,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:51:07,926][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:51:07,926][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:51:07,926][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:51:07,928][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:51:07,959][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.021             
[2024-06-20 21:51:58,060][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/6>
[2024-06-20 21:51:58,068][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:51:58,086][HYDRA] 	#287 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:51:58,272][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:51:58,274][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:51:58,275][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:51:58,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:51:58,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:51:58,278][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:51:58,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:51:58,282][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:51:58,282][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:51:58,282][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:51:58,284][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:51:58,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.026             
[2024-06-20 21:52:47,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/7>
[2024-06-20 21:52:47,841][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:52:47,843][HYDRA] 	#288 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:52:48,020][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:52:48,022][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:52:48,023][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:52:48,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:52:48,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:52:48,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:52:48,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:52:48,028][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:52:48,028][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:52:48,028][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:52:48,030][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:52:48,059][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.015             
[2024-06-20 21:53:36,757][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/8>
[2024-06-20 21:53:36,758][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:53:36,760][HYDRA] 	#289 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:53:36,936][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:53:36,938][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:53:36,939][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:53:36,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:53:36,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:53:36,942][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:53:36,942][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:53:36,943][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:53:36,943][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:53:36,943][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:53:36,945][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:53:36,973][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 21:54:26,529][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.0/9>
[2024-06-20 21:54:26,529][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:54:26,532][HYDRA] 	#290 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:54:26,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:54:26,717][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:54:26,718][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:54:26,718][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:54:26,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:54:26,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:54:26,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:54:26,724][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:54:26,725][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:54:26,725][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:54:26,726][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:54:26,794][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.500 val/mre:    
                                                              0.068 train/auc:  
                                                              0.983 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 21:55:15,868][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/0>
[2024-06-20 21:55:15,868][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:55:15,871][HYDRA] 	#291 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:55:16,047][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:55:16,049][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:55:16,050][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:55:16,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:55:16,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:55:16,055][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:55:16,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:55:16,056][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:55:16,056][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:55:16,056][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:55:16,058][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:55:16,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 21:56:07,031][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/1>
[2024-06-20 21:56:07,032][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:56:07,034][HYDRA] 	#292 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:56:07,219][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:56:07,220][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:56:07,221][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:56:07,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:56:07,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:56:07,224][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:56:07,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:56:07,228][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:56:07,229][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:56:07,229][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:56:07,230][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:56:07,296][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.512    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.125 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 21:56:56,590][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/2>
[2024-06-20 21:56:56,590][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:56:56,593][HYDRA] 	#293 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:56:56,787][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:56:56,788][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:56:56,789][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:56:56,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:56:56,791][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:56:56,792][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:56:56,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:56:56,793][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:56:56,793][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:56:56,793][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:56:56,795][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:56:56,830][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.02it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 21:57:45,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/3>
[2024-06-20 21:57:45,236][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:57:45,239][HYDRA] 	#294 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:57:45,460][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:57:45,462][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:57:45,463][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:57:45,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:57:45,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:57:45,467][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:57:45,467][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:57:45,470][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:57:45,470][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:57:45,471][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:57:45,472][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:57:45,598][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.125 val/mre:    
                                                              0.062 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.020             
[2024-06-20 21:58:34,595][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/4>
[2024-06-20 21:58:34,596][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:58:34,598][HYDRA] 	#295 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:58:34,773][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:58:34,775][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:58:34,776][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:58:34,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:58:34,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:58:34,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:58:34,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:58:34,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:58:34,781][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:58:34,781][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:58:34,782][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:58:34,810][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.067 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.017             
[2024-06-20 21:59:23,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/5>
[2024-06-20 21:59:23,887][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:59:23,890][HYDRA] 	#296 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:59:24,083][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:59:24,085][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 21:59:24,086][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:59:24,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:59:24,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:59:24,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:59:24,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:59:24,090][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:59:24,090][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:59:24,090][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:59:24,091][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:59:24,121][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.475    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.250 val/mre:    
                                                              0.067 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.019             
[2024-06-20 22:00:13,404][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/6>
[2024-06-20 22:00:13,405][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:00:13,408][HYDRA] 	#297 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:00:13,613][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:00:13,614][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:00:13,615][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:00:13,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:00:13,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:00:13,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:00:13,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:00:13,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:00:13,622][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:00:13,622][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:00:13,623][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:00:13,689][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 22:01:02,724][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/7>
[2024-06-20 22:01:02,725][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:01:02,728][HYDRA] 	#298 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:01:02,901][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:01:02,902][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:01:02,903][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:01:02,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:01:02,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:01:02,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:01:02,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:01:02,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:01:02,909][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:01:02,909][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:01:02,910][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:01:02,940][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 22:01:52,206][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/8>
[2024-06-20 22:01:52,206][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:01:52,208][HYDRA] 	#299 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:01:52,390][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:01:52,392][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:01:52,393][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:01:52,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:01:52,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:01:52,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:01:52,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:01:52,419][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:01:52,420][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:01:52,420][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:01:52,421][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:01:52,501][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.87it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 22:02:41,389][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.05/9>
[2024-06-20 22:02:41,390][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:02:41,394][HYDRA] 	#300 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:02:41,572][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:02:41,574][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:02:41,575][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:02:41,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:02:41,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:02:41,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:02:41,580][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:02:41,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:02:41,581][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:02:41,582][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:02:41,583][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:02:41,612][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.02it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.500 val/mre:    
                                                              0.068 train/auc:  
                                                              0.967 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.019             
[2024-06-20 22:03:30,340][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/0>
[2024-06-20 22:03:30,341][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:03:30,344][HYDRA] 	#301 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:03:30,526][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:03:30,527][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:03:30,528][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:03:30,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:03:30,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:03:30,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:03:30,532][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:03:30,534][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:03:30,535][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:03:30,535][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:03:30,536][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:03:30,602][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.588    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.375 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 22:04:19,984][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/1>
[2024-06-20 22:04:19,985][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:04:19,987][HYDRA] 	#302 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:04:20,164][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:04:20,166][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:04:20,167][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:04:20,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:04:20,169][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:04:20,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:04:20,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:04:20,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:04:20,171][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:04:20,171][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:04:20,173][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:04:20,202][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 22:05:08,633][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/2>
[2024-06-20 22:05:08,634][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:05:08,637][HYDRA] 	#303 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:05:08,811][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:05:08,812][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:05:08,813][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:05:08,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:05:08,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:05:08,816][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:05:08,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:05:08,817][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:05:08,818][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:05:08,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:05:08,820][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:05:08,849][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.475    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.250 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 22:05:58,337][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/3>
[2024-06-20 22:05:58,338][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:05:58,340][HYDRA] 	#304 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:05:58,523][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:05:58,524][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:05:58,525][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:05:58,526][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:05:58,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:05:58,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:05:58,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:05:58,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:05:58,532][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:05:58,532][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:05:58,533][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:05:58,632][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.625 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 22:06:47,916][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/4>
[2024-06-20 22:06:47,917][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:06:47,919][HYDRA] 	#305 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:06:48,097][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:06:48,099][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:06:48,100][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:06:48,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:06:48,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:06:48,103][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:06:48,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:06:48,104][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:06:48,104][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:06:48,104][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:06:48,105][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:06:48,134][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.01it/s v_num: 0.000      
                                                              val/auc: 0.525    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.250 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 22:07:37,118][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/5>
[2024-06-20 22:07:37,118][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:07:37,121][HYDRA] 	#306 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:07:37,303][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:07:37,305][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:07:37,306][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:07:37,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:07:37,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:07:37,309][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:07:37,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:07:37,312][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:07:37,313][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:07:37,313][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:07:37,314][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:07:37,380][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.500 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 22:08:26,697][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/6>
[2024-06-20 22:08:26,699][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:08:26,702][HYDRA] 	#307 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:08:27,070][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:08:27,072][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:08:27,073][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:08:27,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:08:27,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:08:27,076][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:08:27,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:08:27,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:08:27,078][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:08:27,078][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:08:27,079][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:08:27,108][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.500 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 22:09:16,411][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/7>
[2024-06-20 22:09:16,411][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:09:16,414][HYDRA] 	#308 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:09:16,597][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:09:16,598][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:09:16,599][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:09:16,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:09:16,602][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:09:16,602][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:09:16,603][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:09:16,605][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:09:16,606][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:09:16,606][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:09:16,607][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:09:16,674][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.512    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.625 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 22:10:06,051][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/8>
[2024-06-20 22:10:06,051][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:10:06,053][HYDRA] 	#309 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:10:06,227][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:10:06,228][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:10:06,229][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:10:06,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:10:06,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:10:06,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:10:06,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:10:06,233][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:10:06,234][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:10:06,234][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:10:06,235][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:10:06,265][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 22:10:54,722][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.1/9>
[2024-06-20 22:10:54,722][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:10:54,725][HYDRA] 	#310 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:10:54,905][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:10:54,907][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:10:54,908][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:10:54,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:10:54,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:10:54,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:10:54,911][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:10:54,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:10:54,912][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:10:54,913][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:10:54,914][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:10:54,943][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.073 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.019             
[2024-06-20 22:11:45,230][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/0>
[2024-06-20 22:11:45,230][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:11:45,233][HYDRA] 	#311 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:11:45,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:11:45,437][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:11:45,438][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:11:45,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:11:45,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:11:45,441][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:11:45,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:11:45,469][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:11:45,469][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:11:45,470][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:11:45,471][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:11:45,559][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 22:12:34,094][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/1>
[2024-06-20 22:12:34,094][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:12:34,097][HYDRA] 	#312 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:12:34,267][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:12:34,269][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:12:34,269][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:12:34,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:12:34,272][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:12:34,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:12:34,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:12:34,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:12:34,274][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:12:34,274][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:12:34,275][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:12:34,303][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.312    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.125 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 22:13:23,390][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/2>
[2024-06-20 22:13:23,390][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:13:23,393][HYDRA] 	#313 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:13:23,578][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:13:23,579][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:13:23,580][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:13:23,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:13:23,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:13:23,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:13:23,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:13:23,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:13:23,587][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:13:23,587][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:13:23,589][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:13:23,665][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.625 val/mre:    
                                                              0.073 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.018             
[2024-06-20 22:14:13,136][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/3>
[2024-06-20 22:14:13,137][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:14:13,139][HYDRA] 	#314 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:14:13,350][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:14:13,351][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:14:13,352][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:14:13,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:14:13,355][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:14:13,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:14:13,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:14:13,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:14:13,357][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:14:13,357][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:14:13,358][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:14:13,387][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.438    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.375 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 22:15:02,362][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/4>
[2024-06-20 22:15:02,363][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:15:02,365][HYDRA] 	#315 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:15:02,547][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:15:02,549][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:15:02,550][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:15:02,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:15:02,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:15:02,553][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:15:02,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:15:02,556][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:15:02,556][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:15:02,556][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:15:02,558][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:15:02,626][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.02it/s v_num: 0.000      
                                                              val/auc: 0.312    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.125 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 22:15:51,200][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/5>
[2024-06-20 22:15:51,201][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:15:51,204][HYDRA] 	#316 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:15:51,378][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:15:51,380][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:15:51,381][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:15:51,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:15:51,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:15:51,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:15:51,385][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:15:51,386][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:15:51,386][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:15:51,386][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:15:51,387][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:15:51,417][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.500 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-20 22:16:41,337][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/6>
[2024-06-20 22:16:41,338][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:16:41,340][HYDRA] 	#317 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:16:41,514][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:16:41,516][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:16:41,517][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:16:41,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:16:41,519][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:16:41,520][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:16:41,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:16:41,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:16:41,521][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:16:41,521][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:16:41,523][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:16:41,551][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.438    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.375 val/mre:    
                                                              0.075 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.019             
[2024-06-20 22:17:30,810][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/7>
[2024-06-20 22:17:30,810][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:17:30,813][HYDRA] 	#318 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:17:30,997][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:17:30,999][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:17:31,000][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:17:31,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:17:31,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:17:31,003][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:17:31,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:17:31,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:17:31,007][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:17:31,007][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:17:31,008][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:17:31,110][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.071 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.016             
[2024-06-20 22:18:19,914][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/8>
[2024-06-20 22:18:19,915][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:18:19,917][HYDRA] 	#319 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:18:20,089][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:18:20,090][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:18:20,091][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:18:20,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:18:20,094][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:18:20,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:18:20,095][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:18:20,095][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:18:20,096][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:18:20,096][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:18:20,097][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:18:20,126][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.475    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.250 val/mre:    
                                                              0.078 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.019             
[2024-06-20 22:19:10,050][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.15/9>
[2024-06-20 22:19:10,050][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:19:10,053][HYDRA] 	#320 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:19:10,233][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:19:10,235][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:19:10,236][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:19:10,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:19:10,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:19:10,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:19:10,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:19:10,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:19:10,243][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:19:10,243][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:19:10,244][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:19:10,316][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.081 train/auc:  
                                                              0.950 train/f1:   
                                                              0.948             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.018             
[2024-06-20 22:19:59,324][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/0>
[2024-06-20 22:19:59,325][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:19:59,327][HYDRA] 	#321 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:19:59,505][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:19:59,506][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:19:59,507][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:19:59,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:19:59,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:19:59,510][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:19:59,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:19:59,511][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:19:59,512][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:19:59,512][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:19:59,513][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:19:59,542][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.03it/s v_num: 0.000      
                                                              val/auc: 0.613    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.625 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 22:20:48,346][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/1>
[2024-06-20 22:20:48,346][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:20:48,349][HYDRA] 	#322 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:20:48,525][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:20:48,526][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:20:48,527][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:20:48,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:20:48,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:20:48,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:20:48,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:20:48,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:20:48,532][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:20:48,532][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:20:48,534][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:20:48,563][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.512    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.625 val/mre:    
                                                              0.079 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.016             
[2024-06-20 22:21:39,367][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/2>
[2024-06-20 22:21:39,368][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:21:39,370][HYDRA] 	#323 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:21:39,555][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:21:39,556][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:21:39,557][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:21:39,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:21:39,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:21:39,560][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:21:39,561][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:21:39,564][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:21:39,564][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:21:39,565][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:21:39,566][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:21:39,632][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.01it/s v_num: 0.000      
                                                              val/auc: 0.325    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.250 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-20 22:22:28,535][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/3>
[2024-06-20 22:22:28,536][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:22:28,538][HYDRA] 	#324 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:22:28,715][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:22:28,717][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:22:28,718][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:22:28,718][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:22:28,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:22:28,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:22:28,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:22:28,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:22:28,722][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:22:28,722][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:22:28,724][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:22:28,752][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.08it/s v_num: 0.000      
                                                              val/auc: 0.525    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.250 val/mre:    
                                                              0.083 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.017             
[2024-06-20 22:23:17,721][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/4>
[2024-06-20 22:23:17,721][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:23:17,724][HYDRA] 	#325 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:23:17,909][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:23:17,911][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:23:17,912][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:23:17,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:23:17,914][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:23:17,915][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:23:17,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:23:17,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:23:17,922][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:23:17,922][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:23:17,924][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:23:17,996][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 22:24:08,432][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/5>
[2024-06-20 22:24:08,432][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:24:08,434][HYDRA] 	#326 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:24:08,603][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:24:08,605][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:24:08,606][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:24:08,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:24:08,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:24:08,608][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:24:08,609][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:24:08,609][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:24:08,610][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:24:08,610][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:24:08,611][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:24:08,639][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 22:24:57,902][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/6>
[2024-06-20 22:24:57,903][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:24:57,905][HYDRA] 	#327 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:24:58,090][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:24:58,092][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:24:58,093][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:24:58,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:24:58,096][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:24:58,096][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:24:58,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:24:58,099][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:24:58,100][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:24:58,100][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:24:58,101][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:24:58,170][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.500 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 22:25:47,636][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/7>
[2024-06-20 22:25:47,639][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:25:47,657][HYDRA] 	#328 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:25:47,860][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:25:47,862][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:25:47,863][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:25:47,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:25:47,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:25:47,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:25:47,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:25:47,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:25:47,867][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:25:47,868][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:25:47,869][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:25:47,900][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.562    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.625 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 22:26:37,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/8>
[2024-06-20 22:26:37,114][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:26:37,117][HYDRA] 	#329 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:26:37,307][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:26:37,309][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:26:37,310][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:26:37,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:26:37,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:26:37,313][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:26:37,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:26:37,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:26:37,314][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:26:37,315][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:26:37,316][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:26:37,350][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 22:27:27,280][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.2/9>
[2024-06-20 22:27:27,281][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:27:27,283][HYDRA] 	#330 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:27:27,465][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:27:27,466][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:27:27,467][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:27:27,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:27:27,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:27:27,470][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:27:27,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:27:27,474][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:27:27,474][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:27:27,474][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:27:27,476][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:27:27,543][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.625    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.750 val/mre:    
                                                              0.086 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.020             
[2024-06-20 22:28:16,460][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/0>
[2024-06-20 22:28:16,461][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:28:16,464][HYDRA] 	#331 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:28:16,660][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:28:16,662][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:28:16,663][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:28:16,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:28:16,666][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:28:16,666][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:28:16,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:28:16,667][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:28:16,668][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:28:16,668][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:28:16,669][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:28:16,698][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 22:29:05,678][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/1>
[2024-06-20 22:29:05,679][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:29:05,683][HYDRA] 	#332 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:29:05,871][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:29:05,872][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:29:05,873][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:29:05,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:29:05,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:29:05,876][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:29:05,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:29:05,880][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:29:05,880][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:29:05,880][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:29:05,882][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:29:05,949][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 22:29:55,037][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/2>
[2024-06-20 22:29:55,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:29:55,040][HYDRA] 	#333 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:29:55,215][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:29:55,216][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:29:55,217][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:29:55,218][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:29:55,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:29:55,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:29:55,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:29:55,221][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:29:55,222][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:29:55,222][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:29:55,223][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:29:55,252][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.85it/s v_num: 0.000      
                                                              val/auc: 0.325    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.250 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-20 22:30:44,413][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/3>
[2024-06-20 22:30:44,414][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:30:44,416][HYDRA] 	#334 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:30:44,603][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:30:44,605][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:30:44,606][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:30:44,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:30:44,608][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:30:44,609][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:30:44,609][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:30:44,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:30:44,612][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:30:44,612][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:30:44,614][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:30:44,686][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.083 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.016             
[2024-06-20 22:31:34,404][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/4>
[2024-06-20 22:31:34,404][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:31:34,407][HYDRA] 	#335 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:31:34,576][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:31:34,578][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:31:34,579][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:31:34,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:31:34,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:31:34,582][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:31:34,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:31:34,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:31:34,584][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:31:34,584][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:31:34,585][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:31:34,613][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.475    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.250 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 22:32:24,953][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/5>
[2024-06-20 22:32:24,955][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:32:24,974][HYDRA] 	#336 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:32:25,220][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:32:25,221][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:32:25,222][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:32:25,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:32:25,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:32:25,225][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:32:25,226][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:32:25,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:32:25,250][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:32:25,251][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:32:25,252][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:32:25,331][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 22:33:13,946][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/6>
[2024-06-20 22:33:13,946][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:33:13,949][HYDRA] 	#337 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:33:14,122][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:33:14,123][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:33:14,124][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:33:14,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:33:14,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:33:14,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:33:14,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:33:14,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:33:14,129][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:33:14,129][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:33:14,131][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:33:14,164][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 22:34:03,019][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/7>
[2024-06-20 22:34:03,020][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:34:03,022][HYDRA] 	#338 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:34:03,203][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:34:03,204][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:34:03,205][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:34:03,206][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:34:03,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:34:03,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:34:03,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:34:03,211][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:34:03,212][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:34:03,212][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:34:03,213][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:34:03,473][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.562    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.625 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 22:34:53,115][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/8>
[2024-06-20 22:34:53,116][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:34:53,118][HYDRA] 	#339 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:34:53,291][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:34:53,292][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:34:53,293][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:34:53,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:34:53,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:34:53,296][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:34:53,297][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:34:53,298][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:34:53,298][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:34:53,298][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:34:53,300][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:34:53,329][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.537    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.375 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-20 22:35:41,904][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.25/9>
[2024-06-20 22:35:41,905][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:35:41,907][HYDRA] 	#340 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:35:42,083][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:35:42,084][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:35:42,085][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:35:42,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:35:42,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:35:42,088][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:35:42,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:35:42,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:35:42,090][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:35:42,090][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:35:42,091][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:35:42,120][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.500 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 22:36:31,324][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/0>
[2024-06-20 22:36:31,324][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:36:31,327][HYDRA] 	#341 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:36:31,510][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:36:31,512][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:36:31,512][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:36:31,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:36:31,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:36:31,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:36:31,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:36:31,518][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:36:31,519][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:36:31,519][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:36:31,520][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:36:31,586][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.500 val/mre:    
                                                              0.088 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 22:37:20,890][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/1>
[2024-06-20 22:37:20,890][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:37:20,893][HYDRA] 	#342 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:37:21,078][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:37:21,079][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:37:21,080][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:37:21,080][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:37:21,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:37:21,083][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:37:21,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:37:21,084][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:37:21,084][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:37:21,085][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:37:21,086][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:37:21,115][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 22:38:10,891][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/2>
[2024-06-20 22:38:10,892][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:38:10,895][HYDRA] 	#343 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:38:11,081][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:38:11,083][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:38:11,084][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:38:11,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:38:11,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:38:11,087][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:38:11,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:38:11,090][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:38:11,090][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:38:11,091][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:38:11,092][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:38:11,161][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.325    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.250 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-06-20 22:38:59,925][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/3>
[2024-06-20 22:38:59,925][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:38:59,928][HYDRA] 	#344 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:39:00,101][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:39:00,102][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:39:00,103][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:39:00,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:39:00,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:39:00,107][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:39:00,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:39:00,108][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:39:00,109][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:39:00,109][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:39:00,110][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:39:00,138][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.500 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 22:39:49,125][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/4>
[2024-06-20 22:39:49,126][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:39:49,128][HYDRA] 	#345 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:39:49,309][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:39:49,311][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:39:49,312][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:39:49,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:39:49,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:39:49,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:39:49,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:39:49,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:39:49,319][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:39:49,319][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:39:49,320][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:39:49,386][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-20 22:40:38,634][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/5>
[2024-06-20 22:40:38,634][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:40:38,637][HYDRA] 	#346 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:40:38,813][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:40:38,815][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:40:38,816][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:40:38,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:40:38,818][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:40:38,819][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:40:38,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:40:38,820][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:40:38,820][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:40:38,820][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:40:38,822][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:40:38,851][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 22:41:27,804][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/6>
[2024-06-20 22:41:27,805][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:41:27,807][HYDRA] 	#347 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:41:28,019][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:41:28,021][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:41:28,022][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:41:28,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:41:28,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:41:28,025][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:41:28,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:41:28,029][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:41:28,029][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:41:28,029][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:41:28,031][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:41:28,159][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.082 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.016             
[2024-06-20 22:42:17,226][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/7>
[2024-06-20 22:42:17,227][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:42:17,229][HYDRA] 	#348 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:42:17,405][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:42:17,407][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:42:17,408][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:42:17,408][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:42:17,411][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:42:17,411][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:42:17,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:42:17,412][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:42:17,413][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:42:17,413][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:42:17,414][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:42:17,454][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.01it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 22:43:06,509][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/8>
[2024-06-20 22:43:06,509][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:43:06,512][HYDRA] 	#349 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:43:06,689][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:43:06,691][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:43:06,692][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:43:06,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:43:06,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:43:06,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:43:06,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:43:06,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:43:06,696][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:43:06,696][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:43:06,698][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:43:06,727][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.387    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.375 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-20 22:43:55,630][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/last_happy/0.3/9>
[2024-06-20 22:43:55,631][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:43:55,633][HYDRA] 	#350 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:43:55,814][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:43:55,816][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:43:55,817][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:43:55,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:43:55,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:43:55,820][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:43:55,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:43:55,823][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:43:55,823][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:43:55,823][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:43:55,825][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:43:55,895][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.018             
[2024-06-20 22:44:45,120][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/0>
[2024-06-20 22:44:45,121][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:44:45,123][HYDRA] 	#351 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:44:45,301][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:44:45,303][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:44:45,304][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:44:45,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:44:45,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:44:45,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:44:45,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:44:45,308][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:44:45,309][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:44:45,309][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:44:45,310][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:44:45,339][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.934  
                                                              train/f1: 0.930   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.869 train/mre:  
                                                              0.020             
[2024-06-20 22:45:35,098][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/1>
[2024-06-20 22:45:35,098][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:45:35,101][HYDRA] 	#352 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:45:35,281][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:45:35,283][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:45:35,284][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:45:35,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:45:35,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:45:35,287][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:45:35,288][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:45:35,290][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:45:35,291][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:45:35,291][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:45:35,292][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:45:35,370][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.72it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.967  
                                                              train/f1: 0.968   
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 22:46:24,849][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/2>
[2024-06-20 22:46:24,849][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:46:24,852][HYDRA] 	#353 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:46:25,025][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:46:25,026][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:46:25,027][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:46:25,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:46:25,031][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:46:25,032][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:46:25,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:46:25,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:46:25,033][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:46:25,033][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:46:25,035][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:46:25,064][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.71it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.023             
[2024-06-20 22:47:15,250][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/3>
[2024-06-20 22:47:15,251][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:47:15,253][HYDRA] 	#354 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:47:15,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:47:15,437][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:47:15,438][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:47:15,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:47:15,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:47:15,441][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:47:15,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:47:15,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:47:15,445][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:47:15,445][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:47:15,447][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:47:15,514][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 22:48:04,869][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/4>
[2024-06-20 22:48:04,870][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:48:04,872][HYDRA] 	#355 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:48:05,043][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:48:05,044][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:48:05,045][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:48:05,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:48:05,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:48:05,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:48:05,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:48:05,049][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:48:05,049][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:48:05,050][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:48:05,051][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:48:05,079][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.014             
[2024-06-20 22:48:54,459][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/5>
[2024-06-20 22:48:54,459][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:48:54,461][HYDRA] 	#356 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:48:54,645][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:48:54,647][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:48:54,648][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:48:54,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:48:54,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:48:54,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:48:54,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:48:54,655][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:48:54,656][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:48:54,656][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:48:54,657][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:48:54,726][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.967  
                                                              train/f1: 0.968   
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.017             
[2024-06-20 22:49:43,941][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/6>
[2024-06-20 22:49:43,941][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:49:43,944][HYDRA] 	#357 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:49:44,123][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:49:44,124][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:49:44,125][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:49:44,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:49:44,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:49:44,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:49:44,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:49:44,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:49:44,130][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:49:44,130][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:49:44,132][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:49:44,162][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 22:50:33,438][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/7>
[2024-06-20 22:50:33,439][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:50:33,442][HYDRA] 	#358 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:50:33,627][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:50:33,628][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:50:33,629][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:50:33,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:50:33,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:50:33,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:50:33,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:50:33,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:50:33,635][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:50:33,636][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:50:33,637][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:50:33,704][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.014             
[2024-06-20 22:51:23,757][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/8>
[2024-06-20 22:51:23,758][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:51:23,763][HYDRA] 	#359 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:51:23,952][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:51:23,953][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:51:23,954][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:51:23,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:51:23,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:51:23,961][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:51:23,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:51:23,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:51:23,963][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:51:23,963][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:51:23,964][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:51:24,022][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.976   
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.017             
[2024-06-20 22:52:14,065][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.0/9>
[2024-06-20 22:52:14,065][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:52:14,068][HYDRA] 	#360 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:52:14,247][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:52:14,248][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:52:14,249][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:52:14,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:52:14,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:52:14,252][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:52:14,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:52:14,253][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:52:14,254][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:52:14,254][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:52:14,255][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:52:14,295][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 22:53:04,233][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/0>
[2024-06-20 22:53:04,234][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:53:04,236][HYDRA] 	#361 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:53:04,423][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:53:04,424][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:53:04,425][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:53:04,426][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:53:04,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:53:04,428][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:53:04,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:53:04,431][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:53:04,432][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:53:04,432][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:53:04,433][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:53:04,502][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 22:53:54,427][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/1>
[2024-06-20 22:53:54,428][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:53:54,430][HYDRA] 	#362 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:53:54,606][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:53:54,608][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:53:54,609][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:53:54,609][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:53:54,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:53:54,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:53:54,613][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:53:54,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:53:54,614][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:53:54,614][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:53:54,615][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:53:54,644][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.064 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 22:54:44,426][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/2>
[2024-06-20 22:54:44,426][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:54:44,429][HYDRA] 	#363 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:54:44,611][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:54:44,613][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:54:44,614][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:54:44,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:54:44,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:54:44,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:54:44,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:54:44,620][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:54:44,621][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:54:44,621][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:54:44,622][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:54:44,690][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.311    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.222 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 22:55:33,905][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/3>
[2024-06-20 22:55:33,906][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:55:33,908][HYDRA] 	#364 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:55:34,081][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:55:34,082][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:55:34,083][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:55:34,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:55:34,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:55:34,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:55:34,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:55:34,087][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:55:34,088][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:55:34,088][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:55:34,089][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:55:34,118][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.063 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 22:56:24,117][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/4>
[2024-06-20 22:56:24,117][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:56:24,119][HYDRA] 	#365 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:56:24,307][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:56:24,308][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:56:24,309][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:56:24,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:56:24,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:56:24,312][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:56:24,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:56:24,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:56:24,316][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:56:24,316][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:56:24,317][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:56:24,382][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.13it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 22:57:14,379][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/5>
[2024-06-20 22:57:14,379][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:57:14,382][HYDRA] 	#366 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:57:14,552][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:57:14,554][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:57:14,555][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:57:14,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:57:14,557][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:57:14,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:57:14,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:57:14,558][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:57:14,559][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:57:14,559][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:57:14,560][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:57:14,588][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 22:58:04,119][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/6>
[2024-06-20 22:58:04,120][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:58:04,122][HYDRA] 	#367 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:58:04,308][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:58:04,309][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:58:04,310][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:58:04,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:58:04,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:58:04,313][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:58:04,314][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:58:04,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:58:04,317][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:58:04,317][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:58:04,319][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:58:04,384][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.066 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 22:58:54,250][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/7>
[2024-06-20 22:58:54,251][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:58:54,255][HYDRA] 	#368 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:58:54,456][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:58:54,457][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:58:54,458][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:58:54,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:58:54,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:58:54,461][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:58:54,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:58:54,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:58:54,463][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:58:54,463][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:58:54,465][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:58:54,558][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 22:59:43,753][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/8>
[2024-06-20 22:59:43,753][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:59:43,755][HYDRA] 	#369 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:59:43,939][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:59:43,940][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 22:59:43,941][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:59:43,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:59:43,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:59:43,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:59:43,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:59:43,947][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:59:43,948][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:59:43,948][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:59:43,949][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:59:44,015][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-20 23:00:33,375][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.05/9>
[2024-06-20 23:00:33,376][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:00:33,378][HYDRA] 	#370 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:00:33,556][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:00:33,558][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:00:33,559][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:00:33,559][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:00:33,561][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:00:33,562][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:00:33,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:00:33,563][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:00:33,563][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:00:33,563][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:00:33,565][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:00:33,593][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.82it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 23:01:23,237][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/0>
[2024-06-20 23:01:23,238][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:01:23,241][HYDRA] 	#371 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:01:23,437][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:01:23,438][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:01:23,439][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:01:23,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:01:23,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:01:23,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:01:23,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:01:23,455][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:01:23,456][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:01:23,456][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:01:23,457][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:01:23,519][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 23:02:13,718][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/1>
[2024-06-20 23:02:13,719][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:02:13,721][HYDRA] 	#372 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:02:13,906][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:02:13,907][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:02:13,908][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:02:13,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:02:13,911][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:02:13,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:02:13,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:02:13,914][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:02:13,914][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:02:13,914][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:02:13,916][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:02:13,983][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.076 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 23:03:03,056][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/2>
[2024-06-20 23:03:03,056][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:03:03,059][HYDRA] 	#373 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:03:03,232][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:03:03,234][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:03:03,235][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:03:03,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:03:03,237][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:03:03,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:03:03,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:03:03,240][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:03:03,240][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:03:03,240][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:03:03,241][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:03:03,270][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 23:03:52,405][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/3>
[2024-06-20 23:03:52,406][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:03:52,408][HYDRA] 	#374 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:03:52,591][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:03:52,592][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:03:52,593][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:03:52,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:03:52,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:03:52,596][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:03:52,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:03:52,599][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:03:52,600][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:03:52,600][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:03:52,601][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:03:52,671][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.072 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.020             
[2024-06-20 23:04:42,444][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/4>
[2024-06-20 23:04:42,445][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:04:42,447][HYDRA] 	#375 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:04:42,651][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:04:42,652][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:04:42,653][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:04:42,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:04:42,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:04:42,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:04:42,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:04:42,657][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:04:42,658][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:04:42,658][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:04:42,659][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:04:42,689][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 23:05:32,152][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/5>
[2024-06-20 23:05:32,153][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:05:32,155][HYDRA] 	#376 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:05:32,337][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:05:32,338][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:05:32,339][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:05:32,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:05:32,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:05:32,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:05:32,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:05:32,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:05:32,346][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:05:32,346][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:05:32,348][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:05:32,414][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.778 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:06:22,638][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/6>
[2024-06-20 23:06:22,639][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:06:22,642][HYDRA] 	#377 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:06:22,824][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:06:22,826][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:06:22,827][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:06:22,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:06:22,829][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:06:22,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:06:22,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:06:22,830][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:06:22,831][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:06:22,831][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:06:22,832][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:06:22,866][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre:    
                                                              0.071 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 23:07:13,024][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/7>
[2024-06-20 23:07:13,025][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:07:13,027][HYDRA] 	#378 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:07:13,205][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:07:13,207][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:07:13,208][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:07:13,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:07:13,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:07:13,211][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:07:13,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:07:13,214][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:07:13,214][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:07:13,214][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:07:13,216][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:07:13,283][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:08:02,877][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/8>
[2024-06-20 23:08:02,878][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:08:02,881][HYDRA] 	#379 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:08:03,056][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:08:03,057][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:08:03,058][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:08:03,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:08:03,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:08:03,061][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:08:03,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:08:03,062][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:08:03,063][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:08:03,063][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:08:03,064][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:08:03,095][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:08:52,240][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.1/9>
[2024-06-20 23:08:52,240][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:08:52,243][HYDRA] 	#380 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:08:52,422][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:08:52,424][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:08:52,425][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:08:52,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:08:52,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:08:52,429][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:08:52,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:08:52,432][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:08:52,432][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:08:52,433][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:08:52,434][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:08:52,501][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:09:42,262][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/0>
[2024-06-20 23:09:42,263][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:09:42,265][HYDRA] 	#381 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:09:42,442][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:09:42,444][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:09:42,445][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:09:42,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:09:42,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:09:42,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:09:42,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:09:42,449][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:09:42,449][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:09:42,449][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:09:42,451][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:09:42,479][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 23:10:32,305][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/1>
[2024-06-20 23:10:32,306][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:10:32,308][HYDRA] 	#382 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:10:32,489][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:10:32,491][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:10:32,492][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:10:32,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:10:32,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:10:32,495][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:10:32,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:10:32,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:10:32,496][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:10:32,496][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:10:32,498][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:10:32,527][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 23:11:22,589][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/2>
[2024-06-20 23:11:22,590][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:11:22,595][HYDRA] 	#383 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:11:22,817][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:11:22,818][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:11:22,819][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:11:22,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:11:22,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:11:22,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:11:22,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:11:22,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:11:22,827][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:11:22,827][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:11:22,828][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:11:22,912][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:12:12,289][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/3>
[2024-06-20 23:12:12,290][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:12:12,292][HYDRA] 	#384 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:12:12,469][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:12:12,471][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:12:12,472][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:12:12,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:12:12,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:12:12,475][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:12:12,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:12:12,476][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:12:12,476][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:12:12,476][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:12:12,477][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:12:12,508][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.444 val/mre:    
                                                              0.076 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 23:13:03,110][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/4>
[2024-06-20 23:13:03,112][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:13:03,131][HYDRA] 	#385 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:13:03,369][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:13:03,370][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:13:03,371][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:13:03,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:13:03,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:13:03,374][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:13:03,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:13:03,377][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:13:03,379][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:13:03,379][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:13:03,381][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:13:03,450][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 23:13:52,645][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/5>
[2024-06-20 23:13:52,646][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:13:52,648][HYDRA] 	#386 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:13:52,824][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:13:52,826][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:13:52,827][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:13:52,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:13:52,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:13:52,830][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:13:52,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:13:52,831][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:13:52,832][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:13:52,832][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:13:52,833][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:13:52,862][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 23:14:42,791][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/6>
[2024-06-20 23:14:42,791][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:14:42,794][HYDRA] 	#387 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:14:42,974][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:14:42,976][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:14:42,977][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:14:42,977][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:14:42,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:14:42,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:14:42,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:14:42,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:14:42,983][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:14:42,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:14:42,985][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:14:43,054][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 23:15:32,955][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/7>
[2024-06-20 23:15:32,956][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:15:32,958][HYDRA] 	#388 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:15:33,126][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:15:33,127][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:15:33,128][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:15:33,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:15:33,131][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:15:33,131][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:15:33,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:15:33,132][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:15:33,132][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:15:33,133][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:15:33,134][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:15:33,170][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.99it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:16:22,632][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/8>
[2024-06-20 23:16:22,633][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:16:22,635][HYDRA] 	#389 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:16:22,818][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:16:22,819][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:16:22,820][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:16:22,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:16:22,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:16:22,823][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:16:22,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:16:22,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:16:22,827][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:16:22,827][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:16:22,828][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:16:22,895][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:17:12,190][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.15/9>
[2024-06-20 23:17:12,190][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:17:12,193][HYDRA] 	#390 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:17:12,368][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:17:12,370][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:17:12,371][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:17:12,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:17:12,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:17:12,374][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:17:12,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:17:12,378][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:17:12,378][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:17:12,378][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:17:12,380][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:17:12,422][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.89it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-20 23:18:01,777][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/0>
[2024-06-20 23:18:01,778][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:18:01,780][HYDRA] 	#391 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:18:01,965][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:18:01,967][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:18:01,968][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:18:01,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:18:01,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:18:01,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:18:01,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:18:01,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:18:01,974][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:18:01,975][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:18:01,976][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:18:02,044][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:18:52,849][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/1>
[2024-06-20 23:18:52,849][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:18:52,851][HYDRA] 	#392 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:18:53,024][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:18:53,026][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:18:53,027][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:18:53,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:18:53,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:18:53,030][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:18:53,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:18:53,031][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:18:53,032][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:18:53,032][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:18:53,033][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:18:53,063][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-20 23:19:42,565][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/2>
[2024-06-20 23:19:42,565][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:19:42,568][HYDRA] 	#393 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:19:42,751][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:19:42,753][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:19:42,754][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:19:42,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:19:42,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:19:42,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:19:42,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:19:42,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:19:42,758][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:19:42,758][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:19:42,760][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:19:42,794][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.079 train/auc:  
                                                              0.967 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 23:20:32,072][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/3>
[2024-06-20 23:20:32,072][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:20:32,075][HYDRA] 	#394 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:20:32,303][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:20:32,305][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:20:32,306][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:20:32,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:20:32,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:20:32,309][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:20:32,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:20:32,312][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:20:32,312][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:20:32,312][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:20:32,314][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:20:32,384][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.079 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.022             
[2024-06-20 23:21:22,595][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/4>
[2024-06-20 23:21:22,595][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:21:22,598][HYDRA] 	#395 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:21:22,779][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:21:22,780][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:21:22,781][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:21:22,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:21:22,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:21:22,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:21:22,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:21:22,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:21:22,786][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:21:22,786][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:21:22,787][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:21:22,818][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 23:22:13,197][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/5>
[2024-06-20 23:22:13,198][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:22:13,201][HYDRA] 	#396 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:22:13,387][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:22:13,389][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:22:13,390][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:22:13,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:22:13,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:22:13,393][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:22:13,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:22:13,420][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:22:13,420][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:22:13,421][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:22:13,422][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:22:13,516][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-20 23:23:02,901][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/6>
[2024-06-20 23:23:02,902][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:23:02,904][HYDRA] 	#397 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:23:03,080][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:23:03,081][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:23:03,082][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:23:03,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:23:03,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:23:03,085][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:23:03,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:23:03,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:23:03,087][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:23:03,087][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:23:03,088][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:23:03,117][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.081 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.020             
[2024-06-20 23:23:52,770][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/7>
[2024-06-20 23:23:52,771][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:23:52,773][HYDRA] 	#398 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:23:52,957][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:23:52,959][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:23:52,960][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:23:52,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:23:52,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:23:52,966][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:23:52,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:23:52,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:23:52,969][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:23:52,969][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:23:52,971][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:23:53,044][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.90it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.084 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.015             
[2024-06-20 23:24:44,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/8>
[2024-06-20 23:24:44,114][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:24:44,120][HYDRA] 	#399 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:24:44,302][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:24:44,304][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:24:44,305][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:24:44,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:24:44,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:24:44,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:24:44,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:24:44,308][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:24:44,309][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:24:44,309][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:24:44,310][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:24:44,340][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 23:25:33,678][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.2/9>
[2024-06-20 23:25:33,679][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:25:33,681][HYDRA] 	#400 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:25:33,860][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:25:33,862][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:25:33,863][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:25:33,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:25:33,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:25:33,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:25:33,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:25:33,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:25:33,870][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:25:33,870][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:25:33,871][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:25:33,938][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 23:26:23,556][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/0>
[2024-06-20 23:26:23,557][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:26:23,559][HYDRA] 	#401 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:26:23,740][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:26:23,742][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:26:23,743][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:26:23,743][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:26:23,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:26:23,746][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:26:23,746][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:26:23,747][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:26:23,747][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:26:23,748][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:26:23,749][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:26:23,779][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.104 train/auc:  
                                                              0.910 train/f1:   
                                                              0.916             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.021             
[2024-06-20 23:27:13,848][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/1>
[2024-06-20 23:27:13,849][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:27:13,851][HYDRA] 	#402 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:27:14,026][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:27:14,027][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:27:14,028][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:27:14,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:27:14,031][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:27:14,031][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:27:14,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:27:14,032][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:27:14,034][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:27:14,035][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:27:14,036][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:27:14,064][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 23:28:03,262][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/2>
[2024-06-20 23:28:03,262][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:28:03,266][HYDRA] 	#403 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:28:03,453][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:28:03,455][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:28:03,456][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:28:03,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:28:03,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:28:03,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:28:03,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:28:03,462][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:28:03,462][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:28:03,462][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:28:03,464][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:28:03,532][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 23:28:52,737][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/3>
[2024-06-20 23:28:52,737][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:28:52,740][HYDRA] 	#404 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:28:52,916][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:28:52,918][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:28:52,919][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:28:52,919][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:28:52,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:28:52,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:28:52,922][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:28:52,923][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:28:52,923][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:28:52,923][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:28:52,925][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:28:52,955][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-20 23:29:42,557][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/4>
[2024-06-20 23:29:42,557][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:29:42,560][HYDRA] 	#405 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:29:43,106][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:29:43,107][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:29:43,108][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:29:43,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:29:43,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:29:43,111][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:29:43,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:29:43,114][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:29:43,115][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:29:43,115][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:29:43,116][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:29:43,198][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.079 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.015             
[2024-06-20 23:30:32,767][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/5>
[2024-06-20 23:30:32,768][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:30:32,770][HYDRA] 	#406 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:30:32,945][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:30:32,947][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:30:32,948][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:30:32,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:30:32,950][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:30:32,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:30:32,951][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:30:32,952][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:30:32,952][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:30:32,952][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:30:32,954][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:30:32,982][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.082 train/auc:  
                                                              0.926 train/f1:   
                                                              0.922             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.869 train/mre:  
                                                              0.015             
[2024-06-20 23:31:22,703][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/6>
[2024-06-20 23:31:22,704][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:31:22,708][HYDRA] 	#407 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:31:22,901][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:31:22,903][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:31:22,904][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:31:22,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:31:22,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:31:22,907][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:31:22,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:31:22,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:31:22,912][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:31:22,913][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:31:22,914][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:31:23,000][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.084 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-20 23:32:12,041][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/7>
[2024-06-20 23:32:12,042][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:32:12,044][HYDRA] 	#408 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:32:12,220][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:32:12,222][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:32:12,223][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:32:12,223][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:32:12,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:32:12,226][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:32:12,226][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:32:12,227][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:32:12,227][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:32:12,228][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:32:12,229][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:32:12,258][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 23:33:02,072][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/8>
[2024-06-20 23:33:02,072][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:33:02,076][HYDRA] 	#409 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:33:02,260][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:33:02,261][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:33:02,262][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:33:02,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:33:02,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:33:02,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:33:02,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:33:02,268][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:33:02,269][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:33:02,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:33:02,270][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:33:02,338][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.95it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.075 train/auc:  
                                                              0.967 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-20 23:33:51,806][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.25/9>
[2024-06-20 23:33:51,806][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:33:51,809][HYDRA] 	#410 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:33:51,995][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:33:51,996][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:33:51,997][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:33:51,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:33:52,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:33:52,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:33:52,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:33:52,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:33:52,002][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:33:52,002][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:33:52,003][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:33:52,032][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:34:41,572][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/0>
[2024-06-20 23:34:41,573][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:34:41,576][HYDRA] 	#411 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:34:41,777][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:34:41,779][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:34:41,780][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:34:41,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:34:41,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:34:41,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:34:41,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:34:41,787][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:34:41,788][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:34:41,788][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:34:41,789][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:34:41,882][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.94it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-20 23:35:32,095][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/1>
[2024-06-20 23:35:32,095][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:35:32,097][HYDRA] 	#412 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:35:32,310][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:35:32,311][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:35:32,312][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:35:32,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:35:32,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:35:32,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:35:32,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:35:32,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:35:32,318][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:35:32,318][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:35:32,319][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:35:32,366][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:36:21,786][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/2>
[2024-06-20 23:36:21,786][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:36:21,788][HYDRA] 	#413 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:36:21,964][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:36:21,966][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:36:21,967][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:36:21,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:36:21,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:36:21,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:36:21,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:36:21,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:36:21,971][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:36:21,971][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:36:21,973][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:36:22,002][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.080 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.014             
[2024-06-20 23:37:11,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/3>
[2024-06-20 23:37:11,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:37:11,456][HYDRA] 	#414 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:37:11,693][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:37:11,695][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:37:11,696][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:37:11,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:37:11,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:37:11,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:37:11,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:37:11,703][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:37:11,704][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:37:11,704][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:37:11,705][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:37:11,771][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.086 train/auc:  
                                                              0.959 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.924             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:38:00,799][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/4>
[2024-06-20 23:38:00,800][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:38:00,802][HYDRA] 	#415 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:38:00,977][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:38:00,979][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:38:00,980][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:38:00,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:38:00,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:38:00,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:38:00,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:38:00,984][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:38:00,984][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:38:00,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:38:00,986][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:38:01,014][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 23:38:51,116][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/5>
[2024-06-20 23:38:51,117][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:38:51,119][HYDRA] 	#416 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:38:51,305][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:38:51,306][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:38:51,307][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:38:51,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:38:51,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:38:51,310][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:38:51,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:38:51,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:38:51,314][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:38:51,315][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:38:51,316][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:38:51,383][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:39:40,394][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/6>
[2024-06-20 23:39:40,394][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:39:40,396][HYDRA] 	#417 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:39:40,576][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:39:40,578][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:39:40,578][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:39:40,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:39:40,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:39:40,582][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:39:40,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:39:40,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:39:40,583][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:39:40,583][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:39:40,584][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:39:40,624][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.97it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.101 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 23:40:30,121][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/7>
[2024-06-20 23:40:30,122][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:40:30,124][HYDRA] 	#418 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:40:30,307][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:40:30,309][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:40:30,310][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:40:30,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:40:30,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:40:30,313][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:40:30,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:40:30,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:40:30,316][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:40:30,316][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:40:30,318][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:40:30,387][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.081 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.013             
[2024-06-20 23:41:20,441][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/8>
[2024-06-20 23:41:20,442][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:41:20,444][HYDRA] 	#419 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:41:20,622][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:41:20,624][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:41:20,625][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:41:20,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:41:20,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:41:20,627][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:41:20,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:41:20,628][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:41:20,629][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:41:20,629][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:41:20,630][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:41:20,669][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.92it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:42:10,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/proud_life/0.3/9>
[2024-06-20 23:42:10,371][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:42:10,373][HYDRA] 	#420 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:42:10,571][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:42:10,573][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:42:10,574][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:42:10,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:42:10,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:42:10,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:42:10,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:42:10,580][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:42:10,581][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:42:10,581][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:42:10,582][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:42:10,865][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.973  
                                                              train/f1: 0.972   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.015             
[2024-06-20 23:42:58,727][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/0>
[2024-06-20 23:42:58,727][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:42:58,730][HYDRA] 	#421 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:42:58,910][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:42:58,911][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:42:58,912][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:42:58,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:42:58,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:42:58,917][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:42:58,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:42:58,918][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:42:58,919][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:42:58,919][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:42:58,920][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:42:58,950][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.015             
[2024-06-20 23:43:46,467][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/1>
[2024-06-20 23:43:46,467][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:43:46,474][HYDRA] 	#422 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:43:46,699][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:43:46,701][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:43:46,702][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:43:46,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:43:46,704][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:43:46,705][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:43:46,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:43:46,709][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:43:46,710][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:43:46,710][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:43:46,712][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:43:46,802][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-20 23:44:35,927][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/2>
[2024-06-20 23:44:35,928][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:44:35,930][HYDRA] 	#423 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:44:36,109][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:44:36,111][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:44:36,112][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:44:36,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:44:36,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:44:36,115][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:44:36,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:44:36,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:44:36,116][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:44:36,117][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:44:36,118][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:44:36,146][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.19it/s v_num: 0.000      
                                                              val/auc: 0.439    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-20 23:45:23,626][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/3>
[2024-06-20 23:45:23,627][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:45:23,629][HYDRA] 	#424 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:45:23,804][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:45:23,806][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:45:23,807][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:45:23,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:45:23,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:45:23,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:45:23,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:45:23,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:45:23,812][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:45:23,812][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:45:23,813][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:45:23,844][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.21it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-20 23:46:11,550][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/4>
[2024-06-20 23:46:11,551][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:46:11,553][HYDRA] 	#425 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:46:11,735][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:46:11,737][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:46:11,738][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:46:11,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:46:11,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:46:11,742][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:46:11,742][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:46:11,745][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:46:11,745][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:46:11,745][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:46:11,747][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:46:11,811][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-20 23:46:59,665][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/5>
[2024-06-20 23:46:59,666][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:46:59,675][HYDRA] 	#426 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:46:59,852][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:46:59,853][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:46:59,854][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:46:59,855][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:46:59,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:46:59,857][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:46:59,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:46:59,858][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:46:59,859][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:46:59,859][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:46:59,860][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:46:59,888][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.028             
[2024-06-20 23:47:47,710][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/6>
[2024-06-20 23:47:47,711][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:47:47,714][HYDRA] 	#427 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:47:47,898][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:47:47,900][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:47:47,901][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:47:47,901][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:47:47,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:47:47,904][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:47:47,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:47:47,907][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:47:47,907][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:47:47,908][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:47:47,909][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:47:47,975][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.44it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 23:48:35,779][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/7>
[2024-06-20 23:48:35,780][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:48:35,782][HYDRA] 	#428 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:48:35,954][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:48:35,955][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:48:35,956][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:48:35,957][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:48:35,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:48:35,959][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:48:35,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:48:35,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:48:35,960][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:48:35,961][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:48:35,962][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:48:35,990][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.694    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:49:24,178][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/8>
[2024-06-20 23:49:24,179][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:49:24,181][HYDRA] 	#429 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:49:24,361][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:49:24,363][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:49:24,364][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:49:24,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:49:24,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:49:24,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:49:24,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:49:24,370][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:49:24,371][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:49:24,371][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:49:24,372][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:49:24,439][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.027             
[2024-06-20 23:50:13,434][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.0/9>
[2024-06-20 23:50:13,436][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:50:13,456][HYDRA] 	#430 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:50:13,841][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:50:13,843][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:50:13,844][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:50:13,844][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:50:13,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:50:13,850][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:50:13,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:50:13,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:50:13,853][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:50:13,853][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:50:13,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:50:13,944][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.500 val/mre:    
                                                              0.070 train/auc:  
                                                              0.973 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.024             
[2024-06-20 23:51:01,959][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/0>
[2024-06-20 23:51:01,960][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:51:01,963][HYDRA] 	#431 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:51:02,145][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:51:02,147][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:51:02,148][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:51:02,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:51:02,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:51:02,151][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:51:02,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:51:02,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:51:02,154][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:51:02,155][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:51:02,156][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:51:02,223][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.076 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:51:50,187][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/1>
[2024-06-20 23:51:50,188][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:51:50,190][HYDRA] 	#432 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:51:50,370][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:51:50,371][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:51:50,372][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:51:50,372][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:51:50,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:51:50,375][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:51:50,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:51:50,376][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:51:50,377][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:51:50,377][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:51:50,378][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:51:50,408][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.10it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:52:38,206][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/2>
[2024-06-20 23:52:38,207][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:52:38,209][HYDRA] 	#433 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:52:38,785][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:52:38,787][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:52:38,788][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:52:38,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:52:38,790][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:52:38,791][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:52:38,791][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:52:38,792][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:52:38,792][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:52:38,793][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:52:38,794][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:52:38,823][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.20it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-20 23:53:26,268][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/3>
[2024-06-20 23:53:26,268][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:53:26,271][HYDRA] 	#434 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:53:26,455][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:53:26,456][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:53:26,457][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:53:26,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:53:26,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:53:26,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:53:26,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:53:26,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:53:26,464][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:53:26,465][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:53:26,466][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:53:26,534][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.10it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-20 23:54:13,549][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/4>
[2024-06-20 23:54:13,550][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:54:13,553][HYDRA] 	#435 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:54:13,730][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:54:13,731][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:54:13,732][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:54:13,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:54:13,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:54:13,735][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:54:13,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:54:13,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:54:13,737][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:54:13,737][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:54:13,738][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:54:13,768][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.27it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.028             
[2024-06-20 23:55:01,408][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/5>
[2024-06-20 23:55:01,409][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:55:01,411][HYDRA] 	#436 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:55:01,594][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:55:01,595][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:55:01,596][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:55:01,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:55:01,599][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:55:01,599][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:55:01,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:55:01,603][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:55:01,603][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:55:01,603][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:55:01,605][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:55:01,674][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.073 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.020             
[2024-06-20 23:55:49,861][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/6>
[2024-06-20 23:55:49,861][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:55:49,863][HYDRA] 	#437 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:55:50,042][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:55:50,043][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:55:50,044][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:55:50,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:55:50,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:55:50,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:55:50,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:55:50,049][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:55:50,049][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:55:50,049][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:55:50,051][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:55:50,131][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.071 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.016             
[2024-06-20 23:56:38,278][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/7>
[2024-06-20 23:56:38,279][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:56:38,282][HYDRA] 	#438 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:56:38,468][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:56:38,470][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:56:38,471][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:56:38,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:56:38,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:56:38,474][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:56:38,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:56:38,479][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:56:38,479][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:56:38,479][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:56:38,480][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:56:38,561][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.03it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-20 23:57:26,159][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/8>
[2024-06-20 23:57:26,160][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:57:26,162][HYDRA] 	#439 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:57:26,336][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:57:26,338][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:57:26,339][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:57:26,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:57:26,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:57:26,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:57:26,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:57:26,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:57:26,343][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:57:26,344][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:57:26,345][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:57:26,373][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-20 23:58:13,519][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.05/9>
[2024-06-20 23:58:13,519][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:58:13,522][HYDRA] 	#440 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:58:13,700][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:58:13,702][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:58:13,703][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:58:13,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:58:13,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:58:13,706][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:58:13,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:58:13,707][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:58:13,708][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:58:13,708][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:58:13,709][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:58:13,738][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-20 23:59:02,509][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/0>
[2024-06-20 23:59:02,510][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:59:02,512][HYDRA] 	#441 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:59:02,693][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:59:02,694][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:59:02,695][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:59:02,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:59:02,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:59:02,698][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:59:02,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:59:02,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:59:02,702][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:59:02,702][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:59:02,704][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:59:02,775][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.21it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.089 train/auc:  
                                                              0.973 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.030             
[2024-06-20 23:59:50,526][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/1>
[2024-06-20 23:59:50,526][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:59:50,529][HYDRA] 	#442 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:59:50,709][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:59:50,711][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-20 23:59:50,712][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:59:50,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:59:50,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:59:50,715][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:59:50,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:59:50,716][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:59:50,717][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:59:50,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:59:50,718][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:59:50,747][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.21it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.700 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.032             
[2024-06-21 00:00:39,365][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/2>
[2024-06-21 00:00:39,365][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:00:39,368][HYDRA] 	#443 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:00:39,552][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:00:39,554][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:00:39,555][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:00:39,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:00:39,557][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:00:39,558][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:00:39,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:00:39,561][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:00:39,562][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:00:39,562][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:00:39,563][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:00:39,636][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.44it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-21 00:01:27,647][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/3>
[2024-06-21 00:01:27,647][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:01:27,654][HYDRA] 	#444 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:01:27,846][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:01:27,847][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:01:27,848][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:01:27,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:01:27,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:01:27,851][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:01:27,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:01:27,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:01:27,852][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:01:27,852][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:01:27,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:01:27,882][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.26it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-21 00:02:18,046][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/4>
[2024-06-21 00:02:18,050][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:02:18,070][HYDRA] 	#445 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:02:18,602][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:02:18,604][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:02:18,605][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:02:18,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:02:18,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:02:18,611][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:02:18,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:02:18,623][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:02:18,623][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:02:18,623][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:02:18,625][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:02:19,281][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.20it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.032             
[2024-06-21 00:03:08,843][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/5>
[2024-06-21 00:03:08,844][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:03:08,846][HYDRA] 	#446 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:03:09,020][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:03:09,021][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:03:09,022][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:03:09,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:03:09,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:03:09,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:03:09,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:03:09,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:03:09,027][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:03:09,028][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:03:09,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:03:09,059][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.21it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-21 00:03:57,501][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/6>
[2024-06-21 00:03:57,502][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:03:57,505][HYDRA] 	#447 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:03:57,687][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:03:57,689][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:03:57,690][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:03:57,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:03:57,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:03:57,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:03:57,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:03:57,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:03:57,713][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:03:57,713][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:03:57,715][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:03:57,787][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.072 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.012             
[2024-06-21 00:04:46,444][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/7>
[2024-06-21 00:04:46,445][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:04:46,447][HYDRA] 	#448 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:04:46,623][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:04:46,625][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:04:46,626][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:04:46,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:04:46,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:04:46,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:04:46,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:04:46,630][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:04:46,630][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:04:46,631][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:04:46,632][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:04:46,663][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.078 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-06-21 00:05:34,400][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/8>
[2024-06-21 00:05:34,408][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:05:34,432][HYDRA] 	#449 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:05:34,613][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:05:34,614][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:05:34,615][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:05:34,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:05:34,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:05:34,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:05:34,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:05:34,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:05:34,620][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:05:34,620][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:05:34,621][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:05:34,649][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-21 00:06:22,269][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.1/9>
[2024-06-21 00:06:22,269][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:06:22,272][HYDRA] 	#450 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:06:22,450][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:06:22,452][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:06:22,453][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:06:22,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:06:22,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:06:22,456][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:06:22,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:06:22,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:06:22,460][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:06:22,460][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:06:22,461][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:06:22,525][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.22it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-21 00:07:10,315][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/0>
[2024-06-21 00:07:10,315][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:07:10,318][HYDRA] 	#451 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:07:10,491][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:07:10,492][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:07:10,493][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:07:10,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:07:10,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:07:10,498][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:07:10,498][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:07:10,499][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:07:10,500][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:07:10,500][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:07:10,501][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:07:10,531][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.19it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-21 00:07:58,982][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/1>
[2024-06-21 00:07:58,982][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:07:58,985][HYDRA] 	#452 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:07:59,168][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:07:59,170][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:07:59,171][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:07:59,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:07:59,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:07:59,174][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:07:59,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:07:59,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:07:59,177][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:07:59,177][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:07:59,178][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:07:59,245][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.34it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.070 train/auc:  
                                                              0.875 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.920             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.011             
[2024-06-21 00:08:46,788][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/2>
[2024-06-21 00:08:46,789][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:08:46,791][HYDRA] 	#453 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:08:46,962][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:08:46,963][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:08:46,964][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:08:46,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:08:46,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:08:46,967][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:08:46,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:08:46,968][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:08:46,968][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:08:46,968][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:08:46,970][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:08:47,000][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-06-21 00:09:34,645][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/3>
[2024-06-21 00:09:34,645][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:09:34,648][HYDRA] 	#454 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:09:34,829][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:09:34,830][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:09:34,831][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:09:34,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:09:34,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:09:34,835][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:09:34,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:09:34,838][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:09:34,839][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:09:34,839][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:09:34,840][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:09:34,913][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.19it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.077 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.020             
[2024-06-21 00:10:22,828][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/4>
[2024-06-21 00:10:22,828][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:10:22,831][HYDRA] 	#455 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:10:23,004][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:10:23,005][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:10:23,006][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:10:23,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:10:23,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:10:23,009][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:10:23,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:10:23,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:10:23,011][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:10:23,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:10:23,012][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:10:23,041][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.23it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-21 00:11:11,059][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/5>
[2024-06-21 00:11:11,059][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:11:11,063][HYDRA] 	#456 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:11:11,249][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:11:11,251][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:11:11,252][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:11:11,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:11:11,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:11:11,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:11:11,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:11:11,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:11:11,258][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:11:11,258][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:11:11,260][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:11:11,327][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.20it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-21 00:11:59,383][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/6>
[2024-06-21 00:11:59,384][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:11:59,387][HYDRA] 	#457 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:11:59,567][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:11:59,568][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:11:59,569][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:11:59,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:11:59,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:11:59,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:11:59,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:11:59,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:11:59,574][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:11:59,574][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:11:59,575][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:11:59,605][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.439    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.102 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-06-21 00:12:47,175][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/7>
[2024-06-21 00:12:47,176][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:12:47,178][HYDRA] 	#458 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:12:47,353][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:12:47,355][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:12:47,356][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:12:47,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:12:47,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:12:47,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:12:47,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:12:47,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:12:47,362][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:12:47,362][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:12:47,364][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:12:47,393][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-06-21 00:13:35,397][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/8>
[2024-06-21 00:13:35,398][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:13:35,400][HYDRA] 	#459 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:13:35,582][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:13:35,584][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:13:35,584][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:13:35,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:13:35,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:13:35,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:13:35,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:13:35,591][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:13:35,591][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:13:35,591][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:13:35,593][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:13:35,662][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.093 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.039             
[2024-06-21 00:14:23,349][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.15/9>
[2024-06-21 00:14:23,350][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:14:23,352][HYDRA] 	#460 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:14:23,533][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:14:23,535][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:14:23,536][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:14:23,536][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:14:23,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:14:23,539][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:14:23,539][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:14:23,540][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:14:23,540][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:14:23,541][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:14:23,542][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:14:23,571][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.20it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-21 00:15:11,939][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/0>
[2024-06-21 00:15:11,939][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:15:11,942][HYDRA] 	#461 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:15:12,123][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:15:12,125][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:15:12,126][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:15:12,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:15:12,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:15:12,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:15:12,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:15:12,132][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:15:12,132][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:15:12,132][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:15:12,134][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:15:12,237][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.100 train/auc:  
                                                              0.964 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-21 00:15:59,929][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/1>
[2024-06-21 00:15:59,930][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:15:59,933][HYDRA] 	#462 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:16:00,110][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:16:00,111][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:16:00,112][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:16:00,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:16:00,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:16:00,115][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:16:00,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:16:00,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:16:00,117][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:16:00,117][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:16:00,118][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:16:00,146][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 00:16:50,561][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/2>
[2024-06-21 00:16:50,562][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:16:50,564][HYDRA] 	#463 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:16:50,747][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:16:50,749][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:16:50,750][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:16:50,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:16:50,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:16:50,753][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:16:50,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:16:50,756][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:16:50,756][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:16:50,756][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:16:50,758][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:16:50,824][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.544    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 00:17:39,026][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/3>
[2024-06-21 00:17:39,027][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:17:39,029][HYDRA] 	#464 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:17:39,206][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:17:39,207][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:17:39,208][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:17:39,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:17:39,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:17:39,211][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:17:39,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:17:39,212][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:17:39,213][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:17:39,213][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:17:39,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:17:39,244][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.086 train/auc:  
                                                              0.955 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-21 00:18:27,158][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/4>
[2024-06-21 00:18:27,158][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:18:27,161][HYDRA] 	#465 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:18:27,335][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:18:27,337][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:18:27,338][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:18:27,338][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:18:27,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:18:27,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:18:27,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:18:27,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:18:27,343][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:18:27,343][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:18:27,344][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:18:27,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-21 00:19:15,789][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/5>
[2024-06-21 00:19:15,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:19:15,792][HYDRA] 	#466 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:19:15,990][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:19:15,991][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:19:15,992][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:19:15,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:19:15,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:19:15,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:19:15,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:19:16,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:19:16,000][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:19:16,000][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:19:16,001][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:19:16,074][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.20it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-21 00:20:05,316][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/6>
[2024-06-21 00:20:05,316][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:20:05,320][HYDRA] 	#467 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:20:05,533][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:20:05,534][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:20:05,535][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:20:05,536][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:20:05,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:20:05,538][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:20:05,539][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:20:05,539][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:20:05,540][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:20:05,540][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:20:05,541][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:20:05,581][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.544    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-06-21 00:20:54,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/7>
[2024-06-21 00:20:54,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:20:54,490][HYDRA] 	#468 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:20:54,672][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:20:54,674][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:20:54,675][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:20:54,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:20:54,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:20:54,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:20:54,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:20:54,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:20:54,682][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:20:54,682][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:20:54,683][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:20:54,754][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.24it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.095 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-21 00:21:42,946][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/8>
[2024-06-21 00:21:42,949][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:21:42,967][HYDRA] 	#469 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:21:43,236][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:21:43,238][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:21:43,239][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:21:43,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:21:43,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:21:43,242][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:21:43,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:21:43,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:21:43,243][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:21:43,244][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:21:43,245][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:21:43,275][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.694    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-21 00:22:31,648][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.2/9>
[2024-06-21 00:22:31,648][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:22:31,651][HYDRA] 	#470 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:22:31,851][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:22:31,853][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:22:31,854][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:22:31,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:22:31,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:22:31,857][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:22:31,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:22:31,860][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:22:31,860][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:22:31,860][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:22:31,862][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:22:31,928][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.107 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.032             
[2024-06-21 00:23:20,730][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/0>
[2024-06-21 00:23:20,731][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:23:20,733][HYDRA] 	#471 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:23:20,913][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:23:20,915][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:23:20,916][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:23:20,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:23:20,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:23:20,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:23:20,919][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:23:20,920][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:23:20,921][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:23:20,921][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:23:20,922][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:23:20,952][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 00:24:09,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/1>
[2024-06-21 00:24:09,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:24:09,180][HYDRA] 	#472 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:24:09,360][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:24:09,362][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:24:09,363][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:24:09,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:24:09,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:24:09,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:24:09,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:24:09,369][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:24:09,369][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:24:09,369][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:24:09,371][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:24:09,436][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.098 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-21 00:24:58,194][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/2>
[2024-06-21 00:24:58,194][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:24:58,197][HYDRA] 	#473 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:24:58,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:24:58,387][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:24:58,388][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:24:58,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:24:58,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:24:58,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:24:58,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:24:58,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:24:58,395][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:24:58,395][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:24:58,397][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:24:58,432][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.13it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-06-21 00:25:46,820][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/3>
[2024-06-21 00:25:46,821][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:25:46,823][HYDRA] 	#474 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:25:46,997][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:25:46,998][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:25:46,999][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:25:47,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:25:47,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:25:47,002][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:25:47,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:25:47,003][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:25:47,004][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:25:47,004][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:25:47,005][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:25:47,034][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.19it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-21 00:26:35,190][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/4>
[2024-06-21 00:26:35,190][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:26:35,193][HYDRA] 	#475 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:26:35,376][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:26:35,378][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:26:35,379][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:26:35,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:26:35,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:26:35,382][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:26:35,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:26:35,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:26:35,385][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:26:35,386][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:26:35,387][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:26:35,455][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.769     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              1.000 val/mre:    
                                                              0.086 train/auc:  
                                                              0.973 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.016             
[2024-06-21 00:27:23,032][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/5>
[2024-06-21 00:27:23,033][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:27:23,035][HYDRA] 	#476 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:27:23,222][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:27:23,224][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:27:23,225][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:27:23,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:27:23,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:27:23,228][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:27:23,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:27:23,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:27:23,229][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:27:23,229][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:27:23,231][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:27:23,258][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.12it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.084 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.016             
[2024-06-21 00:28:11,303][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/6>
[2024-06-21 00:28:11,304][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:28:11,306][HYDRA] 	#477 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:28:11,491][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:28:11,493][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:28:11,494][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:28:11,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:28:11,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:28:11,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:28:11,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:28:11,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:28:11,501][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:28:11,501][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:28:11,502][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:28:11,569][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.35it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.640     
                                                              val/precision:    
                                                              0.533 val/recall: 
                                                              0.800 val/mre:    
                                                              0.089 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-21 00:29:01,386][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/7>
[2024-06-21 00:29:01,387][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:29:01,389][HYDRA] 	#478 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:29:01,564][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:29:01,566][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:29:01,567][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:29:01,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:29:01,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:29:01,570][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:29:01,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:29:01,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:29:01,571][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:29:01,571][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:29:01,573][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:29:01,602][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-21 00:29:49,314][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/8>
[2024-06-21 00:29:49,315][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:29:49,317][HYDRA] 	#479 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:29:49,500][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:29:49,502][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:29:49,503][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:29:49,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:29:49,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:29:49,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:29:49,506][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:29:49,509][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:29:49,509][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:29:49,510][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:29:49,511][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:29:49,582][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.094 train/auc:  
                                                              0.964 train/f1:   
                                                              0.963             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.028             
[2024-06-21 00:30:38,128][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.25/9>
[2024-06-21 00:30:38,129][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:30:38,131][HYDRA] 	#480 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:30:38,312][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:30:38,314][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:30:38,315][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:30:38,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:30:38,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:30:38,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:30:38,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:30:38,319][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:30:38,319][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:30:38,320][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:30:38,321][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:30:38,350][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.13it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.099 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.027             
[2024-06-21 00:31:26,819][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/0>
[2024-06-21 00:31:26,819][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:31:26,822][HYDRA] 	#481 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:31:27,022][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:31:27,023][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:31:27,024][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:31:27,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:31:27,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:31:27,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:31:27,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:31:27,032][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:31:27,032][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:31:27,032][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:31:27,034][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:31:27,132][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.085 train/auc:  
                                                              0.902 train/f1:   
                                                              0.911             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-21 00:32:16,524][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/1>
[2024-06-21 00:32:16,525][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:32:16,527][HYDRA] 	#482 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:32:16,702][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:32:16,704][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:32:16,705][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:32:16,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:32:16,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:32:16,708][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:32:16,708][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:32:16,709][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:32:16,709][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:32:16,710][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:32:16,711][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:32:16,777][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.23it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.091 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.016             
[2024-06-21 00:33:05,070][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/2>
[2024-06-21 00:33:05,071][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:33:05,075][HYDRA] 	#483 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:33:05,254][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:33:05,255][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:33:05,256][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:33:05,256][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:33:05,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:33:05,261][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:33:05,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:33:05,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:33:05,262][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:33:05,262][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:33:05,264][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:33:05,291][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.17it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.088 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.015             
[2024-06-21 00:33:53,300][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/3>
[2024-06-21 00:33:53,306][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:33:53,316][HYDRA] 	#484 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:33:53,507][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:33:53,508][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:33:53,509][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:33:53,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:33:53,512][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:33:53,513][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:33:53,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:33:53,516][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:33:53,516][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:33:53,516][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:33:53,518][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:33:53,585][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-06-21 00:34:41,826][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/4>
[2024-06-21 00:34:41,827][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:34:41,830][HYDRA] 	#485 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:34:42,005][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:34:42,006][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:34:42,007][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:34:42,008][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:34:42,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:34:42,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:34:42,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:34:42,011][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:34:42,012][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:34:42,012][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:34:42,013][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:34:42,064][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.095 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-21 00:35:30,499][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/5>
[2024-06-21 00:35:30,500][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:35:30,502][HYDRA] 	#486 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:35:30,692][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:35:30,693][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:35:30,694][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:35:30,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:35:30,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:35:30,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:35:30,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:35:30,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:35:30,701][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:35:30,701][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:35:30,702][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:35:30,770][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.18it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.600 val/mre:    
                                                              0.086 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.021             
[2024-06-21 00:36:18,749][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/6>
[2024-06-21 00:36:18,750][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:36:18,752][HYDRA] 	#487 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:36:18,928][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:36:18,929][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:36:18,930][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:36:18,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:36:18,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:36:18,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:36:18,934][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:36:18,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:36:18,935][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:36:18,935][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:36:18,936][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:36:18,965][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.094 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 00:37:07,119][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/7>
[2024-06-21 00:37:07,119][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:37:07,122][HYDRA] 	#488 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:37:07,303][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:37:07,305][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:37:07,306][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:37:07,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:37:07,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:37:07,309][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:37:07,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:37:07,312][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:37:07,312][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:37:07,313][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:37:07,314][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:37:07,380][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.93it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.028             
[2024-06-21 00:37:55,227][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/8>
[2024-06-21 00:37:55,228][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:37:55,230][HYDRA] 	#489 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:37:55,407][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:37:55,408][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:37:55,409][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:37:55,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:37:55,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:37:55,412][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:37:55,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:37:55,413][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:37:55,414][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:37:55,414][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:37:55,415][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:37:55,445][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 00:38:43,272][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/study_school/0.3/9>
[2024-06-21 00:38:43,272][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:38:43,274][HYDRA] 	#490 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:38:43,451][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:38:43,453][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:38:43,454][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:38:43,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:38:43,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:38:43,457][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:38:43,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:38:43,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:38:43,458][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:38:43,458][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:38:43,460][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:38:43,493][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.05it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-21 00:39:31,991][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/0>
[2024-06-21 00:39:31,992][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:39:31,994][HYDRA] 	#491 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:39:32,177][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:39:32,179][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:39:32,180][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:39:32,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:39:32,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:39:32,183][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:39:32,184][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:39:32,187][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:39:32,187][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:39:32,188][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:39:32,189][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:39:32,259][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.036             
[2024-06-21 00:40:20,596][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/1>
[2024-06-21 00:40:20,597][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:40:20,599][HYDRA] 	#492 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:40:20,769][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:40:20,770][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:40:20,771][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:40:20,771][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:40:20,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:40:20,775][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:40:20,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:40:20,776][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:40:20,776][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:40:20,776][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:40:20,777][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:40:20,805][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-21 00:41:09,210][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/2>
[2024-06-21 00:41:09,211][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:41:09,213][HYDRA] 	#493 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:41:09,404][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:41:09,406][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:41:09,407][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:41:09,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:41:09,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:41:09,409][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:41:09,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:41:09,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:41:09,415][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:41:09,415][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:41:09,416][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:41:09,525][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-06-21 00:41:59,080][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/3>
[2024-06-21 00:41:59,080][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:41:59,084][HYDRA] 	#494 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:41:59,276][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:41:59,277][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:41:59,278][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:41:59,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:41:59,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:41:59,281][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:41:59,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:41:59,282][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:41:59,283][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:41:59,283][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:41:59,284][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:41:59,314][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-21 00:42:47,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/4>
[2024-06-21 00:42:47,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:42:47,494][HYDRA] 	#495 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:42:47,670][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:42:47,671][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:42:47,672][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:42:47,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:42:47,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:42:47,675][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:42:47,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:42:47,676][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:42:47,677][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:42:47,677][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:42:47,678][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:42:47,708][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.05it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 00:43:36,349][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/5>
[2024-06-21 00:43:36,349][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:43:36,352][HYDRA] 	#496 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:43:36,535][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:43:36,537][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:43:36,537][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:43:36,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:43:36,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:43:36,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:43:36,541][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:43:36,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:43:36,544][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:43:36,544][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:43:36,546][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:43:36,614][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.974  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.026             
[2024-06-21 00:44:24,765][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/6>
[2024-06-21 00:44:24,766][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:44:24,769][HYDRA] 	#497 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:44:24,942][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:44:24,944][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:44:24,945][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:44:24,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:44:24,947][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:44:24,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:44:24,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:44:24,949][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:44:24,949][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:44:24,949][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:44:24,951][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:44:24,980][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.05it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-06-21 00:45:14,335][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/7>
[2024-06-21 00:45:14,335][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:45:14,337][HYDRA] 	#498 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:45:14,522][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:45:14,524][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:45:14,525][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:45:14,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:45:14,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:45:14,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:45:14,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:45:14,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:45:14,531][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:45:14,532][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:45:14,533][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:45:14,601][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.24it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 00:46:03,248][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/8>
[2024-06-21 00:46:03,249][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:46:03,252][HYDRA] 	#499 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:46:03,452][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:46:03,454][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:46:03,455][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:46:03,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:46:03,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:46:03,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:46:03,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:46:03,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:46:03,460][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:46:03,460][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:46:03,461][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:46:03,506][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.05it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 00:46:52,305][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.0/9>
[2024-06-21 00:46:52,305][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:46:52,307][HYDRA] 	#500 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:46:52,488][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:46:52,489][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:46:52,490][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:46:52,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:46:52,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:46:52,494][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:46:52,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:46:52,498][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:46:52,498][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:46:52,498][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:46:52,499][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:46:52,566][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.044 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 00:47:41,312][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/0>
[2024-06-21 00:47:41,312][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:47:41,315][HYDRA] 	#501 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:47:41,512][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:47:41,514][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:47:41,515][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:47:41,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:47:41,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:47:41,518][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:47:41,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:47:41,519][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:47:41,519][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:47:41,520][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:47:41,521][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:47:41,554][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.053 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-21 00:48:29,703][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/1>
[2024-06-21 00:48:29,704][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:48:29,706][HYDRA] 	#502 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:48:29,880][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:48:29,882][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:48:29,883][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:48:29,883][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:48:29,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:48:29,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:48:29,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:48:29,887][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:48:29,888][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:48:29,888][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:48:29,889][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:48:29,918][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-21 00:49:18,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/2>
[2024-06-21 00:49:18,651][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:49:18,653][HYDRA] 	#503 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:49:18,835][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:49:18,837][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:49:18,838][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:49:18,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:49:18,841][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:49:18,841][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:49:18,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:49:18,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:49:18,845][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:49:18,845][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:49:18,846][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:49:18,914][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.12it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.053 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-21 00:50:07,043][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/3>
[2024-06-21 00:50:07,043][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:50:07,046][HYDRA] 	#504 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:50:07,236][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:50:07,237][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:50:07,238][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:50:07,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:50:07,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:50:07,242][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:50:07,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:50:07,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:50:07,243][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:50:07,244][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:50:07,245][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:50:07,288][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.03it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 00:50:56,922][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/4>
[2024-06-21 00:50:56,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:50:56,925][HYDRA] 	#505 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:50:57,109][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:50:57,110][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:50:57,111][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:50:57,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:50:57,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:50:57,114][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:50:57,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:50:57,118][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:50:57,118][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:50:57,119][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:50:57,120][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:50:57,229][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.052 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-21 00:51:46,920][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/5>
[2024-06-21 00:51:46,920][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:51:46,923][HYDRA] 	#506 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:51:47,100][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:51:47,101][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:51:47,102][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:51:47,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:51:47,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:51:47,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:51:47,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:51:47,106][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:51:47,106][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:51:47,107][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:51:47,108][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:51:47,136][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-21 00:52:35,448][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/6>
[2024-06-21 00:52:35,448][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:52:35,451][HYDRA] 	#507 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:52:35,633][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:52:35,635][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:52:35,636][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:52:35,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:52:35,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:52:35,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:52:35,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:52:35,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:52:35,642][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:52:35,643][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:52:35,644][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:52:35,710][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.052 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-21 00:53:24,858][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/7>
[2024-06-21 00:53:24,858][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:53:24,860][HYDRA] 	#508 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:53:25,084][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:53:25,085][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:53:25,086][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:53:25,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:53:25,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:53:25,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:53:25,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:53:25,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:53:25,091][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:53:25,092][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:53:25,093][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:53:25,144][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.98it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.051 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 00:54:14,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/8>
[2024-06-21 00:54:14,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:54:14,039][HYDRA] 	#509 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:54:14,219][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:54:14,220][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:54:14,221][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:54:14,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:54:14,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:54:14,224][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:54:14,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:54:14,225][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:54:14,226][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:54:14,226][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:54:14,227][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:54:14,256][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.054 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-06-21 00:55:02,797][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.05/9>
[2024-06-21 00:55:02,797][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:55:02,799][HYDRA] 	#510 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:55:02,992][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:55:02,994][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:55:02,995][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:55:02,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:55:02,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:55:02,999][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:55:03,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:55:03,002][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:55:03,003][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:55:03,003][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:55:03,004][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:55:03,114][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.29it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.027             
[2024-06-21 00:55:51,307][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/0>
[2024-06-21 00:55:51,308][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:55:51,310][HYDRA] 	#511 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:55:51,481][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:55:51,482][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:55:51,483][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:55:51,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:55:51,486][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:55:51,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:55:51,486][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:55:51,487][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:55:51,487][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:55:51,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:55:51,489][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:55:51,517][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.13it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.046 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 00:56:40,269][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/1>
[2024-06-21 00:56:40,270][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:56:40,273][HYDRA] 	#512 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:56:40,455][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:56:40,457][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:56:40,458][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:56:40,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:56:40,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:56:40,461][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:56:40,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:56:40,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:56:40,464][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:56:40,464][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:56:40,466][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:56:40,536][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.05it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.051 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-21 00:57:29,985][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/2>
[2024-06-21 00:57:29,985][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:57:29,987][HYDRA] 	#513 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:57:30,165][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:57:30,166][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:57:30,167][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:57:30,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:57:30,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:57:30,170][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:57:30,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:57:30,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:57:30,172][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:57:30,172][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:57:30,173][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:57:30,202][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-06-21 00:58:18,381][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/3>
[2024-06-21 00:58:18,382][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:58:18,384][HYDRA] 	#514 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:58:18,564][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:58:18,566][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:58:18,567][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:58:18,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:58:18,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:58:18,570][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:58:18,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:58:18,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:58:18,573][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:58:18,574][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:58:18,575][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:58:18,645][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.03it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.050 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-21 00:59:07,242][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/4>
[2024-06-21 00:59:07,242][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:59:07,245][HYDRA] 	#515 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:59:07,425][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:59:07,427][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:59:07,428][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:59:07,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:59:07,431][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:59:07,431][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:59:07,431][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:59:07,432][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:59:07,433][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:59:07,433][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:59:07,434][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:59:07,464][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.02it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.048 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 00:59:55,361][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/5>
[2024-06-21 00:59:55,362][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:59:55,365][HYDRA] 	#516 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:59:55,539][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:59:55,541][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 00:59:55,542][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:59:55,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:59:55,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:59:55,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:59:55,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:59:55,546][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:59:55,546][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:59:55,546][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:59:55,547][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:59:55,576][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.02it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-06-21 01:00:46,070][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/6>
[2024-06-21 01:00:46,071][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:00:46,073][HYDRA] 	#517 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:00:46,289][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:00:46,291][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:00:46,292][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:00:46,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:00:46,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:00:46,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:00:46,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:00:46,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:00:46,299][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:00:46,299][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:00:46,300][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:00:46,373][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.28it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.048 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 01:01:35,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/7>
[2024-06-21 01:01:35,650][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:01:35,657][HYDRA] 	#518 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:01:35,851][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:01:35,852][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:01:35,853][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:01:35,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:01:35,855][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:01:35,856][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:01:35,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:01:35,857][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:01:35,857][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:01:35,858][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:01:35,859][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:01:35,894][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.03it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.051 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-06-21 01:02:24,956][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/8>
[2024-06-21 01:02:24,957][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:02:24,966][HYDRA] 	#519 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:02:25,169][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:02:25,171][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:02:25,172][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:02:25,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:02:25,175][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:02:25,175][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:02:25,176][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:02:25,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:02:25,179][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:02:25,179][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:02:25,180][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:02:25,250][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-21 01:03:13,660][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.1/9>
[2024-06-21 01:03:13,661][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:03:13,663][HYDRA] 	#520 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 01:03:13,839][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:03:13,841][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:03:13,842][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:03:13,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:03:13,844][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:03:13,845][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:03:13,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:03:13,846][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:03:13,847][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:03:13,847][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:03:13,848][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:03:13,877][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.052 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 01:04:02,567][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/0>
[2024-06-21 01:04:02,568][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:04:02,570][HYDRA] 	#521 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 01:04:02,744][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:04:02,746][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:04:02,747][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:04:02,747][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:04:02,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:04:02,750][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:04:02,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:04:02,751][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:04:02,751][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:04:02,751][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:04:02,753][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:04:02,784][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.03it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-21 01:04:51,597][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/1>
[2024-06-21 01:04:51,597][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:04:51,599][HYDRA] 	#522 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 01:04:51,783][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:04:51,785][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:04:51,786][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:04:51,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:04:51,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:04:51,789][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:04:51,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:04:51,792][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:04:51,793][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:04:51,793][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:04:51,794][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:04:51,862][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-06-21 01:05:39,577][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/2>
[2024-06-21 01:05:39,578][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:05:39,580][HYDRA] 	#523 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 01:05:39,757][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:05:39,758][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:05:39,759][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:05:39,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:05:39,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:05:39,762][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:05:39,763][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:05:39,763][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:05:39,764][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:05:39,764][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:05:39,765][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:05:39,794][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.051 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-21 01:06:28,170][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/3>
[2024-06-21 01:06:28,171][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:06:28,177][HYDRA] 	#524 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:06:28,388][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:06:28,390][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:06:28,391][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:06:28,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:06:28,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:06:28,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:06:28,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:06:28,399][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:06:28,400][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:06:28,400][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:06:28,401][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:06:28,492][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.00it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.051 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 01:07:17,677][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/4>
[2024-06-21 01:07:17,678][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:07:17,680][HYDRA] 	#525 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 01:07:17,855][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:07:17,856][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:07:17,857][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:07:17,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:07:17,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:07:17,860][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:07:17,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:07:17,862][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:07:17,862][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:07:17,862][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:07:17,864][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:07:17,893][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-06-21 01:08:06,078][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/5>
[2024-06-21 01:08:06,079][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:08:06,081][HYDRA] 	#526 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 01:08:06,264][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:08:06,265][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:08:06,266][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:08:06,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:08:06,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:08:06,270][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:08:06,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:08:06,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:08:06,273][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:08:06,273][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:08:06,275][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:08:06,375][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-06-21 01:08:54,876][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/6>
[2024-06-21 01:08:54,877][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:08:54,882][HYDRA] 	#527 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:08:55,088][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:08:55,089][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:08:55,090][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:08:55,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:08:55,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:08:55,101][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:08:55,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:08:55,103][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:08:55,103][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:08:55,103][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:08:55,104][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:08:55,155][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.050 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 01:09:43,103][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/7>
[2024-06-21 01:09:43,103][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:09:43,106][HYDRA] 	#528 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:09:43,281][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:09:43,282][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:09:43,283][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:09:43,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:09:43,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:09:43,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:09:43,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:09:43,287][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:09:43,288][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:09:43,288][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:09:43,289][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:09:43,335][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.052 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 01:10:31,624][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/8>
[2024-06-21 01:10:31,625][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:10:31,627][HYDRA] 	#529 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:10:32,233][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:10:32,235][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:10:32,236][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:10:32,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:10:32,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:10:32,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:10:32,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:10:32,261][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:10:32,262][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:10:32,262][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:10:32,263][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:10:32,335][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.16it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:11:20,475][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.15/9>
[2024-06-21 01:11:20,476][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:11:20,478][HYDRA] 	#530 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 01:11:20,650][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:11:20,652][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:11:20,652][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:11:20,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:11:20,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:11:20,655][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:11:20,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:11:20,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:11:20,657][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:11:20,657][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:11:20,658][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:11:20,686][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.08it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 01:12:09,865][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/0>
[2024-06-21 01:12:09,868][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:12:09,888][HYDRA] 	#531 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 01:12:10,378][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:12:10,380][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:12:10,381][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:12:10,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:12:10,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:12:10,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:12:10,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:12:10,390][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:12:10,390][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:12:10,390][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:12:10,392][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:12:10,606][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.08it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-06-21 01:12:59,080][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/1>
[2024-06-21 01:12:59,081][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:12:59,083][HYDRA] 	#532 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 01:12:59,258][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:12:59,259][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:12:59,260][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:12:59,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:12:59,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:12:59,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:12:59,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:12:59,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:12:59,265][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:12:59,265][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:12:59,267][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:12:59,297][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.052 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 01:13:47,531][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/2>
[2024-06-21 01:13:47,532][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:13:47,534][HYDRA] 	#533 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 01:13:48,140][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:13:48,142][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:13:48,143][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:13:48,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:13:48,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:13:48,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:13:48,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:13:48,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:13:48,149][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:13:48,149][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:13:48,150][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:13:48,217][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.14it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:14:36,492][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/3>
[2024-06-21 01:14:36,493][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:14:36,495][HYDRA] 	#534 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:14:36,674][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:14:36,675][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:14:36,676][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:14:36,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:14:36,679][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:14:36,679][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:14:36,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:14:36,680][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:14:36,681][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:14:36,681][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:14:36,682][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:14:36,711][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.11it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.053 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 01:15:24,695][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/4>
[2024-06-21 01:15:24,696][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:15:24,699][HYDRA] 	#535 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 01:15:24,879][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:15:24,881][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:15:24,882][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:15:24,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:15:24,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:15:24,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:15:24,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:15:24,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:15:24,886][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:15:24,886][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:15:24,888][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:15:24,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.051 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 01:16:13,114][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/5>
[2024-06-21 01:16:13,115][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:16:13,117][HYDRA] 	#536 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 01:16:13,301][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:16:13,303][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:16:13,304][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:16:13,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:16:13,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:16:13,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:16:13,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:16:13,310][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:16:13,311][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:16:13,311][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:16:13,313][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:16:13,379][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.24it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 01:17:01,674][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/6>
[2024-06-21 01:17:01,675][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:17:01,678][HYDRA] 	#537 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:17:01,848][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:17:01,849][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:17:01,850][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:17:01,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:17:01,852][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:17:01,853][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:17:01,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:17:01,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:17:01,855][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:17:01,855][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:17:01,856][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:17:01,884][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.052 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:17:50,305][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/7>
[2024-06-21 01:17:50,306][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:17:50,308][HYDRA] 	#538 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:17:50,490][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:17:50,492][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:17:50,493][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:17:50,493][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:17:50,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:17:50,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:17:50,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:17:50,503][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:17:50,503][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:17:50,503][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:17:50,504][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:17:50,582][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.054 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 01:18:39,473][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/8>
[2024-06-21 01:18:39,476][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:18:39,493][HYDRA] 	#539 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:18:39,793][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:18:39,795][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:18:39,796][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:18:39,796][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:18:39,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:18:39,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:18:39,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:18:39,800][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:18:39,800][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:18:39,800][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:18:39,802][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:18:39,831][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:19:27,880][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.2/9>
[2024-06-21 01:19:27,881][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:19:27,883][HYDRA] 	#540 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 01:19:28,061][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:19:28,062][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:19:28,063][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:19:28,064][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:19:28,066][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:19:28,066][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:19:28,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:19:28,068][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:19:28,068][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:19:28,068][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:19:28,069][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:19:28,098][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.05it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 01:20:16,633][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/0>
[2024-06-21 01:20:16,634][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:20:16,636][HYDRA] 	#541 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 01:20:16,819][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:20:16,821][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:20:16,822][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:20:16,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:20:16,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:20:16,825][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:20:16,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:20:16,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:20:16,828][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:20:16,829][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:20:16,830][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:20:16,897][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.09it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-06-21 01:21:05,921][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/1>
[2024-06-21 01:21:05,921][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:21:05,923][HYDRA] 	#542 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 01:21:06,097][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:21:06,098][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:21:06,099][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:21:06,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:21:06,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:21:06,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:21:06,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:21:06,104][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:21:06,104][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:21:06,104][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:21:06,106][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:21:06,136][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:21:55,350][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/2>
[2024-06-21 01:21:55,351][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:21:55,354][HYDRA] 	#543 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 01:21:55,539][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:21:55,540][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:21:55,541][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:21:55,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:21:55,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:21:55,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:21:55,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:21:55,548][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:21:55,548][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:21:55,548][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:21:55,550][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:21:55,618][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.15it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-21 01:22:44,614][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/3>
[2024-06-21 01:22:44,615][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:22:44,617][HYDRA] 	#544 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:22:44,791][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:22:44,792][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:22:44,793][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:22:44,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:22:44,796][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:22:44,796][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:22:44,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:22:44,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:22:44,798][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:22:44,798][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:22:44,800][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:22:44,829][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.054 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-06-21 01:23:32,964][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/4>
[2024-06-21 01:23:32,964][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:23:32,967][HYDRA] 	#545 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 01:23:33,148][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:23:33,150][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:23:33,151][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:23:33,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:23:33,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:23:33,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:23:33,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:23:33,157][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:23:33,157][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:23:33,157][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:23:33,159][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:23:33,224][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:24:22,925][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/5>
[2024-06-21 01:24:22,926][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:24:22,929][HYDRA] 	#546 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 01:24:23,102][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:24:23,104][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:24:23,105][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:24:23,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:24:23,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:24:23,108][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:24:23,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:24:23,109][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:24:23,109][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:24:23,109][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:24:23,111][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:24:23,140][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.13it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.053 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 01:25:11,365][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/6>
[2024-06-21 01:25:11,366][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:25:11,368][HYDRA] 	#547 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:25:11,545][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:25:11,547][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:25:11,548][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:25:11,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:25:11,551][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:25:11,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:25:11,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:25:11,553][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:25:11,553][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:25:11,553][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:25:11,555][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:25:11,586][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.96it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 01:26:00,007][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/7>
[2024-06-21 01:26:00,008][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:26:00,010][HYDRA] 	#548 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:26:00,195][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:26:00,197][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:26:00,198][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:26:00,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:26:00,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:26:00,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:26:00,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:26:00,205][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:26:00,206][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:26:00,206][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:26:00,207][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:26:00,302][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.31it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-06-21 01:26:48,381][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/8>
[2024-06-21 01:26:48,382][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:26:48,386][HYDRA] 	#549 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:26:48,581][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:26:48,582][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:26:48,583][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:26:48,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:26:48,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:26:48,586][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:26:48,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:26:48,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:26:48,587][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:26:48,587][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:26:48,589][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:26:48,619][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.08it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-21 01:27:37,758][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.25/9>
[2024-06-21 01:27:37,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:27:37,761][HYDRA] 	#550 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 01:27:37,942][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:27:37,944][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:27:37,945][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:27:37,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:27:37,947][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:27:37,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:27:37,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:27:37,968][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:27:37,969][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:27:37,969][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:27:37,970][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:27:38,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.12it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:28:26,267][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/0>
[2024-06-21 01:28:26,268][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:28:26,270][HYDRA] 	#551 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 01:28:26,448][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:28:26,450][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:28:26,451][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:28:26,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:28:26,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:28:26,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:28:26,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:28:26,455][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:28:26,456][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:28:26,456][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:28:26,457][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:28:26,488][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.07it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.054 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 01:29:14,645][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/1>
[2024-06-21 01:29:14,646][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:29:14,648][HYDRA] 	#552 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 01:29:14,829][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:29:14,830][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:29:14,831][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:29:14,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:29:14,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:29:14,835][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:29:14,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:29:14,842][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:29:14,842][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:29:14,842][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:29:14,844][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:29:14,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 01:30:03,215][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/2>
[2024-06-21 01:30:03,216][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:30:03,221][HYDRA] 	#553 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 01:30:03,438][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:30:03,440][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:30:03,441][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:30:03,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:30:03,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:30:03,444][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:30:03,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:30:03,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:30:03,446][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:30:03,446][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:30:03,447][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:30:03,485][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.08it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-06-21 01:30:52,351][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/3>
[2024-06-21 01:30:52,352][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:30:52,356][HYDRA] 	#554 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:30:52,567][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:30:52,569][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:30:52,570][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:30:52,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:30:52,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:30:52,575][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:30:52,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:30:52,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:30:52,578][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:30:52,578][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:30:52,579][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:30:52,620][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.04it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 01:31:41,702][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/4>
[2024-06-21 01:31:41,703][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:31:41,706][HYDRA] 	#555 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 01:31:41,892][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:31:41,893][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:31:41,895][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:31:41,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:31:41,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:31:41,898][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:31:41,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:31:41,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:31:41,901][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:31:41,901][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:31:41,903][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:31:41,995][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.23it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 01:32:30,454][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/5>
[2024-06-21 01:32:30,454][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:32:30,457][HYDRA] 	#556 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 01:32:30,640][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:32:30,641][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:32:30,642][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:32:30,642][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:32:30,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:32:30,645][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:32:30,645][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:32:30,646][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:32:30,646][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:32:30,646][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:32:30,648][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:32:30,683][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.03it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-06-21 01:33:18,982][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/6>
[2024-06-21 01:33:18,983][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:33:18,986][HYDRA] 	#557 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:33:19,175][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:33:19,177][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:33:19,178][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:33:19,178][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:33:19,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:33:19,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:33:19,182][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:33:19,185][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:33:19,185][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:33:19,185][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:33:19,187][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:33:19,277][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 5.91it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-06-21 01:34:08,853][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/7>
[2024-06-21 01:34:08,853][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:34:08,855][HYDRA] 	#558 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:34:09,028][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:34:09,030][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:34:09,031][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:34:09,031][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:34:09,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:34:09,034][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:34:09,034][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:34:09,035][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:34:09,035][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:34:09,035][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:34:09,036][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:34:09,068][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-06-21 01:34:57,390][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/8>
[2024-06-21 01:34:57,391][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:34:57,393][HYDRA] 	#559 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:34:57,572][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:34:57,574][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-06-21 01:34:57,575][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:34:57,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:34:57,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:34:57,578][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:34:57,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:34:57,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:34:57,579][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:34:57,580][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:34:57,581][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:34:57,613][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  459 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  452 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  226 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │ 10.5 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  226 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 99.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │ 10.4 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │ 10.5 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  3.8 K │
│ 17 │ model.W_s1                          │ Linear            │  1.9 K │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 459 K                                                         
Non-trainable params: 0                                                         
Total params: 459 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 6.06it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-06-21 01:35:45,948][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-05/travel_lot/0.3/9>
[2024-06-21 01:35:45,949][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
